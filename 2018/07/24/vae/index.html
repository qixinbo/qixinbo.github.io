<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"qixinbo.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="参考文献 变分自编码器（Variational Auto-Encoder）可以说是深度学习领域的一股清流，没有采用“盲目堆砌各种神经层而乱碰瞎试”的套路，而是将神经网络与贝叶斯概率图结合，是理论指导模型结构设计的范例。深入了解它的原理，可以有助于建立良好的算法设计思想。 本文是对“科学空间”博主苏剑林的三篇博客的摘抄总结（话说苏博主真是沉得下心来研究算法啊。。科普得还那么好。。）：   * 变分自">
<meta property="og:type" content="article">
<meta property="og:title" content="变分自编码器的原理和程序解析">
<meta property="og:url" content="http://qixinbo.github.io/2018/07/24/vae/index.html">
<meta property="og:site_name" content="数字旗手">
<meta property="og:description" content="参考文献 变分自编码器（Variational Auto-Encoder）可以说是深度学习领域的一股清流，没有采用“盲目堆砌各种神经层而乱碰瞎试”的套路，而是将神经网络与贝叶斯概率图结合，是理论指导模型结构设计的范例。深入了解它的原理，可以有助于建立良好的算法设计思想。 本文是对“科学空间”博主苏剑林的三篇博客的摘抄总结（话说苏博主真是沉得下心来研究算法啊。。科普得还那么好。。）：   * 变分自">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/0072Lfvtly1fu4dv5emcdj30or0c7myy.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/0072Lfvtly1fu4eolz910j30qt0dyq5e.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/0072Lfvtly1fu4jl5x0o9j30rf0imtcm.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/0072Lfvtly1fu4ku70trbj30rd0nmwiv.jpg">
<meta property="article:published_time" content="2018-07-23T16:00:00.000Z">
<meta property="article:modified_time" content="2021-03-26T07:57:13.181Z">
<meta property="article:author" content="Xin-Bo Qi(亓欣波)">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ws1.sinaimg.cn/large/0072Lfvtly1fu4dv5emcdj30or0c7myy.jpg">

<link rel="canonical" href="http://qixinbo.github.io/2018/07/24/vae/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>变分自编码器的原理和程序解析 | 数字旗手</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="数字旗手" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">数字旗手</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">电气化、自动化、数字化、智能化、智慧化</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-scholar">

    <a href="/scholar/" rel="section"><i class="fa fa-chart-bar fa-fw"></i>scholar</a>

  </li>
        <li class="menu-item menu-item-sources">

    <a href="/sources/" rel="section"><i class="fa fa-rss fa-fw"></i>sources</a>

  </li>
        <li class="menu-item menu-item-gallery">

    <a href="/gallery/" rel="section"><i class="fa fa-file-image fa-fw"></i>gallery</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404.html" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Commonweal 404</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2018/07/24/vae/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Digitize everything to realize Digitalization!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数字旗手">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          变分自编码器的原理和程序解析
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-07-24 00:00:00" itemprop="dateCreated datePublished" datetime="2018-07-24T00:00:00+08:00">2018-07-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-26 15:57:13" itemprop="dateModified" datetime="2021-03-26T15:57:13+08:00">2021-03-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">algorithm</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2018/07/24/vae/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/07/24/vae/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>变分自编码器（Variational Auto-Encoder）可以说是深度学习领域的一股清流，没有采用“盲目堆砌各种神经层而乱碰瞎试”的套路，而是将神经网络与贝叶斯概率图结合，是理论指导模型结构设计的范例。深入了解它的原理，可以有助于建立良好的算法设计思想。<br>本文是对“科学空间”博主苏剑林的三篇博客的摘抄总结（话说苏博主真是沉得下心来研究算法啊。。科普得还那么好。。）：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://kexue.fm/archives/5253">变分自编码器（一）：原来是这么一回事</a></li>
<li><a target="_blank" rel="noopener" href="https://kexue.fm/archives/5343">变分自编码器（二）：从贝叶斯观点出发</a></li>
<li><a target="_blank" rel="noopener" href="https://kexue.fm/archives/5383">变分自编码器（三）：这样做为什么能成？</a></li>
</ul>
<p>以及结合变分自编码VAE的PyTorch实现<a target="_blank" rel="noopener" href="https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py">VAE in PyTorch</a>,来对VAE进行理解。</p>
<h1 id="VAE的原理"><a href="#VAE的原理" class="headerlink" title="VAE的原理"></a>VAE的原理</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>我们有一批数据样本${X_1, X_2, …, X_n}$，其整体用$X$来描述。我们想通过这些数据样本来得到$X$的分布$p(X)$，这样就可以得到所有可能的$X$。<br>但直接通过这些样本点来得到分布是不现实的，因为我们也不知道它符合什么样的分布。机器学习中的“生成模型”，比如VAE和GAN，对于这个问题的解决方式是引入一个中间隐变量$Z$，然后构建一个从隐变量$Z$生成数据$\hat{X}$的模型。那么，生成数据的边缘分布就可以如下计算：</p>
<script type="math/tex; mode=display">
p(\hat{X})=\int\_Z p(\hat{X}|Z)p(Z)dz</script><p>理想情况下，该生成分布就是原始样本点的概率分布，即$p(\hat{X})=p(X)$。如果这个目标达到了，这样就既得到了$p(X)$这个概率分布，又得到了生成模型$p(X|Z)$，一举两得。<br>具体计算时，条件概率$p(X|Z)$和先验概率$p(Z)$都可以事先假定，但此时生成的数据是采样结果，并不知道它们的分布，所以生成模型的难题就是判断生成分布与真实分布的相似度。<br><img src="https://ws1.sinaimg.cn/large/0072Lfvtly1fu4dv5emcdj30or0c7myy.jpg" alt=""></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>注意，原论文中的优化目标是最大化原始样本的对数似然函数，即：</strong></p>
<script type="math/tex; mode=display">
\ln p\_\theta (X\_1, X\_2,..., X\_n) = \sum\_{i=1}^N \ln p\_\theta (X\_i)</script><p><strong>而经过推导以后，得到：该似然函数是编码模型$q(Z|X)$与后验概率$p(Z|X)$的KL散度与另一个loss的和，而直接优化这个似然函数不可行，且这个KL散度最小为0，因此转而优化这个loss，即似然函数的下界。而下方的推导，一开始的优化目标就是与原论文不同，下方的优化目标是真实联合分布和生成模型的联合分布的KL散度最小。实际上，两方一对比可以发现，原论文要优化的下界loss就是下方推导的联合分布的KL散度。<br>所以，下方的loss只是原论文的一个中间步骤，VAE的理想目标还是对原始数据的极大似然估计，但发现该目标实现不了，因此VAE只能说是一个近似模型。</strong></p>
<p>假设$p(X)$就是要求的真实分布，因为：</p>
<script type="math/tex; mode=display">
p(X)=\int\_Z p(X|Z)p(Z)dz=\int\_Z p(X,Z)dZ</script><p>所以从联合分布的角度来看，假设有一个任意的联合概率分布$q(X,Z)$，用KL散度来度量两个联合分布之间的距离：</p>
<script type="math/tex; mode=display">
KL(p(x,z)||q(x,z))= \iint p(X,Z) \ln\frac{p(X,Z)}{q(X,Z)}dZdX</script><p>我们希望这个KL散度越小越好，因此，损失函数就是以这个KL散度为基本。由于我们手头上只有$X$的样本，因此利用$p(X,Z)=\tilde{p}(X)p(Z|X)$对上式进行改写：<br>\begin{aligned}<br>KL(p(X,Z)\parallel q(X,Z)) =&amp; \int \tilde{p}(X)\Big[ \int p(Z|X) \ln \frac{\tilde{p}(X)p(Z|X)}{q(X,Z)}dZ \Big]dX \\<br>=&amp; \mathbb{E}_{X\sim\tilde{p}(X)}\Big[ \int p(Z|X) \ln\frac{\tilde{p}(X)p(Z|X)}{q(X,Z)}dZ \Big] \\<br>=&amp; \mathbb{E}_{X\sim\tilde{p}(X)}\Big[ \int p(Z|X) \big[ \ln\tilde{p}(X)+\ln\frac{p(Z|X)}{q(X,Z)} \big]dZ \Big] \\<br>=&amp; \mathbb{E}_{X\sim\tilde{p}(X)}\Big[ \int p(Z|X) \ln\tilde{p}(X)dZ \Big] + \mathbb{E}_{X\sim\tilde{p}(X)}\Big[ \int p(Z|X) \ln\frac{p(Z|X)}{q(X,Z)} dZ \Big] \\<br>=&amp; \mathbb{E}_{X\sim\tilde{p}(X)}\Big[ \ln\tilde{p}(X)\int p(Z|X) dZ \Big] + \mathbb{E}_{X\sim\tilde{p}(X)}\Big[ \int p(Z|X) \ln\frac{p(Z|X)}{q(X,Z)} dZ \Big] \\<br>=&amp; \mathbb{E}_{X\sim\tilde{p}(X)}\Big[ \ln\tilde{p}(X)\Big] + \mathbb{E}_{X\sim\tilde{p}(X)}\Big[ \int p(Z|X) \ln\frac{p(Z|X)}{q(X,Z)} dZ \Big] \\<br>\end{aligned}<br>注意，这里的$\tilde{p}(X)$是根据样本$X_1,X_2,…,X_n$确定的关于$X$的先验分布，尽管我们不一定能准确写出它的形式，但它是确定的、存在的，因此第一项只是一个常数，所以，损失函数中可以去掉这一项，只包含KL散度的第二部分：<br>\begin{aligned}<br>L&amp;= KL(p(X|Z)||q(X,Z))-CONSTANT \\<br>&amp;= \mathbb{E}_{X\sim\tilde{p}(X)}\Big[ \int p(Z|X) \ln\frac{p(Z|X)}{q(X,Z)} dZ \Big] \\<br>\end{aligned}<br>因为KL散度最小为0，所以这里的损失函数的下界就是$-\mathbb{E}_{\tidle{p}(X)} \Big[ \ln \tilde{p}(X) \Big]$，注意到$\tilde{p}(X)$不一定是概率，在连续情形时它是概率密度，所以它可以大于1也可以小于1，所以这个下界有可能为负，即loss可能为负。实际比较损失与下界的接近程度就可以比较生成器的相对质量。<br>进一步地，把$q(X,Z)$利用联合概率$q(X,Z)=q(X|Z)q(Z)$进行变换：<br>\begin{aligned}<br>L &amp;= \mathbb{E}_{X\sim\tilde{p}(X)}\Big[ \int p(Z|X) \ln\frac{p(Z|X)}{q(X|Z)q(Z)} dZ \Big] \\<br>&amp;= \mathbb{E}_{X\sim\tilde{p}(X)}<br>\Big[ -\int p(Z|X)\ln q(X|Z)dZ + \int p(Z|X) \ln\frac{p(Z|X)}{q(Z)} dZ \Big] \\<br>&amp;= \mathbb{E}_{X\sim\tilde{p}(X)}<br>\Big[ \mathbb{E}_{Z\sim p(Z|X)}\big[ -\ln q(X|Z) \big] + \mathbb{E}_{z\sim p(Z|X)} \big[ \ln\frac{p(Z|X)}{q(Z)} \big] \Big] \\<br>&amp;= \mathbb{E}_{X\sim\tilde{p}(X)}<br>\Big[ \mathbb{E}_{Z\sim p(Z|X)}\big[ -\ln q(X|Z) \big] + KL \big( p(Z|X) || q(Z) \big) \Big] \\<br>\end{aligned}<br>注意，第一项中$Z$是根据$p(Z|X)$来采样的，而计算的却是$-\ln q(X|Z)$的期望。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>现在$q(X|Z), q(Z|X), q(Z)$全都是未知的，连形式都没有确定，而为了实际算法，就得明确地把它们写出来。<br>首先，为了便于采样，假设$Z\sim N(0,1)$，即隐参量符合标准正态分布，这就解决了$q(Z)$。<br>然后，对于$q(Z|X)$，也是假设它是（各分量独立的）正态分布，其均值和方差由$X$决定，这个决定由神经网络算出。具体来说就是设计两个神经网络，它们接收$X$，分别输出均值$\mu(X)$和方差的对数$\ln \sigma^2$。这两个参数训练出来以后，就可以得到$p(Z|X)$的形式（注意是多元正态分布的概率密度）：</p>
<script type="math/tex; mode=display">
p(Z|X)=\frac{1}{\prod\limits_{k=1}^d \sqrt{2\pi \sigma_{(k)}^2(X)}}\exp\left(-\frac{1}{2}\left\Vert\frac{Z-\mu(X)}{\sigma(X)}\right\Vert^2\right)</script><p>其中，$d$是隐参量$Z$的维度。以$Z$的维度为2为例，上式的具体写法就是：</p>
<script type="math/tex; mode=display">
p(Z|X)=\frac{1}{2\pi \sigma_{(1)}(X)\sigma_{(2)}(X)}\exp\left(-\frac{1}{2}\left((\frac{Z-\mu\_1(X)}{\sigma\_1(X)})^2+(\frac{Z-\mu\_2(X)}{\sigma\_2(X)})^2\right)\right)</script><p>这部分实际就是Encoder的作用。<br>既然假定了$q(Z)$和$q(Z|X)$都是正态分布，那么它们的KL散度也就可以计算出来（由于考虑的是各分量独立的多元正态分布，因此只需要推导一元正态分布的情形即可）：<br>\begin{aligned}<br>&amp;KL\Big(N(\mu,\sigma^2) || N(0,1)\Big) \\<br>&amp;= \int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \left(\ln \frac{e^{-(x-\mu)^2/2\sigma^2}/\sqrt{2\pi\sigma^2}}{e^{-x^2/2}/\sqrt{2\pi}}\right)dx \\<br>&amp;= \int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \ln \Big[ \frac{1}{\sqrt{\sigma^2}}\exp ( \frac{1}{2}\big[x^2-(x-\mu)^2/\sigma^2\big] ) \Big] dx \\<br>&amp;=\frac{1}{2}\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \Big[-\ln \sigma^2+x^2-(x-\mu)^2/\sigma^2 \Big] dx \\<br>&amp;=\frac{1}{2}\Big( \mu^2+\sigma^2-\ln \sigma^2-1) \\<br>\end{aligned}</p>
<p>现在只剩下$q(X|Z)$。原始论文中给出了两种分布供选择，分别对应于不同的情形。</p>
<h3 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h3><p>伯努利分布，即二元分布，只适用于$X$是一个多元的二值向量（只能是0或1）的清醒，比如$X$是二值图像，如MNIST数据集。这个分布的唯一参数就是$X=1$时的概率$\rho$，此时还是构建神经网络来训练这个参数。而后验分布$q(X|Z)$的表达式是：</p>
<script type="math/tex; mode=display">
q(X|Z)=\prod_{k=1}^D \Big(\rho_{(k)}(Z)\Big)^{X_{(k)}} \Big(1 - \rho_{(k)}(Z)\Big)^{1 - X_{(k)}}</script><p>这里，$D$是$X$的维度。<br>这时候就可以算出损失函数的第一项中的要求期望的那部分：</p>
<script type="math/tex; mode=display">
-\ln q(X|Z) = \sum_{k=1}^D \Big[- X_{(k)} \ln \rho_{(k)}(Z) - (1-X_{(k)}) \ln \Big(1 -\rho_{(k)}(Z)\Big)\Big]</script><p>可以看出，这一项正好是交叉熵的形式！所以，这部分可以直接调用软件中的交叉熵的计算函数。另外，要注意的是，$\rho(Z)$正好是用作Decoder，它要压缩到$0\sim 1$之间，这可以用sigmoid函数来整流。</p>
<h3 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a>正态分布</h3><p>如果假设$q(X|Z)$服从正态分布，那么它的形式就跟之前的$P(Z|X)$是一样的，只不过是$X$和$Z$交换位置（同时别忘了将维度也更改了）：</p>
<script type="math/tex; mode=display">
p(X|Z)=\frac{1}{\prod\limits_{k=1}^D \sqrt{2\pi \sigma_{(k)}^2(Z)}}\exp\left(-\frac{1}{2}\left\Vert\frac{X-\mu(Z)}{\sigma(Z)}\right\Vert^2\right)</script><p>这里的均值和方差的对数也是通过构建神经网络来训练得到（从下面可以看出，一般固定方差，所以就不用训练方差这个网络了），这里也是Decoder的作用。<br>那么，此时损失函数的第一项中的要求期望的那部分：</p>
<script type="math/tex; mode=display">
-\ln q(X|Z) = \frac{1}{2}\left\Vert\frac{X-\mu(Z)}{\sigma(Z)}\right\Vert^2 + \frac{D}{2}\ln 2\pi + \frac{1}{2}\sum_{k=1}^D \ln \sigma_{(k)}^2(Z)</script><p>如果我们将方差固定为一个常数$\sigma^2$，此时：</p>
<script type="math/tex; mode=display">
-\ln q(X|Z) \sim \frac{1}{2\sigma^2}\left\Vert X-\mu(Z)\right\Vert^2</script><p>上式就是将$\mu(Z)$作为Decoder时的MSE损失函数。<br>因此，对于二值数据，我们选择$\rho(Z)$作为Decoder（注意用sigmoid函数整流），然后用交叉熵作为损失函数，这对应于$q(X|Z)$是伯努利分布的情形；而对于一般数据，我们选择训练出来的均值$\mu(Z)$作为decoder，然后用MSE作为损失函数，这对应于$q(X|Z)$是固定方差的正态分布时的情形。具体这个方差取多少，是有一定人为性的，这个方差也决定了重构误差和KL散度的比例是多少。</p>
<h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>对于损失函数：</p>
<script type="math/tex; mode=display">
L = \mathbb{E}\_{X\sim\tilde{p}(X)} 
\Big[ \mathbb{E}\_{Z\sim p(Z|X)}\big[ -\ln q(X|Z) \big] + KL \big( p(Z|X) || q(Z) \big) \Big]</script><p>已经知道了$-\ln q(X|Z)$ 和$KL \big( p(Z|X) || q(Z) \big)$，还涉及到$\mathbb{E}_{Z\sim p(Z|X)}\big[ -\ln q(X|Z) \big]$怎么采样计算的问题。VAE直接给出了“只采一个样本就可以”的结论，因此损失函数变为：</p>
<script type="math/tex; mode=display">
L = \mathbb{E}\_{X\sim\tilde{p}(X)} \Big[ -\ln q(X|Z) + KL \big( p(Z|X) || q(Z) \big) \Big]</script><p>事实上我们会运行多个epoch，每次的隐参量都是随机生成的，因此当epoch数足够多时，事实上是可以保证采样的充分性的。</p>
<h1 id="VAE的形象化理解"><a href="#VAE的形象化理解" class="headerlink" title="VAE的形象化理解"></a>VAE的形象化理解</h1><h2 id="VAE的错误理解"><a href="#VAE的错误理解" class="headerlink" title="VAE的错误理解"></a>VAE的错误理解</h2><p>关于VAE的一个常见的错误理解如下图所示：<br><img src="https://ws1.sinaimg.cn/large/0072Lfvtly1fu4eolz910j30qt0dyq5e.jpg" alt=""><br>即，先根据原始的样本数据算出$Z$所符合的标准正态分布的均值和方差，然后再从该分布中采样一个$Z$，根据$Z$算出一个$X$。但问题是：究竟经过重新采样出来的$Z_k$及其算出来的$X_k$，是不是还对应着原来的$X_k$，所以此时如果直接最小化$D(\hat{X}_k, X_k)$是很不科学的（$D$代表某种距离函数），事实上代码也不是这么写的。</p>
<h2 id="VAE的正确理解"><a href="#VAE的正确理解" class="headerlink" title="VAE的正确理解"></a>VAE的正确理解</h2><p>为了保证经过重新采样出来的$Z_k$及其算出来的$X_k$，还对应着原来的$X_k$，我们可以假定这个采样点$Z_k$所遵循的分布$p(Z)$是专属于$X_k$的，即$p(Z|X_k)$，而且该分布还假定是独立的、多元的正态分布。这样，每一个$X_k$都配备了一个专属的正态分布，这样才方便后面的生成器做还原（即每个样本$X_k$都有自己的均值和方差）。但这样一来，多少个$X$就有多少个正态分布。我们知道正态分布有两组参数：均值$\mu$和方差$\sigma^2$（多元的话，它们都是向量），那怎样确定专属于$X_k$的正态分布的均值和方差呢？方法就是用神经网络来拟合。这就是神经网络时代的哲学：难算的都用神经网络来拟合。注意，这里是构建两个分别含有两个全连接层的神经网络$\mu_k=f_1(X_k)$和$log\sigma^2=f_2(X_k)$来计算均值和方差。注意，$f_2$是训练的方差的对数。<br>示意图如下：<br><img src="https://ws1.sinaimg.cn/large/0072Lfvtly1fu4jl5x0o9j30rf0imtcm.jpg" alt=""></p>
<h2 id="VAE的生成能力体现在哪"><a href="#VAE的生成能力体现在哪" class="headerlink" title="VAE的生成能力体现在哪"></a>VAE的生成能力体现在哪</h2><p>在最小化$D(\hat{X}_k,X_k)$时，结果会受到噪声的影响，这是因为$Z_k$是通过重新采样得到的（即通过一个随机数生成）。显然噪声会增加训练的难度，不过好在这个噪声强度（也就是方差）是通过一个神经网络算出来的，所以最终模型为了降低训练难度（只拟合均值，肯定比拟合多个来得容易），会想尽办法让方差为0。而方差为0的话，也就丧失了随机性，所以不管如何采样，其实都只是取得了均值。即，模型会慢慢退化成普通的AutoEncoder，没有了生成能力。<br>而VAE之所以是生成模型，是因为它还让所有的$p(Z|X)$都向标准正态分布看齐。因为此时：</p>
<script type="math/tex; mode=display">
p(Z)=\sum\_X p(Z|X)p(X)=\sum\_X N(0,1)p(X)=N(0,1)\sum\_X p(X)=N(0,1)</script><p>这样就能推出这样的先验假设，即$p(Z)$是标准正态分布，这就意味着，隐参量符合了正态分布，这样就可以放心地从这个分布中随机采样，保证了生成能力。<br>示意图如下：<br><img src="https://ws1.sinaimg.cn/large/0072Lfvtly1fu4ku70trbj30rd0nmwiv.jpg" alt=""><br>那么，问题就变成了怎样让$p(Z|X)$向标准正态分布看齐。最直接的一个方法就是在损失函数中加入额外的loss：</p>
<script type="math/tex; mode=display">
L\_\mu = ||f\_1(X\_k)||^2 \quad and \quad L\_{\sigma^2}=||f\_2(X\_K)||^2</script><p>因为$f_1$和$f_2$分别代表了均值和方差的对数，达到标准正态分布意味着二者都为零。但是这种方法面临着这两个损失怎样选取的问题。所以，原文中直接算了一般的正态分布和标准正态分布的KL散度作为额外的损失，即</p>
<script type="math/tex; mode=display">
L\_{\mu,\sigma^2}=\frac{1}{2}\sum\_{i=1}^d(\mu\_{(i)}^2+\sigma\_{(i)}^2-log\sigma\_{(i)}^2-1)</script><p>这里$d$是隐变量$Z$的维度，而$\mu_{(i)}$和$\sigma_{(i)}^2$分别表示一般正态分布的均值向量和方差向量的第i个分量。</p>
<h2 id="VAE的本质"><a href="#VAE的本质" class="headerlink" title="VAE的本质"></a>VAE的本质</h2><p>VAE本质上就是在我们常规的自编码器的基础上，对encoder的结果（在VAE中对应着计算均值的网络）加上了“高斯噪声”，使得结果decoder能够对噪声有鲁棒性；而那个额外的KL loss（目的是让均值为0，方差为1），事实上就是相当于对encoder的一个正则项，希望encoder出来的东西均有零均值。<br>那另外一个encoder（对应着计算方差的网络）的作用呢？它是用来动态调节噪声的强度的。直觉上来想，当decoder还没有训练好时（重构误差远大于KL loss），就会适当降低噪声（KL loss增加，注意KL loss等于0表示分布就是标准正态分布），使得拟合起来容易一些（重构误差开始下降）；反之，如果decoder训练得还不错时（重构误差小于KL loss），这时候噪声就会增加（KL loss减少），使得拟合更加困难了（重构误差又开始增加），这时候decoder就要想办法提高它的生成能力了。<br>说白了，重构的过程是希望没噪声的，而KL loss则希望有高斯噪声的，两者是对立的。所以，VAE跟GAN一样，内部其实是包含了一个对抗的过程，只不过它们两者是混合起来，共同进化的。</p>
<h1 id="条件VAE"><a href="#条件VAE" class="headerlink" title="条件VAE"></a>条件VAE</h1><p>因为目前的VAE是无监督训练的，因此很自然想到：如果有标签数据，那么能不能把标签信息加进去辅助生成样本呢？这个问题的意图，往往是希望能够实现控制某个变量来实现生成某一类图像。当然，这是肯定可以的，我们把这种情况叫做Conditional VAE，或者叫CVAE。（相应地，在GAN中我们也有个CGAN。）</p>
<h1 id="程序实现"><a href="#程序实现" class="headerlink" title="程序实现"></a>程序实现</h1><h2 id="导入必要的包"><a href="#导入必要的包" class="headerlink" title="导入必要的包"></a>导入必要的包</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn as nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional as F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">from torchvision <span class="keyword">import</span> transforms</span><br><span class="line">from torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"></span><br><span class="line"># Device configuration</span><br><span class="line">device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line"></span><br><span class="line"># Create a directory <span class="keyword">if</span> <span class="keyword">not</span> exists</span><br><span class="line">sample_dir = &#x27;samples&#x27;</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.<span class="built_in">exists</span>(sample_dir):</span><br><span class="line">    os.<span class="built_in">makedirs</span>(sample_dir)</span><br></pre></td></tr></table></figure>
<p>导入PyTorch和TorchVision，并根据实际情形看使用GPU还是CPU。建立samples文件夹放置图片。</p>
<h2 id="设置超参数和导入数据"><a href="#设置超参数和导入数据" class="headerlink" title="设置超参数和导入数据"></a>设置超参数和导入数据</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">image_size = <span class="number">784</span> # 这个是输入图片的长宽像素之积，MNIST图片是<span class="number">28</span>*<span class="number">28</span>的，所以是<span class="number">784</span></span><br><span class="line">h_dim = <span class="number">400</span> # 计算均值方差的神经网络是一个含有两个全连接层和一个ReLU层的网络。这里是第一个全连接层的输出神经元个数 </span><br><span class="line">z_dim = <span class="number">20</span> # 这里是第二个全连接层的输出神经元个数</span><br><span class="line">num_epochs = <span class="number">15</span> # 迭代次数</span><br><span class="line">batch_size = <span class="number">128</span> # 批处理样本时每批的个数</span><br><span class="line">learning_rate = <span class="number">1e-3</span> # 学习速率</span><br><span class="line"></span><br><span class="line"># MNIST dataset</span><br><span class="line">dataset = torchvision.datasets.MNIST(root=&#x27;../../data&#x27;,</span><br><span class="line">                                     train=True,</span><br><span class="line">                                     transform=transforms.<span class="built_in">ToTensor</span>(),</span><br><span class="line">                                     download=True)</span><br><span class="line"></span><br><span class="line"># Data loader</span><br><span class="line">data_loader = torch.utils.data.<span class="built_in">DataLoader</span>(dataset=dataset,</span><br><span class="line">                                          batch_size=batch_size, </span><br><span class="line">                                          shuffle=True)</span><br></pre></td></tr></table></figure>
<h2 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># VAE model</span><br><span class="line"><span class="function">class <span class="title">VAE</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"><span class="function">    def __init__(self, image_size=</span><span class="number">784</span>, h_dim=<span class="number">400</span>, z_dim=<span class="number">20</span>):</span><br><span class="line">        <span class="built_in">super</span>(VAE, self).__init__()</span><br><span class="line">        self.fc1 = nn.<span class="built_in">Linear</span>(image_size, h_dim)</span><br><span class="line">        self.fc2 = nn.<span class="built_in">Linear</span>(h_dim, z_dim)</span><br><span class="line">        self.fc3 = nn.<span class="built_in">Linear</span>(h_dim, z_dim)</span><br><span class="line">        self.fc4 = nn.<span class="built_in">Linear</span>(z_dim, h_dim)</span><br><span class="line">        self.fc5 = nn.<span class="built_in">Linear</span>(h_dim, image_size)</span><br><span class="line">        </span><br><span class="line">    def <span class="built_in">encode</span>(self, x): # 编码器</span><br><span class="line">        h = F.<span class="built_in">relu</span>(self.<span class="built_in">fc1</span>(x)) # 计算第一个全连接层，然后用ReLU整流</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">fc2</span>(h), self.<span class="built_in">fc3</span>(h) # 对均值和方差(实际是$log\sigma^<span class="number">2</span>$)都计算第二个全连接层</span><br><span class="line">    </span><br><span class="line">    def <span class="built_in">reparameterize</span>(self, mu, log_var): # 重参数化</span><br><span class="line">        std = torch.<span class="built_in">exp</span>(log_var/<span class="number">2</span>)  # 这一步是将方差的对数转换为标准差。</span><br><span class="line">        eps = torch.<span class="built_in">randn_like</span>(std) # 生成一个跟std同样大小的取自标准正态分布的随机数。</span><br><span class="line">        <span class="keyword">return</span> mu + eps * std  # 根据标准正态分布的采样得到之前正态分布中的采样。</span><br><span class="line"></span><br><span class="line">    def <span class="built_in">decode</span>(self, z): # 解码器，也是两个全连接层，中间有个ReLU整流，最后有个Sigmoid层进行激活。</span><br><span class="line">        h = F.<span class="built_in">relu</span>(self.<span class="built_in">fc4</span>(z)) </span><br><span class="line">        <span class="keyword">return</span> F.<span class="built_in">sigmoid</span>(self.<span class="built_in">fc5</span>(h))</span><br><span class="line">    </span><br><span class="line">    def forward(self, x): # 前向计算</span><br><span class="line">        mu, log_var = self.<span class="built_in">encode</span>(x) # 对输入变量进行编码，得到隐变量所满足的正态分布的均值和方差</span><br><span class="line">        z = self.<span class="built_in">reparameterize</span>(mu, log_var) # 重参数化</span><br><span class="line">        x_reconst = self.<span class="built_in">decode</span>(z) # 解码器</span><br><span class="line">        <span class="keyword">return</span> x_reconst, mu, log_var</span><br></pre></td></tr></table></figure>
<p>有几个注意点：<br>(1) 对每个输入的x都生成一个正态分布，然后再从该分布中采样得到z，然后再根据生成器得到新x。所以，输入x、隐参量z和输出x是一一对应的。<br>(2) 计算方差的神经网络实际计算的不是方差，而是它的对数，即$log(\sigma^2)$。<br>(3) 重参数reparameterize的作用：得到隐参量z的正态分布后，还要对其采样得到离散值。但这个“采样”操作是不可导的，因此没法应用于梯度下降的训练过程。而根据公式：</p>
<script type="math/tex; mode=display">
\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(z-\mu)^2}{2\sigma^2})dz = 
\frac{1}{\sqrt{2\pi}}exp[-\frac{1}{2}(\frac{z-\mu}{\sigma})^2]d(\frac{z-\mu}{\sigma})</script><p>可以看出变量$\epsilon=\frac{z-\mu}{\sigma}$服从标准正态分布。因此，从原正态分布中采样一个z，就等同于从标准正态分布中采样一个$\epsilon$，然后让$z=\mu+\epsilon\sigma$。这样，就可以不用把采样这个操作加入到反向传播中，而只需要将结果放进来即可。</p>
<h2 id="训练和测试模型"><a href="#训练和测试模型" class="headerlink" title="训练和测试模型"></a>训练和测试模型</h2><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">model = <span class="built_in">VAE</span>().<span class="built_in">to</span>(device) </span><br><span class="line">optimizer = torch.optim.<span class="built_in">Adam</span>(model.<span class="built_in">parameters</span>(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"># Start training</span><br><span class="line"><span class="keyword">for</span> epoch in <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (x, _) in <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        # Forward pass</span><br><span class="line">        x = x.<span class="built_in">to</span>(device).<span class="built_in">view</span>(<span class="number">-1</span>, image_size)</span><br><span class="line">        x_reconst, mu, log_var = <span class="built_in">model</span>(x)  # 前向计算，得到新的变量x，以及正态分布的均值和方差</span><br><span class="line">        </span><br><span class="line">        # Compute reconstruction loss <span class="keyword">and</span> kl divergence</span><br><span class="line">        # For KL divergence, see Appendix B in VAE paper <span class="keyword">or</span> http:<span class="comment">//yunjey47.tistory.com/43</span></span><br><span class="line">        reconst_loss = F.<span class="built_in">binary_cross_entropy</span>(x_reconst, x, size_average=False) # 计算输入变量和新变量之间的二进制交叉熵</span><br><span class="line">        kl_div = - <span class="number">0.5</span> * torch.<span class="built_in">sum</span>(<span class="number">1</span> + log_var - mu.<span class="built_in">pow</span>(<span class="number">2</span>) - log_var.<span class="built_in">exp</span>()) # 计算KL散度，公式见下方。</span><br><span class="line">        </span><br><span class="line">        # Backprop <span class="keyword">and</span> optimize</span><br><span class="line">        loss = reconst_loss + kl_div # 总损失等于交叉熵和KL散度之和</span><br><span class="line">        optimizer.<span class="built_in">zero_grad</span>() # 梯度置零</span><br><span class="line">        loss.<span class="built_in">backward</span>() # 反向传播</span><br><span class="line">        optimizer.<span class="built_in">step</span>() # 步进</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;Epoch[&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Reconst Loss: &#123;:.4f&#125;, KL Div: &#123;:.4f&#125;&quot;</span> </span><br><span class="line">                   .format(epoch+<span class="number">1</span>, num_epochs, i+<span class="number">1</span>, <span class="built_in">len</span>(data_loader), reconst_loss.<span class="built_in">item</span>(), kl_div.<span class="built_in">item</span>()))</span><br><span class="line">    </span><br><span class="line">    with torch.<span class="built_in">no_grad</span>():</span><br><span class="line">        # Save the sampled images</span><br><span class="line">        z = torch.<span class="built_in">randn</span>(batch_size, z_dim).<span class="built_in">to</span>(device)</span><br><span class="line">        out = model.<span class="built_in">decode</span>(z).<span class="built_in">view</span>(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        save_image(out, os.path.join(sample_dir, &#x27;sampled-&#123;&#125;.png&#x27;.format(epoch+1)))</span><br><span class="line"></span><br><span class="line">        # Save the reconstructed images</span><br><span class="line">        out, _, _ = <span class="built_in">model</span>(x)</span><br><span class="line">        x_concat = torch.<span class="built_in">cat</span>([x.<span class="built_in">view</span>(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), out.<span class="built_in">view</span>(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)], dim=<span class="number">3</span>)</span><br><span class="line">        save_image(x_concat, os.path.join(sample_dir, &#x27;reconst-&#123;&#125;.png&#x27;.format(epoch+1)))</span><br></pre></td></tr></table></figure>
<p>注意：<br>总损失函数等于交叉熵和KL散度。<br>KL散度是为了度量两个概率分布之间的差异。如果两个分布相等，那么KL散度为0。KL散度的一个主要性质是非负性，因此最小化KL散度的结果就是使得两个分布尽可能相等，这一点的严格证明要用到变分法，这里正是VAE中的V的来源。<br>KL散度的计算公式是：</p>
<script type="math/tex; mode=display">
D\_{KL}(p(x)||q(x))=\int p(x)\ln\frac{p(x)}{q(x)}dx</script><p>上面的x是连续随机变量。如果是离散的随机变量，可以有两种方式计算：<br>一是使用数值计算，即</p>
<script type="math/tex; mode=display">
D\_{KL}(p(x)||q(x))=\sum{p(x_i) \ln \frac{p(x_i)}{q(x_i)}}\Delta x</script><p>二是使用采样计算，即</p>
<script type="math/tex; mode=display">
D\_{KL}(p(x)||q(x))=E\_{x \in p(x)} [\ln \frac{p(x_i)}{q(x_i)}]</script><p>其中，E是期望，有：</p>
<script type="math/tex; mode=display">
E\_{x~p(x)}[f(x)]=\int f(x)p(x)dx \approx \frac{1}{n}\sum\_{i=1}^n f(x\_i), \qquad x\_i \in p(x)</script><p>注意，上式中的$x_i$是从概率分布$p_i$中采样，所以采样结果已经包含在了$p_i$中，故形式上与数值计算不同。</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat_reward.png" alt="Xin-Bo Qi(亓欣波) WeChat Pay">
        <p>WeChat Pay</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/01/19/fcitx-install/" rel="prev" title="linux系统安装fcitx框架和搜狗输入法">
      <i class="fa fa-chevron-left"></i> linux系统安装fcitx框架和搜狗输入法
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/07/30/bayesian-method/" rel="next" title="贝叶斯公式、先验概率、最大似然估计、最大后验概率估计">
      贝叶斯公式、先验概率、最大似然估计、最大后验概率估计 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">1.</span> <span class="nav-text">参考文献</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#VAE%E7%9A%84%E5%8E%9F%E7%90%86"><span class="nav-number">2.</span> <span class="nav-text">VAE的原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0"><span class="nav-number">2.1.</span> <span class="nav-text">问题描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">2.2.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">2.3.</span> <span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83"><span class="nav-number">2.3.1.</span> <span class="nav-text">伯努利分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83"><span class="nav-number">2.3.2.</span> <span class="nav-text">正态分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%87%E6%A0%B7"><span class="nav-number">2.3.3.</span> <span class="nav-text">采样</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#VAE%E7%9A%84%E5%BD%A2%E8%B1%A1%E5%8C%96%E7%90%86%E8%A7%A3"><span class="nav-number">3.</span> <span class="nav-text">VAE的形象化理解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#VAE%E7%9A%84%E9%94%99%E8%AF%AF%E7%90%86%E8%A7%A3"><span class="nav-number">3.1.</span> <span class="nav-text">VAE的错误理解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VAE%E7%9A%84%E6%AD%A3%E7%A1%AE%E7%90%86%E8%A7%A3"><span class="nav-number">3.2.</span> <span class="nav-text">VAE的正确理解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VAE%E7%9A%84%E7%94%9F%E6%88%90%E8%83%BD%E5%8A%9B%E4%BD%93%E7%8E%B0%E5%9C%A8%E5%93%AA"><span class="nav-number">3.3.</span> <span class="nav-text">VAE的生成能力体现在哪</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VAE%E7%9A%84%E6%9C%AC%E8%B4%A8"><span class="nav-number">3.4.</span> <span class="nav-text">VAE的本质</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6VAE"><span class="nav-number">4.</span> <span class="nav-text">条件VAE</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A8%8B%E5%BA%8F%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.</span> <span class="nav-text">程序实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%BC%E5%85%A5%E5%BF%85%E8%A6%81%E7%9A%84%E5%8C%85"><span class="nav-number">5.1.</span> <span class="nav-text">导入必要的包</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE%E8%B6%85%E5%8F%82%E6%95%B0%E5%92%8C%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="nav-number">5.2.</span> <span class="nav-text">设置超参数和导入数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="nav-number">5.3.</span> <span class="nav-text">模型构建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.4.</span> <span class="nav-text">训练和测试模型</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xin-Bo Qi(亓欣波)"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Xin-Bo Qi(亓欣波)</p>
  <div class="site-description" itemprop="description">Digitize everything to realize Digitalization!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">128</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/qixinbo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qixinbo" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qixinbo@gmail.com" title="E-Mail → mailto:qixinbo@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/qixinbo" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;qixinbo" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fas fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.tsinghua.edu.cn/" title="https:&#x2F;&#x2F;www.tsinghua.edu.cn" rel="noopener" target="_blank">THU</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.imr.cas.cn/" title="http:&#x2F;&#x2F;www.imr.cas.cn" rel="noopener" target="_blank">IMR</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.sdu.edu.cn/" title="http:&#x2F;&#x2F;www.sdu.edu.cn" rel="noopener" target="_blank">SDU</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://liam0205.me/" title="http:&#x2F;&#x2F;liam0205.me&#x2F;" rel="noopener" target="_blank">黄晨成</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xin-Bo Qi(亓欣波)</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://qixinbo.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "http://qixinbo.github.io/2018/07/24/vae/";
    this.page.identifier = "2018/07/24/vae/";
    this.page.title = "变分自编码器的原理和程序解析";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://qixinbo.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>

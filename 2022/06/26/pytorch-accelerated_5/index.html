<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"qixinbo.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="简介 首先对pytorch-accelerated的核心类Trainer进行逐行代码的注释理解，然后再以官方的几个例子进行注解说明。  Trainer逐行代码注解 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 4">
<meta property="og:type" content="article">
<meta property="og:title" content="轻量级PyTorch通用训练模板pytorch-accelerated解析：5 -- Trainer运行及案例赏析">
<meta property="og:url" content="http://qixinbo.github.io/2022/06/26/pytorch-accelerated_5/index.html">
<meta property="og:site_name" content="数字旗手">
<meta property="og:description" content="简介 首先对pytorch-accelerated的核心类Trainer进行逐行代码的注释理解，然后再以官方的几个例子进行注解说明。  Trainer逐行代码注解 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 4">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-06-25T16:00:00.000Z">
<meta property="article:modified_time" content="2022-07-01T06:05:06.761Z">
<meta property="article:author" content="Xin-Bo Qi(亓欣波)">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://qixinbo.github.io/2022/06/26/pytorch-accelerated_5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>轻量级PyTorch通用训练模板pytorch-accelerated解析：5 -- Trainer运行及案例赏析 | 数字旗手</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="数字旗手" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">数字旗手</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">电气化、自动化、数字化、智能化、智慧化</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-scholar">

    <a href="/scholar/" rel="section"><i class="fa fa-chart-bar fa-fw"></i>scholar</a>

  </li>
        <li class="menu-item menu-item-sources">

    <a href="/sources/" rel="section"><i class="fa fa-rss fa-fw"></i>sources</a>

  </li>
        <li class="menu-item menu-item-gallery">

    <a href="/gallery/" rel="section"><i class="fa fa-file-image fa-fw"></i>gallery</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404.html" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Commonweal 404</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2022/06/26/pytorch-accelerated_5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Digitize everything to realize Digitalization!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数字旗手">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          轻量级PyTorch通用训练模板pytorch-accelerated解析：5 -- Trainer运行及案例赏析
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-06-26 00:00:00" itemprop="dateCreated datePublished" datetime="2022-06-26T00:00:00+08:00">2022-06-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-01 14:05:06" itemprop="dateModified" datetime="2022-07-01T14:05:06+08:00">2022-07-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/06/26/pytorch-accelerated_5/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/06/26/pytorch-accelerated_5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>首先对<code>pytorch-accelerated</code>的核心类<code>Trainer</code>进行逐行代码的注释理解，然后再以官方的几个例子进行注解说明。</p>
<h1 id="Trainer逐行代码注解"><a href="#Trainer逐行代码注解" class="headerlink" title="Trainer逐行代码注解"></a>Trainer逐行代码注解</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Copyright © 2021 Chris Hughes</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> <span class="type">Callable</span></span><br><span class="line"><span class="keyword">from</span> enum <span class="keyword">import</span> Enum</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Iterable</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator, DistributedType</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment"># 导入内置的回调函数</span></span><br><span class="line"><span class="keyword">from</span> pytorch_accelerated.callbacks <span class="keyword">import</span> (</span><br><span class="line">    CallbackHandler,</span><br><span class="line">    LogMetricsCallback,</span><br><span class="line">    PrintProgressCallback,</span><br><span class="line">    TerminateOnNaNCallback,</span><br><span class="line">    StopTrainingError,</span><br><span class="line">    ProgressBarCallback,</span><br><span class="line">    MoveModulesToDeviceCallback,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> pytorch_accelerated.run_config <span class="keyword">import</span> TrainerRunConfig</span><br><span class="line"><span class="keyword">from</span> pytorch_accelerated.tracking <span class="keyword">import</span> RunHistory, InMemoryRunHistory, LossTracker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认使用的回调函数有如下5个：</span></span><br><span class="line">DEFAULT_CALLBACKS = (</span><br><span class="line">    <span class="comment"># 该回调在训练或评估开始时，将所有属于`torch.nn.Module`的实例（除了网络模型）的`trainer`属性移动到相应的设备上。</span></span><br><span class="line">    MoveModulesToDeviceCallback,</span><br><span class="line">    <span class="comment"># 该回调在训练时监测是否出现&#x27;NaN&#x27;损失值，从而及时终止训练</span></span><br><span class="line">    TerminateOnNaNCallback,</span><br><span class="line">    <span class="comment"># 打印进度</span></span><br><span class="line">    PrintProgressCallback,</span><br><span class="line">    <span class="comment"># 进度条</span></span><br><span class="line">    ProgressBarCallback,</span><br><span class="line">    <span class="comment"># 指标记录</span></span><br><span class="line">    LogMetricsCallback,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一些学习率调度器需要一些信息，比如在训练运行期间发生的总步数。</span></span><br><span class="line"><span class="comment"># 由于在创建训练数据加载器之前无法获得这些信息（它们是`Trainer.train`中产生的），在这种情况下可以使用一个占位符值，比如：</span></span><br><span class="line"><span class="comment"># from functools import Partial</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># from pytorch_accelerated import TrainerPlaceholderValues</span></span><br><span class="line"><span class="comment"># from torch.optim.lr_scheduler import OneCycleLR</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create_scheduler_fn = partial(</span></span><br><span class="line"><span class="comment">#             OneCycleLR,</span></span><br><span class="line"><span class="comment">#             max_lr=config.lr,</span></span><br><span class="line"><span class="comment">#             epochs=TrainerPlaceholderValues.NUM_EPOCHS,</span></span><br><span class="line"><span class="comment">#             steps_per_epoch=TrainerPlaceholderValues.NUM_UPDATE_STEPS_PER_EPOCH,</span></span><br><span class="line"><span class="comment">#         )</span></span><br><span class="line"><span class="comment"># 这些占位符将由trainer在训练期间使用正确的数值替换。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 具体实现原理如下。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先TrainerPlaceholderValues派生自Enum，关于该类的用法仔细阅读如下两篇：</span></span><br><span class="line"><span class="comment"># https://docs.python.org/3/library/enum.html</span></span><br><span class="line"><span class="comment"># https://www.pythontutorial.net/python-oop/python-enum-class/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainerPlaceholderValues</span>(<span class="params">Enum</span>):</span></span><br><span class="line">    <span class="comment"># 这些枚举数值实际会调用trainer中的变量及配置</span></span><br><span class="line">    NUM_EPOCHS = <span class="string">&quot;trainer.run_config.num_epochs&quot;</span></span><br><span class="line">    NUM_UPDATE_STEPS_PER_EPOCH = <span class="string">&quot;trainer.run_config.num_update_steps_per_epoch&quot;</span></span><br><span class="line">    TRAIN_DATALOADER_LEN = <span class="string">&quot;len(trainer._train_dataloader)&quot;</span></span><br><span class="line">    EVAL_DATALOADER_LEN = <span class="string">&quot;len(trainer._eval_dataloader)&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 类方法，教程参考：</span></span><br><span class="line">    <span class="comment"># https://www.jianshu.com/p/87608d92fafe</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">placeholder_set</span>(<span class="params">cls</span>):</span></span><br><span class="line">        <span class="keyword">return</span> &#123;placeholder.name <span class="keyword">for</span> placeholder <span class="keyword">in</span> cls&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__create_new_enum</span>(<span class="params">original_enum, other, operation</span>):</span></span><br><span class="line">        enum_members = &#123;k: v.value <span class="keyword">for</span> k, v <span class="keyword">in</span> original_enum._member_map_.items()&#125;</span><br><span class="line">        enum_members[</span><br><span class="line">            original_enum.name</span><br><span class="line">        ] = <span class="string">f&quot;<span class="subst">&#123;enum_members[original_enum.name]&#125;</span><span class="subst">&#123;operation&#125;</span><span class="subst">&#123;other&#125;</span>&quot;</span></span><br><span class="line">        new_enum = Enum(<span class="string">&quot;TrainerPlaceholderValues&quot;</span>, enum_members)</span><br><span class="line">        <span class="keyword">return</span> new_enum._member_map_[original_enum.name]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__mul__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__create_new_enum(self, other, <span class="string">&quot;*&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__create_new_enum(self, other, <span class="string">&quot;+&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__sub__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> <span class="literal">NotImplemented</span>(</span><br><span class="line">            <span class="string">&quot;Subtraction is not supported, please re-write the expression in terms of addition&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果实例instance是一个偏函数对象，且包含了关键字，将替换它们，返回一个新的偏函数对象</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_trainer_placeholder_values</span>(<span class="params">trainer, instance</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(instance, partial):</span><br><span class="line">        placeholders = TrainerPlaceholderValues.placeholder_set()</span><br><span class="line">        keywords = <span class="built_in">list</span>(instance.keywords.items())</span><br><span class="line"></span><br><span class="line">        new_keywords = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> keyword, value <span class="keyword">in</span> keywords:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(value, <span class="string">&quot;name&quot;</span>):</span><br><span class="line">                <span class="comment"># 如果value有name属性，且在TrainerPlaceholderValues的占位符集合中</span></span><br><span class="line">                <span class="keyword">if</span> value.name <span class="keyword">in</span> placeholders:</span><br><span class="line">                    <span class="comment"># 则马上计算该占位符（即枚举值）的表达式，教程见：</span></span><br><span class="line">                    <span class="comment"># https://www.runoob.com/python/python-func-eval.html</span></span><br><span class="line">                    new_keywords[keyword] = <span class="built_in">eval</span>(value.value)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    new_keywords[keyword] = value</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_keywords[keyword] = value</span><br><span class="line"></span><br><span class="line">        instance = partial(instance.func, *instance.args, **new_keywords)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> instance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        model,</span></span></span><br><span class="line"><span class="function"><span class="params">        loss_func,</span></span></span><br><span class="line"><span class="function"><span class="params">        optimizer,</span></span></span><br><span class="line"><span class="function"><span class="params">        callbacks=DEFAULT_CALLBACKS,</span></span></span><br><span class="line"><span class="function"><span class="params">        run_history=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        <span class="comment"># 传入模型，后面会被替换成被accelerate封装后的模型</span></span><br><span class="line">        self.model = model</span><br><span class="line">        <span class="comment"># 传入损失函数</span></span><br><span class="line">        self.loss_func = loss_func</span><br><span class="line">        <span class="comment"># 传入优化器，后面会被替换成被accelerate封装后的优化器</span></span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        <span class="comment"># 将回调事件列表作为参数传入“回调函数句柄”</span></span><br><span class="line">        <span class="comment"># 该句柄的call_event()方法有如下形式：</span></span><br><span class="line">        <span class="comment"># def call_event(self, event, *args, **kwargs):</span></span><br><span class="line">        <span class="comment"># 即传入一个event事件及参数列表，然后该方法会判断该event在哪些回调函数中存在（即存在该成员函数或成员变量）</span></span><br><span class="line">        self.callback_handler = CallbackHandler(</span><br><span class="line">            callbacks,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 传入运行历史，该运行历史应该是RunHistory类型</span></span><br><span class="line">        <span class="comment"># 如果是None的话，就使用默认实现的InMemoryRunHistory()</span></span><br><span class="line">        <span class="comment"># 也可以自己手写基于基类RunHistory的实现</span></span><br><span class="line">        self.run_history: RunHistory = (</span><br><span class="line">            run_history <span class="keyword">if</span> run_history <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> InMemoryRunHistory()</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 创建一个accelerate.Accelerator的实例，用于管理训练过程</span></span><br><span class="line">        self._accelerator = self._create_accelerator()</span><br><span class="line">        <span class="comment"># 创建一个损失追踪器</span></span><br><span class="line">        self._loss_tracker = LossTracker()</span><br><span class="line">        <span class="comment"># 下面是一些内部变量，它们的值会在训练过程中被设置</span></span><br><span class="line">        self.create_scheduler_fn = <span class="literal">None</span></span><br><span class="line">        self.scheduler = <span class="literal">None</span></span><br><span class="line">        self.collate_fn = <span class="literal">None</span></span><br><span class="line">        self.train_dataset = <span class="literal">None</span></span><br><span class="line">        self.eval_dataset = <span class="literal">None</span></span><br><span class="line">        self._train_dataloader = <span class="literal">None</span></span><br><span class="line">        self._train_dl_kwargs = <span class="literal">None</span></span><br><span class="line">        self._eval_dl_kwargs = <span class="literal">None</span></span><br><span class="line">        self._eval_dataloader = <span class="literal">None</span></span><br><span class="line">        self.run_config: TrainerRunConfig = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化Trainer的末尾会调用一下`on_init_end`事件</span></span><br><span class="line">        <span class="comment"># 目前来看这几个内置的回调函数都没有该属性</span></span><br><span class="line">        self.callback_handler.call_event(<span class="string">&quot;on_init_end&quot;</span>, self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_accelerator</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Create an instance of :class:`accelerate.Accelerator` which will be used to manage training.</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> Accelerator()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建训练集的dataloader</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_train_dataloader</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self, batch_size: <span class="built_in">int</span>, train_dl_kwargs: <span class="built_in">dict</span> = <span class="literal">None</span></span></span></span><br><span class="line"><span class="function"><span class="params">    </span>) -&gt; Iterable:</span></span><br><span class="line">        <span class="comment"># 首先获得默认的训练集dataloader的参数，包括&quot;shuffle&quot;、&quot;pin_memory&quot;、&quot;batch_size&quot;和&quot;num_workers&quot;配置</span></span><br><span class="line">        default_train_dl_kwargs = self.get_default_train_dl_kwargs(batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果明确指定了参数字典，则对默认的字典进行更新，用 update 更新字典 a，会有两种情况：</span></span><br><span class="line">        <span class="comment"># （1）有相同的键时：会使用最新的字典 b 中 该 key 对应的 value 值。</span></span><br><span class="line">        <span class="comment"># （2）有新的键时：会直接把字典 b 中的 key、value 加入到 a 中。</span></span><br><span class="line">        <span class="keyword">if</span> train_dl_kwargs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            default_train_dl_kwargs.update(train_dl_kwargs)</span><br><span class="line"></span><br><span class="line">        self._train_dl_kwargs = default_train_dl_kwargs</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最终是调用torch原生的DataLoader来创建数据加载器</span></span><br><span class="line">        <span class="keyword">return</span> DataLoader(</span><br><span class="line">            dataset=self.train_dataset,</span><br><span class="line">            collate_fn=self.collate_fn,</span><br><span class="line">            **self._train_dl_kwargs,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_eval_dataloader</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self, batch_size: <span class="built_in">int</span>, eval_dl_kwargs: <span class="built_in">dict</span> = <span class="literal">None</span></span></span></span><br><span class="line"><span class="function"><span class="params">    </span>) -&gt; Iterable:</span></span><br><span class="line">        <span class="comment"># 首先获得默认的验证集dataloader的参数，包括&quot;shuffle&quot;、&quot;pin_memory&quot;、&quot;batch_size&quot;和&quot;num_workers&quot;配置</span></span><br><span class="line">        <span class="comment"># 与训练集不同的是，验证集的shuffle是False</span></span><br><span class="line">        default_eval_dl_kwargs = self.get_default_eval_dl_kwargs(batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> eval_dl_kwargs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            default_eval_dl_kwargs.update(eval_dl_kwargs)</span><br><span class="line"></span><br><span class="line">        self._eval_dl_kwargs = default_eval_dl_kwargs</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> DataLoader(</span><br><span class="line">            dataset=self.eval_dataset,</span><br><span class="line">            collate_fn=self.collate_fn,</span><br><span class="line">            **self._eval_dl_kwargs,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于之前传递给Trainer的工厂函数创建一个学习率调度器</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_scheduler</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 该工厂函数是个偏函数对象，它里面的关键词参数如果用到了占位符，会被实时结果所更新替代</span></span><br><span class="line">        scheduler_type = replace_trainer_placeholder_values(</span><br><span class="line">            self, self.create_scheduler_fn</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 该工厂函数再接收优化器参数，从而返回调度器实例</span></span><br><span class="line">        <span class="keyword">return</span> scheduler_type(self.optimizer)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">training_run_start</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This method is called at the start of a training run.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每一训练epoch开始时的动作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_epoch_start</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 默认行为是将模型设置成train模式，教程见：</span></span><br><span class="line">        <span class="comment"># https://zhuanlan.zhihu.com/p/494060986</span></span><br><span class="line">        self.model.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算训练时的损失</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_train_batch_loss</span>(<span class="params">self, batch</span>) -&gt; <span class="built_in">dict</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Calculates the training loss and return this along with the batch size and model outputs.</span></span><br><span class="line"><span class="string">        Any additional values returned will be available in the :meth:`~callbacks.TrainerCallback.on_train_step_end` callback method.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param batch: the output of the train dataloader</span></span><br><span class="line"><span class="string">        :return: A dictionary containing the training loss, model outputs and batch size. Can include any keys, but must include the keys &#x27;loss&#x27;, &#x27;model_outputs&#x27; and &#x27;batch_size&#x27;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 获得x和y</span></span><br><span class="line">        xb, yb = batch[<span class="number">0</span>], batch[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 将x输入模型，获得预测值</span></span><br><span class="line">        model_outputs = self.model(xb)</span><br><span class="line">        <span class="comment"># 计算损失值</span></span><br><span class="line">        loss = self.loss_func(model_outputs, yb)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回值包括损失、模型输出值和batch size</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&quot;loss&quot;</span>: loss,</span><br><span class="line">            <span class="string">&quot;model_outputs&quot;</span>: model_outputs,</span><br><span class="line">            <span class="string">&quot;batch_size&quot;</span>: yb.size(<span class="number">0</span>),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward_step</span>(<span class="params">self, loss</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Use the accelerator to perform the backward pass on the calculated value of the loss returned by :meth:`~Trainer.calculate_train_batch_loss`.</span></span><br><span class="line"><span class="string">        If gradient accumulation is enabled, this loss has been scaled by 1 / accumulation steps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param loss: The loss tensor returned by :meth:`~Trainer.calculate_train_batch_loss`.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self._accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">optimizer_step</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Performs a single optimization step and updates the parameters which have been passed to ``self.optimizer``.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scheduler_step</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Performs a single scheduler step if ``self.scheduler`` has been assigned.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.scheduler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.scheduler.step()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">optimizer_zero_grad</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Sets the gradients of all optimized ``torch.Tensor`` s to zero.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_epoch_end</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This method is called at the end of each training epoch.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_epoch_start</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This method is called at the start of an evaluation epoch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The default behaviour of this method is to call ``self.model.eval()``</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_eval_batch_loss</span>(<span class="params">self, batch</span>) -&gt; <span class="built_in">dict</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Calculates the evaluation loss and return this along with the batch size and model outputs.</span></span><br><span class="line"><span class="string">        Any additional values returned will be available in the :meth:`~callbacks.TrainerCallback.on_eval_step_end` callback.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param batch: the output of the eval dataloader</span></span><br><span class="line"><span class="string">        :return: A dictionary containing the evaluation loss, model outputs and batch size. Can include any keys, but must include the keys ``loss``, ``model_outputs`` and ``batch_size``</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            xb, yb = batch[<span class="number">0</span>], batch[<span class="number">1</span>]</span><br><span class="line">            model_outputs = self.model(xb)</span><br><span class="line">            val_loss = self.loss_func(model_outputs, yb)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&quot;loss&quot;</span>: val_loss,</span><br><span class="line">            <span class="string">&quot;model_outputs&quot;</span>: model_outputs,</span><br><span class="line">            <span class="string">&quot;batch_size&quot;</span>: yb.size(<span class="number">0</span>),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_epoch_end</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This method is called at the end of an evaluation epoch.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">training_run_epoch_end</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This method is called during a training run after both training and evaluation epochs have been completed.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">training_run_end</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This method is called at the end of a training run.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluation_run_start</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This method is called at the start of an evaluation run.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluation_run_end</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This method is called at the end of an evaluation run.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># train是Trainer的入口函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        train_dataset,</span></span></span><br><span class="line"><span class="function"><span class="params">        num_epochs,</span></span></span><br><span class="line"><span class="function"><span class="params">        eval_dataset=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        per_device_batch_size=<span class="number">8</span>, <span class="comment"># 默认每个设备上的batch size是8</span></span></span></span><br><span class="line"><span class="function"><span class="params">        max_num_train_steps=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        gradient_accumulation_steps=<span class="number">1</span>, <span class="comment"># 默认梯度累加步数为1</span></span></span></span><br><span class="line"><span class="function"><span class="params">        gradient_clip_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        create_scheduler_fn: <span class="type">Callable</span> = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        train_dataloader_kwargs: <span class="built_in">dict</span> = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        eval_dataloader_kwargs: <span class="built_in">dict</span> = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        reset_run_history=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        collate_fn=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        <span class="comment"># 传入训练集</span></span><br><span class="line">        self.train_dataset = train_dataset</span><br><span class="line">        <span class="comment"># 传入验证集，默认为None</span></span><br><span class="line">        self.eval_dataset = eval_dataset</span><br><span class="line">        <span class="comment"># 传入调度器，默认为None</span></span><br><span class="line">        <span class="comment"># 注意，这里不是传递一个学习率调度器的实例，而是传递一个能返回这样的实例的工厂函数。</span></span><br><span class="line">        self.create_scheduler_fn = create_scheduler_fn</span><br><span class="line">        <span class="comment"># 传入数据加载器所使用的collate函数，该函数指定如何整理样本以形成一个mini-batch的样本，默认为None，即使用默认的整理方法</span></span><br><span class="line">        <span class="comment"># https://zhuanlan.zhihu.com/p/361830892</span></span><br><span class="line">        self.collate_fn = collate_fn</span><br><span class="line">        <span class="comment"># 如果指定重置运行历史，则调用run_history的reset方法</span></span><br><span class="line">        <span class="comment"># 对于默认的InMemoryRunHistory()，具体就是做了：</span></span><br><span class="line">        <span class="comment"># （1）_current_epoch设为1，</span></span><br><span class="line">        <span class="comment"># （2）_metrics设为defaultdict(list)，这里用了defaultdict，是怕字典里没有key时报错，</span></span><br><span class="line">        <span class="comment"># 教程见：https://www.jianshu.com/p/bbd258f99fd3</span></span><br><span class="line">        <span class="keyword">if</span> reset_run_history:</span><br><span class="line">            self.run_history.reset()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 传入batch_size及训练集数据加载器的参数，创建训练集dataloader</span></span><br><span class="line">        <span class="comment"># 接下来会被替换成被accelerate封装后的加载器</span></span><br><span class="line">        self._train_dataloader = self.create_train_dataloader(</span><br><span class="line">            batch_size=per_device_batch_size, train_dl_kwargs=train_dataloader_kwargs</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果明确指定了验证集后，则以与上面训练集dataloader同样的方式创建验证集的dataloader</span></span><br><span class="line">        <span class="comment"># 两者区别是验证集的默认的shuffle是False</span></span><br><span class="line">        <span class="keyword">if</span> self.eval_dataset <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 接下来它也会被替换成被accelerate封装后的加载器</span></span><br><span class="line">            self._eval_dataloader = self.create_eval_dataloader(</span><br><span class="line">                batch_size=per_device_batch_size, eval_dl_kwargs=eval_dataloader_kwargs</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将模型、优化器和dataloader放到accelerate上</span></span><br><span class="line">        self._prepare_model_optimizer_and_dataloaders()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 封装运行配置</span></span><br><span class="line">        self.run_config = self._create_run_config(</span><br><span class="line">            num_epochs=num_epochs,</span><br><span class="line">            gradient_accumulation_steps=gradient_accumulation_steps,</span><br><span class="line">            max_num_train_steps=max_num_train_steps,</span><br><span class="line">            per_device_batch_size=per_device_batch_size,</span><br><span class="line">            gradient_clip_value=gradient_clip_value,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建调度器实例</span></span><br><span class="line">        <span class="keyword">if</span> self.create_scheduler_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.scheduler = self.create_scheduler()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 开始训练</span></span><br><span class="line">        self._run_training()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        dataset=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        per_device_batch_size=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        dataloader_kwargs: <span class="built_in">dict</span> = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        collate_fn=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Start an evaluation run.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. Note:: Starting an evaluation run will reset the :class:`Trainer`&#x27;s run history.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. Note:: During distributed evaluation, if the `per_device_batch_size` * the number of processes used does not exactly divide the dataset, and `drop_last=False` has not been passed as a dataloader kwarg, the dataloader will repeat from the start in processes that run out of batches. This should be taken into consideration when calculating metrics.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param dataset: the dataset to use during evaluation</span></span><br><span class="line"><span class="string">        :param per_device_batch_size: the batch size to use per device</span></span><br><span class="line"><span class="string">        :param dataloader_kwargs: a dictionary of keyword arguments to pass to the dataloader constructor, for details see :class:`torch.utils.data.DataLoader`</span></span><br><span class="line"><span class="string">        :param collate_fn: the collate function to be used by the dataloader</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.eval_dataset = dataset</span><br><span class="line">        self.collate_fn = collate_fn</span><br><span class="line"></span><br><span class="line">        self.run_history.reset()</span><br><span class="line"></span><br><span class="line">        self._train_dataloader = <span class="literal">None</span></span><br><span class="line">        self._eval_dataloader = self.create_eval_dataloader(</span><br><span class="line">            batch_size=per_device_batch_size, eval_dl_kwargs=dataloader_kwargs</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self._prepare_model_optimizer_and_dataloaders()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.run_config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.run_config = self._create_run_config(</span><br><span class="line">                num_epochs=<span class="number">1</span>,</span><br><span class="line">                gradient_accumulation_steps=<span class="number">0</span>,</span><br><span class="line">                max_num_train_steps=<span class="literal">None</span>,</span><br><span class="line">                per_device_batch_size=per_device_batch_size,</span><br><span class="line">                gradient_clip_value=<span class="literal">None</span>,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self._run_evaluation()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_default_train_dl_kwargs</span>(<span class="params">self, batch_size</span>) -&gt; <span class="built_in">dict</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Return the default arguments that will be used by the training dataloader.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param batch_size: the batch size to use during training</span></span><br><span class="line"><span class="string">        :return: a dictionary containing the default arguments for the training dataloader</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&quot;shuffle&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">            <span class="string">&quot;pin_memory&quot;</span>: <span class="literal">True</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="literal">False</span>,</span><br><span class="line">            <span class="string">&quot;batch_size&quot;</span>: batch_size,</span><br><span class="line">            <span class="string">&quot;num_workers&quot;</span>: <span class="built_in">max</span>(</span><br><span class="line">                os.cpu_count() // torch.cuda.device_count()</span><br><span class="line">                <span class="keyword">if</span> torch.cuda.is_available()</span><br><span class="line">                <span class="keyword">else</span> os.cpu_count(),</span><br><span class="line">                <span class="number">1</span>,</span><br><span class="line">            ),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_default_eval_dl_kwargs</span>(<span class="params">self, batch_size</span>) -&gt; <span class="built_in">dict</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Return the default arguments that will be used by the evaluation dataloader.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param batch_size: the batch size to use during evaluation</span></span><br><span class="line"><span class="string">        :return: a dictionary containing the default arguments for the evaluation dataloader</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&quot;shuffle&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">            <span class="string">&quot;pin_memory&quot;</span>: <span class="literal">True</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="literal">False</span>,</span><br><span class="line">            <span class="string">&quot;batch_size&quot;</span>: batch_size,</span><br><span class="line">            <span class="string">&quot;num_workers&quot;</span>: <span class="built_in">max</span>(</span><br><span class="line">                os.cpu_count() // torch.cuda.device_count()</span><br><span class="line">                <span class="keyword">if</span> torch.cuda.is_available()</span><br><span class="line">                <span class="keyword">else</span> os.cpu_count(),</span><br><span class="line">                <span class="number">1</span>,</span><br><span class="line">            ),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">device</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Use the internal instance of :class:`accelerate.Accelerator` to get the appropriate device</span></span><br><span class="line"><span class="string">        :return: an instance of `torch.device`</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self._accelerator.device</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prepare_model_optimizer_and_dataloaders</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        使用`accelerate.Accelerator`将模型、优化器和数据加载器包裹在任何训练所需的包装器中，并确保参数被放置在适当的设备上。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self._accelerator.free_memory()</span><br><span class="line">        self._accelerator = self._create_accelerator()</span><br><span class="line"></span><br><span class="line">        components = [self.model, self.optimizer]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._train_dataloader <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            components.append(self._train_dataloader)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._eval_dataloader <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            components.append(self._eval_dataloader)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 准备与训练相关的对象（优化器、模型、训练集dataloader、验证集dataloader）</span></span><br><span class="line">        <span class="comment"># 这使得这些东西做好训练的准备</span></span><br><span class="line">        prepared_components = self._accelerator.prepare(*components)</span><br><span class="line"></span><br><span class="line">        self.model = prepared_components[<span class="number">0</span>]</span><br><span class="line">        self.optimizer = prepared_components[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这个if和elif会区分是训练阶段还是测试阶段</span></span><br><span class="line">        <span class="comment"># 训练数据加载器将在所有可用的GPU/TPU核中上进行分片，以便每个核看到训练数据集的不同部分。此外，所有进程的随机状态将在每次迭代开始时在dataloader中进行同步，以确保数据以相同的方式被打乱（如果决定使用shuffle=True或任何种类的随机采样器）。</span></span><br><span class="line">        <span class="comment"># 训练的实际批次大小将是所使用的设备数量乘以在脚本中设置的批次大小：例如，在4个GPU上训练，在创建训练数据加载器时设置的批次大小为16，则实际训练的批次大小为64。另外，可以在创建Accelerator时使用split_batches=True选项，在这种情况下，无论在1、2、4还是64个GPU上运行脚本，批次大小都会保持一致。</span></span><br><span class="line">        <span class="keyword">if</span> self._train_dataloader <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self._train_dataloader = prepared_components[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> self._eval_dataloader <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self._eval_dataloader = prepared_components[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self._eval_dataloader <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self._eval_dataloader = prepared_components[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建运行配置</span></span><br><span class="line">    <span class="comment"># 将运行配置集合在一个地方</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_run_config</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        per_device_batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">        num_epochs,</span></span></span><br><span class="line"><span class="function"><span class="params">        gradient_accumulation_steps,</span></span></span><br><span class="line"><span class="function"><span class="params">        max_num_train_steps,</span></span></span><br><span class="line"><span class="function"><span class="params">        gradient_clip_value,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>) -&gt; TrainerRunConfig:</span></span><br><span class="line">        <span class="comment"># 获得train_per_device_batch_size配置</span></span><br><span class="line">        <span class="keyword">if</span> self._train_dl_kwargs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># get()方法返回指定键的值，如果键不在字典中返回默认值 None 或者设置的默认值。</span></span><br><span class="line">            train_per_device_batch_size = self._train_dl_kwargs.get(</span><br><span class="line">                <span class="string">&quot;batch_size&quot;</span>, per_device_batch_size</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train_per_device_batch_size = per_device_batch_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得eval_per_device_batch_size配置</span></span><br><span class="line">        <span class="keyword">if</span> self._eval_dl_kwargs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            eval_per_device_batch_size = self._eval_dl_kwargs.get(</span><br><span class="line">                <span class="string">&quot;batch_size&quot;</span>, train_per_device_batch_size</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            eval_per_device_batch_size = train_per_device_batch_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得num_update_steps_per_epoch配置</span></span><br><span class="line">        <span class="keyword">if</span> self._train_dataloader <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 这个地方涉及梯度累加步数，关于梯度累加，一些参考教程见：</span></span><br><span class="line">            <span class="comment"># https://blog.kamino.link/2021/10/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A2%AF%E5%BA%A6%E7%B4%AF%E5%8A%A0Pytorch%E5%AE%9E%E7%8E%B0/</span></span><br><span class="line">            <span class="comment"># https://www.cnblogs.com/lart/p/11628696.html</span></span><br><span class="line">            <span class="comment"># https://zhuanlan.zhihu.com/p/351999133</span></span><br><span class="line"></span><br><span class="line">            num_update_steps_per_epoch = math.ceil(</span><br><span class="line">                <span class="built_in">len</span>(self._train_dataloader) / gradient_accumulation_steps</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            num_update_steps_per_epoch = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得max_num_train_steps配置</span></span><br><span class="line">        <span class="comment"># 如果未明确配置它，则根据其他参数计算</span></span><br><span class="line">        <span class="keyword">if</span> max_num_train_steps <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            max_num_train_steps = num_epochs * num_update_steps_per_epoch</span><br><span class="line">        <span class="comment"># 如果明确配置它了，则对num_epochs这个参数重新计算一下</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            num_epochs = math.ceil(max_num_train_steps / num_update_steps_per_epoch)</span><br><span class="line"></span><br><span class="line">        config = &#123;</span><br><span class="line">            <span class="string">&quot;num_epochs&quot;</span>: num_epochs,</span><br><span class="line">            <span class="string">&quot;train_per_device_batch_size&quot;</span>: train_per_device_batch_size,</span><br><span class="line">            <span class="string">&quot;train_dl_kwargs&quot;</span>: self._train_dl_kwargs,</span><br><span class="line">            <span class="string">&quot;eval_per_device_batch_size&quot;</span>: eval_per_device_batch_size,</span><br><span class="line">            <span class="string">&quot;eval_dl_kwargs&quot;</span>: self._eval_dl_kwargs,</span><br><span class="line">            <span class="string">&quot;gradient_accumulation_steps&quot;</span>: gradient_accumulation_steps,</span><br><span class="line">            <span class="string">&quot;train_total_batch_size&quot;</span>: train_per_device_batch_size</span><br><span class="line">            * self._accelerator.num_processes</span><br><span class="line">            * gradient_accumulation_steps,</span><br><span class="line">            <span class="string">&quot;eval_total_batch_size&quot;</span>: eval_per_device_batch_size</span><br><span class="line">            * self._accelerator.num_processes,</span><br><span class="line">            <span class="string">&quot;num_update_steps_per_epoch&quot;</span>: num_update_steps_per_epoch,</span><br><span class="line">            <span class="string">&quot;max_num_train_steps&quot;</span>: max_num_train_steps,</span><br><span class="line">            <span class="string">&quot;is_local_process_zero&quot;</span>: self._accelerator.is_local_main_process,</span><br><span class="line">            <span class="string">&quot;is_world_process_zero&quot;</span>: self._accelerator.is_main_process,</span><br><span class="line">            <span class="string">&quot;is_distributed&quot;</span>: <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> self._accelerator.distributed_type != DistributedType.NO</span><br><span class="line">            <span class="keyword">else</span> <span class="literal">False</span>,</span><br><span class="line">            <span class="string">&quot;mixed_precision&quot;</span>: self._accelerator.mixed_precision,</span><br><span class="line">            <span class="string">&quot;gradient_clip_value&quot;</span>: gradient_clip_value,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将所有配置封装进TrainerRunConfig类型中</span></span><br><span class="line">        <span class="keyword">return</span> TrainerRunConfig(**config)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_training</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 开始训练时调用一次该成员函数，当前该函数是空的</span></span><br><span class="line">        self.training_run_start()</span><br><span class="line">        <span class="comment"># 触发&quot;on_training_run_start&quot;这一回调事件</span></span><br><span class="line">        <span class="comment"># 比如MoveModulesToDeviceCallback、PrintProgressCallback这两个回调就拥有该事件属性</span></span><br><span class="line">        <span class="comment"># 从而能够在训练一开始就做一些事情</span></span><br><span class="line">        self.callback_handler.call_event(</span><br><span class="line">            <span class="string">&quot;on_training_run_start&quot;</span>,</span><br><span class="line">            self,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对epoch进行循环</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(self.run_config.num_epochs):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment"># 每一个训练epoch期间运行所做的事情</span></span><br><span class="line">                self._run_train_epoch(self._train_dataloader)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 如果指定了验证集，则运行验证epoch</span></span><br><span class="line">                <span class="comment"># 其基本流程与train epoch类似，但少了训练过程</span></span><br><span class="line">                <span class="keyword">if</span> self.eval_dataset <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    self._run_eval_epoch(self._eval_dataloader)</span><br><span class="line">                <span class="comment"># 对epoch进行步进</span></span><br><span class="line">                self.run_history._increment_epoch()</span><br><span class="line">                <span class="comment"># 每个epoch结束后调用</span></span><br><span class="line">                self.training_run_epoch_end()</span><br><span class="line">                <span class="comment"># 每个epoch结束后触发on_training_run_epoch_end事件</span></span><br><span class="line">                <span class="comment"># 默认的回调中没有该事件</span></span><br><span class="line">                <span class="comment"># 不过该事件非常重要，在SaveBestModelCallback中有使用，用来保存最佳模型；以及在EarlyStoppingCallback中使用，用来提前终止训练</span></span><br><span class="line">                self.callback_handler.call_event(</span><br><span class="line">                    <span class="string">&quot;on_training_run_epoch_end&quot;</span>,</span><br><span class="line">                    self,</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">except</span> StopTrainingError <span class="keyword">as</span> e:</span><br><span class="line">                self._accelerator.<span class="built_in">print</span>(e)</span><br><span class="line">                self.callback_handler.call_event(</span><br><span class="line">                    <span class="string">&quot;on_stop_training_error&quot;</span>,</span><br><span class="line">                    self,</span><br><span class="line">                )</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 整个训练结束后调用</span></span><br><span class="line">        self.training_run_end()</span><br><span class="line">        <span class="comment"># 整个训练结束后触发on_training_run_end事件</span></span><br><span class="line">        <span class="comment"># 默认的回调中，PrintProgressCallback有该事件属性，会打印出训练结束的字符串。</span></span><br><span class="line">        self.callback_handler.call_event(</span><br><span class="line">            <span class="string">&quot;on_training_run_end&quot;</span>,</span><br><span class="line">            self,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_evaluation</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        The method responsible for the orchestration of the high level steps which will be executed during an evaluation run.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.evaluation_run_start()</span><br><span class="line">        self.callback_handler.call_event(</span><br><span class="line">            <span class="string">&quot;on_evaluation_run_start&quot;</span>,</span><br><span class="line">            self,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self._run_eval_epoch(self._eval_dataloader, is_training=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">except</span> StopTrainingError <span class="keyword">as</span> e:</span><br><span class="line">            self._accelerator.<span class="built_in">print</span>(e)</span><br><span class="line">            self.callback_handler.call_event(</span><br><span class="line">                <span class="string">&quot;on_stop_training_error&quot;</span>,</span><br><span class="line">                self,</span><br><span class="line">            )</span><br><span class="line">        self.evaluation_run_end()</span><br><span class="line">        self.callback_handler.call_event(</span><br><span class="line">            <span class="string">&quot;on_evaluation_run_end&quot;</span>,</span><br><span class="line">            self,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每一个训练epoch期间运行所做的事情</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_train_epoch</span>(<span class="params">self, train_dl</span>):</span></span><br><span class="line">        <span class="comment"># 将网络模型设置成train模式</span></span><br><span class="line">        self.train_epoch_start()</span><br><span class="line">        <span class="comment"># 将损失追踪器重置一下，即设置当前epoch为1，指标列表为空。</span></span><br><span class="line">        self._loss_tracker.reset()</span><br><span class="line">        <span class="comment"># 触发&quot;on_train_epoch_start&quot;事件</span></span><br><span class="line">        <span class="comment"># 默认的回调函数中有如下几个有该事件属性，比如：</span></span><br><span class="line">        <span class="comment"># PrintProgressCallback：每个epoch开始都输出一下当前epoch是多少</span></span><br><span class="line">        <span class="comment"># ProgressBarCallback：每个epoch开始时初始化一个进度条</span></span><br><span class="line">        self.callback_handler.call_event(</span><br><span class="line">            <span class="string">&quot;on_train_epoch_start&quot;</span>,</span><br><span class="line">            self,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 进入对batch的循环，对每个batch的运行称为一个step</span></span><br><span class="line">        <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dl):</span><br><span class="line">            <span class="comment"># 每一步开始之前触发on_train_step_start事件，默认的回调中没有该事件的定义</span></span><br><span class="line">            self.callback_handler.call_event(</span><br><span class="line">                <span class="string">&quot;on_train_step_start&quot;</span>,</span><br><span class="line">                self,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 判断是否达到了梯度累加的步数，或者到了数据集的最后</span></span><br><span class="line">            perform_gradient_update = (</span><br><span class="line">                (step + <span class="number">1</span>) % self.run_config.gradient_accumulation_steps == <span class="number">0</span></span><br><span class="line">            ) <span class="keyword">or</span> (step + <span class="number">1</span> == <span class="built_in">len</span>(train_dl))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果没有达到梯度累加的步数</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> perform_gradient_update:</span><br><span class="line">                <span class="comment"># 那么就在不同的进程中关闭梯度同步</span></span><br><span class="line">                <span class="keyword">with</span> self._accelerator.no_sync(self.model):</span><br><span class="line">                    self._perform_forward_and_backward_passes(batch)</span><br><span class="line">            <span class="comment"># 如果达到梯度累加的步数，则会进行梯度同步</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self._perform_forward_and_backward_passes(batch)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果设定了梯度裁剪阈值，则进行梯度裁剪</span></span><br><span class="line">            <span class="keyword">if</span> self.run_config.gradient_clip_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self._clip_gradients()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果达到了梯度累加</span></span><br><span class="line">            <span class="keyword">if</span> perform_gradient_update:</span><br><span class="line">                <span class="comment"># 优化器更新参数</span></span><br><span class="line">                self.optimizer_step()</span><br><span class="line">                <span class="comment"># 如果设定了学习率调度器，则调用调度器一次</span></span><br><span class="line">                <span class="keyword">if</span> (</span><br><span class="line">                    self.scheduler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">                    <span class="keyword">and</span> <span class="keyword">not</span> self._accelerator.optimizer_step_was_skipped</span><br><span class="line">                ):</span><br><span class="line">                    self.scheduler_step()</span><br><span class="line">                <span class="comment"># 梯度清零</span></span><br><span class="line">                self.optimizer_zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个epoch结束后调用如下方法，当前其为空</span></span><br><span class="line">        self.train_epoch_end()</span><br><span class="line">        <span class="comment"># 使用损失追踪器中的平均损失来更新运行历史中的指标</span></span><br><span class="line">        self.run_history.update_metric(<span class="string">&quot;train_loss_epoch&quot;</span>, self._loss_tracker.average)</span><br><span class="line">        <span class="comment"># 每个epoch结束后触发on_train_epoch_end事件</span></span><br><span class="line">        <span class="comment"># 默认的回调中，有如下拥有该事件属性：</span></span><br><span class="line">        <span class="comment"># ProgressBarCallback：用来关闭进度条</span></span><br><span class="line">        <span class="comment"># LogMetricsCallback：输出训练损失</span></span><br><span class="line">        self.callback_handler.call_event(</span><br><span class="line">            <span class="string">&quot;on_train_epoch_end&quot;</span>,</span><br><span class="line">            self,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算前向传播和反向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_perform_forward_and_backward_passes</span>(<span class="params">self, batch</span>):</span></span><br><span class="line">        <span class="comment"># 计算训练损失</span></span><br><span class="line">        batch_output = self.calculate_train_batch_loss(batch)</span><br><span class="line">        <span class="comment"># 如果梯度累加步数大于1，进行损失标准化，教程参见上面的梯度累加的参考文献</span></span><br><span class="line">        <span class="keyword">if</span> self.run_config.gradient_accumulation_steps &gt; <span class="number">1</span>:</span><br><span class="line">            batch_output[<span class="string">&quot;loss&quot;</span>] /= self.run_config.gradient_accumulation_steps</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过聚合所有进程上的损失值，更新损失追踪器，</span></span><br><span class="line">        <span class="comment"># 包括当前batch上的损失、总损失、已运行的总样本数、平均损失</span></span><br><span class="line">        self._loss_tracker.update(</span><br><span class="line">            self.gather(batch_output[<span class="string">&quot;loss&quot;</span>]).detach().mean().item(),</span><br><span class="line">            batch_output[<span class="string">&quot;batch_size&quot;</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在每一步结束时触发on_train_step_end事件</span></span><br><span class="line">        <span class="comment"># 默认的回调中ProgressBarCallback有该事件属性，做的动作是更新进度条</span></span><br><span class="line">        self.callback_handler.call_event(</span><br><span class="line">            <span class="string">&quot;on_train_step_end&quot;</span>, self, batch_output=batch_output, batch=batch</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 进行反向传播</span></span><br><span class="line">        self.backward_step(batch_output[<span class="string">&quot;loss&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_clip_gradients</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Clip the gradients of the model&#x27;s parameters that fall outside of the threshold specified in :meth:`~Trainer.train`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        By default, this clips the gradients using :meth:`accelerate.Accelerator.clip_grad_value_`</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self._accelerator.clip_grad_value_(</span><br><span class="line">            self.model.parameters(), clip_value=self.run_config.gradient_clip_value</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_eval_epoch</span>(<span class="params">self, valid_dl, is_training: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        The method responsible for the behaviour of each evaluation epoch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param valid_dl: the dataloader to be used during evaluation</span></span><br><span class="line"><span class="string">        :param is_training: signals whether the evaluation is being run as part of a training run</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.eval_epoch_start()</span><br><span class="line">        self._loss_tracker.reset()</span><br><span class="line">        self.callback_handler.call_event(</span><br><span class="line">            <span class="string">&quot;on_eval_epoch_start&quot;</span>,</span><br><span class="line">            self,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> valid_dl:</span><br><span class="line">            self.callback_handler.call_event(</span><br><span class="line">                <span class="string">&quot;on_eval_step_start&quot;</span>,</span><br><span class="line">                self,</span><br><span class="line">            )</span><br><span class="line">            batch_output = self.calculate_eval_batch_loss(batch)</span><br><span class="line">            self._loss_tracker.update(</span><br><span class="line">                self.gather(batch_output[<span class="string">&quot;loss&quot;</span>]).detach().mean().item(),</span><br><span class="line">                batch_output[<span class="string">&quot;batch_size&quot;</span>],</span><br><span class="line">            )</span><br><span class="line">            self.callback_handler.call_event(</span><br><span class="line">                <span class="string">&quot;on_eval_step_end&quot;</span>, self, batch_output=batch_output, batch=batch</span><br><span class="line">            )</span><br><span class="line">        self.eval_epoch_end()</span><br><span class="line">        metric_name = <span class="string">&quot;eval_loss_epoch&quot;</span> <span class="keyword">if</span> is_training <span class="keyword">else</span> <span class="string">&quot;evaluation_loss&quot;</span></span><br><span class="line">        self.run_history.update_metric(metric_name, self._loss_tracker.average)</span><br><span class="line">        self.callback_handler.call_event(</span><br><span class="line">            <span class="string">&quot;on_eval_epoch_end&quot;</span>,</span><br><span class="line">            self,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对在不同进程上的tensor进行聚合</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gather</span>(<span class="params">self, tensor</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Gather the values in `tensor` across all processes and concatenate them on the first dimension. This can be</span></span><br><span class="line"><span class="string">        useful to regroup the predictions from all processes when doing evaluation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. Note:: This gather happens in all processes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param tensor: (:obj:`torch.Tensor`, or a nested tuple/list/dictionary of :obj:`torch.Tensor`) The tensors to gather across all processes.</span></span><br><span class="line"><span class="string">        :return: The gathered tensor(s) (:obj:`torch.Tensor`, or a nested tuple/list/dictionary of :obj:`torch.Tensor`). The first dimension of the result is `num_processes` multiplied by the first dimension of the input tensors.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self._accelerator.gather(tensor)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print</span>(<span class="params">self, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Use in replacement of ``print()`` to only print once per node.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self._accelerator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self._accelerator.<span class="built_in">print</span>(*args, **kwargs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_checkpoint</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self, save_path, checkpoint_kwargs=<span class="literal">None</span>, save_optimizer=<span class="literal">True</span>, save_per_node=<span class="literal">True</span></span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Save the model, optimizer and specified args as a checkpoint file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param save_path: the path where to save the checkpoint, this should end in &#x27;.pt&#x27;</span></span><br><span class="line"><span class="string">        :param checkpoint_kwargs: additional objects to include in the checkpoint</span></span><br><span class="line"><span class="string">        :param save_optimizer: flag to indicate whether to include the optimizer in the checkpoint</span></span><br><span class="line"><span class="string">        :param save_per_node: flag to indicate whether to save the checkpoint once per machine, if False, the checkpoint will only be saved from the world process zero. This is True by default.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> add save method for run history?</span></span><br><span class="line"></span><br><span class="line">        checkpoint = &#123;</span><br><span class="line">            <span class="string">&quot;model_state_dict&quot;</span>: self._accelerator.unwrap_model(self.model).state_dict(),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> save_optimizer:</span><br><span class="line">            checkpoint[<span class="string">&quot;optimizer_state_dict&quot;</span>] = self.optimizer.state_dict()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> checkpoint_kwargs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            checkpoint.update(checkpoint_kwargs)</span><br><span class="line"></span><br><span class="line">        self._accelerator.wait_for_everyone()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> save_per_node:</span><br><span class="line"></span><br><span class="line">            self._accelerator.save(</span><br><span class="line">                checkpoint,</span><br><span class="line">                save_path,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.run_config.is_world_process_zero:</span><br><span class="line">                self._accelerator.save(</span><br><span class="line">                    checkpoint,</span><br><span class="line">                    save_path,</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_checkpoint</span>(<span class="params">self, checkpoint_path, load_optimizer=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Load the model and optimizer from a checkpoint file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param checkpoint_path: the path of the checkpoint file to load</span></span><br><span class="line"><span class="string">        :param load_optimizer: flag to indicate whether to load the optimizer if it is included in the checkpoint</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self._accelerator.wait_for_everyone()</span><br><span class="line">        checkpoint = torch.load(checkpoint_path, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">        self._accelerator.unwrap_model(self.model).load_state_dict(</span><br><span class="line">            checkpoint[<span class="string">&quot;model_state_dict&quot;</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> load_optimizer <span class="keyword">and</span> <span class="string">&quot;optimizer_state_dict&quot;</span> <span class="keyword">in</span> checkpoint:</span><br><span class="line">            <span class="keyword">if</span> self.optimizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">&quot;You are trying to load an optimizer from a checkpoint, but no optimizer&quot;</span></span><br><span class="line">                    <span class="string">&quot;has been set in the Trainer. Either pass the correct optimizer instance when&quot;</span></span><br><span class="line">                    <span class="string">&quot;creating the trainer, or specify load_optimizer=False when loading the checkpoint.&quot;</span></span><br><span class="line">                )</span><br><span class="line">            self.optimizer.load_state_dict(checkpoint[<span class="string">&quot;optimizer_state_dict&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainerWithTimmScheduler</span>(<span class="params">Trainer</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Subclass of the :class:`Trainer` that works with `timm schedulers &lt;https://fastai.github.io/timmdocs/schedulers&gt;`_ instead</span></span><br><span class="line"><span class="string">    of standard PyTorch learning rate schedulers</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">        self.num_updates = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_epoch_start</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().train_epoch_start()</span><br><span class="line">        self.num_updates = self.run_history.current_epoch * <span class="built_in">len</span>(self._train_dataloader)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_epoch_end</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.scheduler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.scheduler.step(self.run_history.current_epoch + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scheduler_step</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.num_updates += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> self.scheduler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.scheduler.step_update(num_updates=self.num_updates)</span><br></pre></td></tr></table></figure>
<h1 id="对PyTorch迁移学习案例的加速改造"><a href="#对PyTorch迁移学习案例的加速改造" class="headerlink" title="对PyTorch迁移学习案例的加速改造"></a>对PyTorch迁移学习案例的加速改造</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Modifications Copyright © 2021 Chris Hughes</span></span><br><span class="line"><span class="comment">########################################################################</span></span><br><span class="line"><span class="comment"># 这个例子是PyTorch迁移学习的官方教程&quot;Transfer Learning for Computer Vision Tutorial&quot;（作者Sasank Chilamkurthy）的&quot;加速&quot;版本，原文在这里:</span></span><br><span class="line"><span class="comment"># https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html</span></span><br><span class="line"><span class="comment">########################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 命令行参数模块，参考教程见：</span></span><br><span class="line"><span class="comment"># https://docs.python.org/zh-cn/3/howto/argparse.html</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 偏函数，用来固定参数的默认值，参考教程见：</span></span><br><span class="line"><span class="comment"># https://www.liaoxuefeng.com/wiki/1016959663602400/1017454145929440</span></span><br><span class="line"><span class="comment"># https://zhuanlan.zhihu.com/p/47124891</span></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch原生的神经网络模块和优化器</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="comment"># torch原生的学习率调度器</span></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="comment"># 使用torchvision的变换、数据集和模型</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets, models</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>, <span class="string">&#x27;../../../&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># pytorch-accelerated的训练器</span></span><br><span class="line"><span class="keyword">from</span> pytorch_accelerated.trainer <span class="keyword">import</span> Trainer, TrainerPlaceholderValues</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">data_dir</span>):</span></span><br><span class="line">    <span class="comment"># 数据变换</span></span><br><span class="line">    data_transforms = &#123;</span><br><span class="line">        <span class="comment"># 对于训练集，使用随机裁剪、翻转等数据增强和标准化</span></span><br><span class="line">        <span class="string">&quot;train&quot;</span>: transforms.Compose(</span><br><span class="line">            [</span><br><span class="line">                transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">                transforms.RandomHorizontalFlip(),</span><br><span class="line">                transforms.ToTensor(),</span><br><span class="line">                transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]),</span><br><span class="line">            ]</span><br><span class="line">        ),</span><br><span class="line">        <span class="comment"># 对于训练集，仅使用标准化</span></span><br><span class="line">        <span class="string">&quot;val&quot;</span>: transforms.Compose(</span><br><span class="line">            [</span><br><span class="line">                transforms.Resize(<span class="number">256</span>),</span><br><span class="line">                transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                transforms.ToTensor(),</span><br><span class="line">                transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]),</span><br><span class="line">            ]</span><br><span class="line">        ),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建数据集</span></span><br><span class="line">    <span class="comment"># 采用的是torchvision的datasets.ImageFolder</span></span><br><span class="line">    <span class="comment"># 字典推导式语法，参考教程见：https://www.runoob.com/python3/python-comprehensions.html</span></span><br><span class="line">    <span class="comment"># 使用的数据集是hymenoptera_data，下载地址在：</span></span><br><span class="line">    <span class="comment"># https://download.pytorch.org/tutorial/hymenoptera_data.zip</span></span><br><span class="line">    image_datasets = &#123;</span><br><span class="line">        x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&quot;train&quot;</span>, <span class="string">&quot;val&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 创建模型</span></span><br><span class="line">    <span class="comment"># 采用的是torchvision的models，以及下载预训练权重</span></span><br><span class="line">    model = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 将模型的分类器修改一下，适用于本例</span></span><br><span class="line">    model.fc = nn.Linear(model.fc.in_features, <span class="built_in">len</span>(image_datasets[<span class="string">&quot;train&quot;</span>].classes))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义损失函数</span></span><br><span class="line">    loss_func = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用torch自己的优化器</span></span><br><span class="line">    optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对于学习率调度器，仍然采用torch原生的调度器</span></span><br><span class="line">    <span class="comment"># 但这里将它用在pytorch-accelerated的Trainer时要经过修改</span></span><br><span class="line">    <span class="comment"># Trainer是在step级别（即对batch进行循环）上调用调度器，而不是torch原生的StepLR那样在epoch级别上调用</span></span><br><span class="line">    <span class="comment"># 比如，torch原生的StepLR是这样调用的：</span></span><br><span class="line">    <span class="comment"># &gt;&gt;&gt; # Assuming optimizer uses lr = 0.05 for all groups</span></span><br><span class="line">    <span class="comment"># &gt;&gt;&gt; # lr = 0.05     if epoch &lt; 30</span></span><br><span class="line">    <span class="comment"># &gt;&gt;&gt; # lr = 0.005    if 30 &lt;= epoch &lt; 60</span></span><br><span class="line">    <span class="comment"># &gt;&gt;&gt; # lr = 0.0005   if 60 &lt;= epoch &lt; 90</span></span><br><span class="line">    <span class="comment"># &gt;&gt;&gt; # ...</span></span><br><span class="line">    <span class="comment"># &gt;&gt;&gt; scheduler = StepLR(optimizer, step_size=30, gamma=0.1)</span></span><br><span class="line">    <span class="comment"># &gt;&gt;&gt; for epoch in range(100):</span></span><br><span class="line">    <span class="comment"># &gt;&gt;&gt;     train(...)</span></span><br><span class="line">    <span class="comment"># &gt;&gt;&gt;     validate(...)</span></span><br><span class="line">    <span class="comment"># &gt;&gt;&gt;     scheduler.step()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 但是，在pytorch-accelerated中，看源码的话就会发现，它的scheduler.step()是在如下地方：</span></span><br><span class="line">    <span class="comment">#     # 进入对batch的循环，对每个batch的运行称为一个step</span></span><br><span class="line">    <span class="comment">#     for step, batch in enumerate(train_dl):</span></span><br><span class="line">    <span class="comment">#         .....</span></span><br><span class="line">    <span class="comment">#         # 如果达到了梯度累加</span></span><br><span class="line">    <span class="comment">#         if perform_gradient_update:</span></span><br><span class="line">    <span class="comment">#             .......</span></span><br><span class="line">    <span class="comment">#             # 如果设定了学习率调度器，则调用调度器一次</span></span><br><span class="line">    <span class="comment">#             if (</span></span><br><span class="line">    <span class="comment">#                 self.scheduler is not None</span></span><br><span class="line">    <span class="comment">#                 and not self._accelerator.optimizer_step_was_skipped</span></span><br><span class="line">    <span class="comment">#             ):</span></span><br><span class="line">    <span class="comment">#                 self.scheduler_step()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所以，原来的step_size在epoch层级假设为30个epoch，那么此时在step层级的step_size就要变为30*steps_per_epoch，才能得到同样的调用效果。（这是对于Pytorch原生的调度器，其他的调度器则要看具体情况）</span></span><br><span class="line">    <span class="comment"># 推测是这样调用的：对于原来的epoch层级的调用，在epoch层级进行循环，每个epoch都会调用它一次，此时step_size设为30，它内部会计数，当进行30次epoch循环后，就会更新一次；对于pytorch-accelerated的step层级的调用（假设每个epoch有5个batch，即5个step），首先是对于epoch进行循环，然后在每个epoch内部，再对step进行循环，每个step都会调用它一次，先假设step_size仍然是30，因为它内部会计数，那么当进行了6个epoch后它就会更新，这显然是错误的，所以为了达到以前的每30次epoch更新一次，step_size就不能再是30，而是变成30*5，即steps_per_epoch*原来的step_size。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 具体原理实现：exp_lr_schedular是一个partial实例，它会在replace_trainer_placeholder_values中被处理，它的参数，比如step_size会被TrainerPlaceholderValues中的占位符的实际计算值所赋值</span></span><br><span class="line">    <span class="comment"># 从而实现了实时更新。</span></span><br><span class="line">    exp_lr_scheduler = partial(</span><br><span class="line">        lr_scheduler.StepLR,</span><br><span class="line">        step_size=TrainerPlaceholderValues.NUM_UPDATE_STEPS_PER_EPOCH * <span class="number">7</span>,</span><br><span class="line">        gamma=<span class="number">0.1</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将模型、损失函数、优化器传入Trainer即可</span></span><br><span class="line">    trainer = Trainer(</span><br><span class="line">        model,</span><br><span class="line">        loss_func=loss_func,</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用train函数，设置训练集、验证集、epoch数、batch size和学习率调度器</span></span><br><span class="line">    trainer.train(</span><br><span class="line">        train_dataset=image_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">        eval_dataset=image_datasets[<span class="string">&quot;val&quot;</span>],</span><br><span class="line">        num_epochs=<span class="number">1</span>,</span><br><span class="line">        per_device_batch_size=<span class="number">4</span>,</span><br><span class="line">        create_scheduler_fn=exp_lr_scheduler,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">&quot;Simple example of training script.&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--data_dir&quot;</span>, required=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&quot;The data folder on disk.&quot;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    main(args.data_dir)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="渐进式调整大小的案例"><a href="#渐进式调整大小的案例" class="headerlink" title="渐进式调整大小的案例"></a>渐进式调整大小的案例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Copyright © 2021 Chris Hughes</span></span><br><span class="line"><span class="comment">########################################################################</span></span><br><span class="line"><span class="comment"># This example trains a ResNet50d on the Imagenette Dataset using progressive resizing</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: this example requires installing the torchmetrics and timm packages</span></span><br><span class="line"><span class="comment">########################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个例子使用了渐进式大小调整progressive resizing</span></span><br><span class="line"><span class="comment"># 该技术的一个讲解可以参考该例子：https://www.yanxishe.com/TextTranslation/1614</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># python的具名元组，见：https://www.runoob.com/note/25726</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> timm <span class="keyword">import</span> create_model</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment"># 使用torch原生的OneCycleLR，该调度器介绍参考：https://zhuanlan.zhihu.com/p/387162205</span></span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> OneCycleLR</span><br><span class="line"><span class="comment"># 额外添加精度这一指标</span></span><br><span class="line"><span class="keyword">from</span> torchmetrics <span class="keyword">import</span> Accuracy</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入内置的回调事件</span></span><br><span class="line"><span class="keyword">from</span> pytorch_accelerated.callbacks <span class="keyword">import</span> (</span><br><span class="line">    TerminateOnNaNCallback,</span><br><span class="line">    LogMetricsCallback,</span><br><span class="line">    PrintProgressCallback,</span><br><span class="line">    EarlyStoppingCallback,</span><br><span class="line">    SaveBestModelCallback,</span><br><span class="line">    TrainerCallback,</span><br><span class="line">    ProgressBarCallback,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pytorch_accelerated.trainer <span class="keyword">import</span> Trainer, TrainerPlaceholderValues</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个新的回调来计算精度</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AccuracyCallback</span>(<span class="params">TrainerCallback</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes</span>):</span></span><br><span class="line">        self.accuracy = Accuracy(num_classes=num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在训练触发时将精度变量放到正确的设备上</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_training_run_start</span>(<span class="params">self, trainer, **kwargs</span>):</span></span><br><span class="line">        self.accuracy.to(trainer._eval_dataloader.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在每一个验证步结束时更新精度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_eval_step_end</span>(<span class="params">self, trainer, batch, batch_output, **kwargs</span>):</span></span><br><span class="line">        preds = batch_output[<span class="string">&quot;model_outputs&quot;</span>].argmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.accuracy.update(preds, batch[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在每一个验证epoch结束时更新精度指标，并重置</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_eval_epoch_end</span>(<span class="params">self, trainer, **kwargs</span>):</span></span><br><span class="line">        trainer.run_history.update_metric(<span class="string">&quot;accuracy&quot;</span>, self.accuracy.compute().item())</span><br><span class="line">        self.accuracy.reset()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_transforms</span>(<span class="params">train_image_size=<span class="number">224</span>, val_image_size=<span class="number">224</span></span>):</span></span><br><span class="line">    <span class="comment"># Data augmentation and normalization for training</span></span><br><span class="line">    <span class="comment"># Just normalization for validation</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: transforms.Compose(</span><br><span class="line">            [</span><br><span class="line">                transforms.RandomResizedCrop(train_image_size),</span><br><span class="line">                transforms.RandomHorizontalFlip(),</span><br><span class="line">                transforms.ToTensor(),</span><br><span class="line">                transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]),</span><br><span class="line">            ]</span><br><span class="line">        ),</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: transforms.Compose(</span><br><span class="line">            [</span><br><span class="line">                transforms.Resize(<span class="built_in">int</span>(<span class="built_in">round</span>(<span class="number">1.15</span> * val_image_size))),</span><br><span class="line">                transforms.CenterCrop(val_image_size),</span><br><span class="line">                transforms.ToTensor(),</span><br><span class="line">                transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]),</span><br><span class="line">            ]</span><br><span class="line">        ),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">data_dir</span>):</span></span><br><span class="line"></span><br><span class="line">    data_dir = Path(data_dir)</span><br><span class="line">    num_classes = <span class="built_in">len</span>(<span class="built_in">list</span>((data_dir / <span class="string">&quot;train&quot;</span>).iterdir()))</span><br><span class="line"></span><br><span class="line">    model = create_model(<span class="string">&quot;resnet50d&quot;</span>, pretrained=<span class="literal">False</span>, num_classes=num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define loss function</span></span><br><span class="line">    loss_func = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define optimizer</span></span><br><span class="line">    optimizer = torch.optim.Adam(params=model.parameters(), lr=<span class="number">0.01</span> / <span class="number">25</span>)</span><br><span class="line"></span><br><span class="line">    trainer = Trainer(</span><br><span class="line">        model,</span><br><span class="line">        loss_func=loss_func,</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        callbacks=(</span><br><span class="line">            TerminateOnNaNCallback,</span><br><span class="line">            AccuracyCallback(num_classes=num_classes),</span><br><span class="line">            PrintProgressCallback,</span><br><span class="line">            ProgressBarCallback,</span><br><span class="line">            LogMetricsCallback,</span><br><span class="line">            EarlyStoppingCallback(early_stopping_patience=<span class="number">2</span>),</span><br><span class="line">            SaveBestModelCallback(watch_metric=<span class="string">&quot;accuracy&quot;</span>, greater_is_better=<span class="literal">True</span>),</span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    EpochConfig = namedtuple(</span><br><span class="line">        <span class="string">&quot;EpochConfig&quot;</span>, [<span class="string">&quot;num_epochs&quot;</span>, <span class="string">&quot;train_image_size&quot;</span>, <span class="string">&quot;eval_image_size&quot;</span>, <span class="string">&quot;lr&quot;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置不同的图像尺寸</span></span><br><span class="line">    epoch_configs = [</span><br><span class="line">        EpochConfig(num_epochs=<span class="number">2</span>, train_image_size=<span class="number">64</span>, eval_image_size=<span class="number">64</span>, lr=<span class="number">0.01</span>),</span><br><span class="line">        EpochConfig(num_epochs=<span class="number">3</span>, train_image_size=<span class="number">128</span>, eval_image_size=<span class="number">128</span>, lr=<span class="number">0.01</span>),</span><br><span class="line">        EpochConfig(num_epochs=<span class="number">6</span>, train_image_size=<span class="number">224</span>, eval_image_size=<span class="number">224</span>, lr=<span class="number">0.001</span>),</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 渐进式调整图像大小</span></span><br><span class="line">    <span class="keyword">for</span> e_config <span class="keyword">in</span> epoch_configs:</span><br><span class="line">        trainer.<span class="built_in">print</span>(<span class="string">f&quot;Training with image size: <span class="subst">&#123;e_config.train_image_size&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        image_datasets = &#123;</span><br><span class="line">            x: datasets.ImageFolder(</span><br><span class="line">                os.path.join(data_dir, x),</span><br><span class="line">                create_transforms(</span><br><span class="line">                    train_image_size=e_config.train_image_size,</span><br><span class="line">                    val_image_size=e_config.eval_image_size,</span><br><span class="line">                )[x],</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&quot;train&quot;</span>, <span class="string">&quot;val&quot;</span>]</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Here we use placeholders for the number of epochs and number of steps per epoch, so that the</span></span><br><span class="line">        <span class="comment"># trainer can inject those values later. This is especially key for the number of update steps</span></span><br><span class="line">        <span class="comment"># which will change depending on whether training is distributed or not</span></span><br><span class="line">        lr_scheduler = partial(</span><br><span class="line">            OneCycleLR,</span><br><span class="line">            max_lr=e_config.lr,</span><br><span class="line">            epochs=TrainerPlaceholderValues.NUM_EPOCHS,</span><br><span class="line">            steps_per_epoch=TrainerPlaceholderValues.NUM_UPDATE_STEPS_PER_EPOCH,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        trainer.train(</span><br><span class="line">            train_dataset=image_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">            eval_dataset=image_datasets[<span class="string">&quot;val&quot;</span>],</span><br><span class="line">            num_epochs=e_config.num_epochs,</span><br><span class="line">            create_scheduler_fn=lr_scheduler,</span><br><span class="line">            per_device_batch_size=<span class="number">32</span>,</span><br><span class="line">            reset_run_history=<span class="literal">False</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">&quot;Simple example of training script.&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--data_dir&quot;</span>, required=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&quot;The data folder on disk.&quot;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    main(args.data_dir)</span><br></pre></td></tr></table></figure>
    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat_reward.png" alt="Xin-Bo Qi(亓欣波) WeChat Pay">
        <p>WeChat Pay</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/06/25/timm/" rel="prev" title="PyTorch图像模型库timm解析">
      <i class="fa fa-chevron-left"></i> PyTorch图像模型库timm解析
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Trainer%E9%80%90%E8%A1%8C%E4%BB%A3%E7%A0%81%E6%B3%A8%E8%A7%A3"><span class="nav-number">2.</span> <span class="nav-text">Trainer逐行代码注解</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AF%B9PyTorch%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E7%9A%84%E5%8A%A0%E9%80%9F%E6%94%B9%E9%80%A0"><span class="nav-number">3.</span> <span class="nav-text">对PyTorch迁移学习案例的加速改造</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B8%90%E8%BF%9B%E5%BC%8F%E8%B0%83%E6%95%B4%E5%A4%A7%E5%B0%8F%E7%9A%84%E6%A1%88%E4%BE%8B"><span class="nav-number">4.</span> <span class="nav-text">渐进式调整大小的案例</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xin-Bo Qi(亓欣波)"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Xin-Bo Qi(亓欣波)</p>
  <div class="site-description" itemprop="description">Digitize everything to realize Digitalization!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">164</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/qixinbo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qixinbo" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qixinbo@gmail.com" title="E-Mail → mailto:qixinbo@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/qixinbo" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;qixinbo" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fas fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.tsinghua.edu.cn/" title="https:&#x2F;&#x2F;www.tsinghua.edu.cn" rel="noopener" target="_blank">THU</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.imr.cas.cn/" title="http:&#x2F;&#x2F;www.imr.cas.cn" rel="noopener" target="_blank">IMR</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.sdu.edu.cn/" title="http:&#x2F;&#x2F;www.sdu.edu.cn" rel="noopener" target="_blank">SDU</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://liam0205.me/" title="http:&#x2F;&#x2F;liam0205.me&#x2F;" rel="noopener" target="_blank">黄晨成</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xin-Bo Qi(亓欣波)</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://qixinbo.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "http://qixinbo.github.io/2022/06/26/pytorch-accelerated_5/";
    this.page.identifier = "2022/06/26/pytorch-accelerated_5/";
    this.page.title = "轻量级PyTorch通用训练模板pytorch-accelerated解析：5 -- Trainer运行及案例赏析";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://qixinbo.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>

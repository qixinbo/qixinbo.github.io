<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"qixinbo.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Be interesting!">
<meta property="og:type" content="website">
<meta property="og:title" content="亓欣波">
<meta property="og:url" content="http://qixinbo.github.io/page/2/index.html">
<meta property="og:site_name" content="亓欣波">
<meta property="og:description" content="Be interesting!">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Xin-Bo Qi(亓欣波)">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://qixinbo.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>亓欣波</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">亓欣波</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Hi, I am Xin-Bo Qi (亓欣波)</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-scholar">

    <a href="/scholar/" rel="section"><i class="bar-chart fa-fw"></i>scholar</a>

  </li>
        <li class="menu-item menu-item-sources">

    <a href="/sources/" rel="section"><i class="rss fa-fw"></i>sources</a>

  </li>
        <li class="menu-item menu-item-gallery">

    <a href="/gallery/" rel="section"><i class="file-image-o fa-fw"></i>gallery</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404.html" rel="section"><i class="heartbeat fa-fw"></i>Commonweal 404</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2020/06/19/ImagePy_21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Be interesting!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="亓欣波">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/19/ImagePy_21/" class="post-title-link" itemprop="url">ImagePy解析：21 -- 管理器</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-19 00:00:00" itemprop="dateCreated datePublished" datetime="2020-06-19T00:00:00+08:00">2020-06-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-25 15:20:30" itemprop="dateModified" datetime="2021-03-25T15:20:30+08:00">2021-03-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/computational-material-science/" itemprop="url" rel="index"><span itemprop="name">computational material science</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/06/19/ImagePy_21/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/06/19/ImagePy_21/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ImagePy中的管理器分两类：Source里的管理器维护全局静态数据，比如读写器、配置文件等， App里面的管理器维护运行时数据，比如图像、表格。</p>
<h1 id="静态管理器"><a href="#静态管理器" class="headerlink" title="静态管理器"></a>静态管理器</h1><h2 id="创建管理器"><a href="#创建管理器" class="headerlink" title="创建管理器"></a>创建管理器</h2><p>这里创建一个money管理器，里面可以添加美元USD、欧元EUR、人民币RMB，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Source.manager(<span class="string">&#x27;money&#x27;</span>).add(<span class="string">&#x27;USD&#x27;</span>, MoneyReader, <span class="string">&#x27;MoneyDisplay&#x27;</span>)</span><br><span class="line">Source.manager(<span class="string">&#x27;money&#x27;</span>).add(<span class="string">&#x27;EUR&#x27;</span>, MoneyReader, <span class="string">&#x27;MoneyDisplay&#x27;</span>)</span><br><span class="line">Source.manager(<span class="string">&#x27;money&#x27;</span>).add(<span class="string">&#x27;RMB&#x27;</span>, MoneyReader, <span class="string">&#x27;MoneyDisplay&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>意思就是管理器名为money，添加的成员有USD、EUR、RMB，处理方式是MoneyReader，显示方式是MoneyDisplay。<br>add方法的这三个参数分别对应的形参为name、obj和tag，可以这样统一理解：name表示对象名，obj表示处理方式，MoneyDisplay是显示方式。可以这样来感性认识，也可以认为这三个参数的地位是平齐的，因为有这三个参数可以用于索引，所以可表示的范围会非常大，导致manager的用处也非常广。</p>
<h2 id="读取管理器"><a href="#读取管理器" class="headerlink" title="读取管理器"></a>读取管理器</h2><p>money管理器中添加元素以后，可以再在全局读取出来。<br>最重要的读取方式就是Manager类的gets()方法，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gets</span>(<span class="params">self, name=<span class="literal">None</span>, tag=<span class="literal">None</span>, obj=<span class="literal">None</span></span>):</span></span><br><span class="line">    rst = [i <span class="keyword">for</span> i <span class="keyword">in</span> self.objs <span class="keyword">if</span> name <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> name == i[<span class="number">0</span>]]</span><br><span class="line">    rst = [i <span class="keyword">for</span> i <span class="keyword">in</span> rst <span class="keyword">if</span> obj <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> obj <span class="keyword">is</span> i[<span class="number">1</span>]]</span><br><span class="line">    <span class="keyword">return</span> [i <span class="keyword">for</span> i <span class="keyword">in</span> rst <span class="keyword">if</span> tag <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> tag == i[<span class="number">2</span>]]</span><br></pre></td></tr></table></figure>
<p>可以看出，可以根据name、tag和obj来读取。从上面代码可以看出，如果不明确指定某一参数的话，就认为该参数不做过滤条件。<br>以tag过滤为例，假设读取设置为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;gets = &quot;</span>, Source.manager(<span class="string">&#x27;money&#x27;</span>).gets(tag=<span class="string">&#x27;MoneyDisplay&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>那么返回结果就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gets =  [(<span class="string">&#x27;RMB&#x27;</span>, &lt;function MoneyReader at <span class="number">0x000002E0C3C6F510</span>&gt;, <span class="string">&#x27;MoneyDisplay&#x27;</span>), (<span class="string">&#x27;EUR&#x27;</span>, &lt;function MoneyReader at <span class="number">0x000002E0C3C6F510</span>&gt;, <span class="string">&#x27;MoneyDisplay&#x27;</span>), (<span class="string">&#x27;USD&#x27;</span>, &lt;function MoneyReader at <span class="number">0x000002E0C3C6F510</span>&gt;, <span class="string">&#x27;MoneyDisplay&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>即将tag为MoneyDisplay的所有对象都返回。<br>还有一个直接获取管理器中的对象名称的快捷方式，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;names = &quot;</span>, Source.manager(<span class="string">&#x27;money&#x27;</span>).names())</span><br></pre></td></tr></table></figure>
<p>返回结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">names =  [<span class="string">&#x27;RMB&#x27;</span>, <span class="string">&#x27;EUR&#x27;</span>, <span class="string">&#x27;USD&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h2 id="管理器持久化"><a href="#管理器持久化" class="headerlink" title="管理器持久化"></a>管理器持久化</h2><p>管理器中的对象可以通过持久化将内存中的数据存储到磁盘上，该功能对于配置类文件非常重要，因为可以及时存储软件设置。<br>管理器的持久化使用的是write函数，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Source.manager(<span class="string">&#x27;money&#x27;</span>).write(<span class="string">&quot;1.txt&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这里需要注意的是上面我们设定的MoneyReader是一个函数，所以无法json化，所以我们这里将其设为None，才能正确存储。<br>这样该txt文件中的内容就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">&quot;RMB&quot;</span>, null, <span class="string">&quot;MoneyDisplay&quot;</span>], [<span class="string">&quot;EUR&quot;</span>, null, <span class="string">&quot;MoneyDisplay&quot;</span>], [<span class="string">&quot;USD&quot;</span>, null, <span class="string">&quot;MoneyDisplay&quot;</span>]]</span><br></pre></td></tr></table></figure>
<p>持久化以后还可以读取回来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Source.manager(<span class="string">&#x27;money&#x27;</span>).read(<span class="string">&quot;1.txt&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="App中的静态管理器"><a href="#App中的静态管理器" class="headerlink" title="App中的静态管理器"></a>App中的静态管理器</h2><p>App类中也有一个与Source类似的静态管理器，用来管理color和roi，即颜色管理器和roi管理器，这样就能在全局来调用颜色和roi。<br>具体用法见上面的静态管理器。</p>
<h1 id="动态管理器"><a href="#动态管理器" class="headerlink" title="动态管理器"></a>动态管理器</h1><p>App类中的动态管理器用来管理ImagePy所打开的图像、图像窗口（即画布）、表格、表格窗口、网格、网格窗口和任务，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">App</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.img_manager = Manager()</span><br><span class="line">        self.wimg_manager = Manager()</span><br><span class="line">        self.tab_manager = Manager()</span><br><span class="line">        self.wtab_manager = Manager()</span><br><span class="line">        self.mesh_manager = Manager()</span><br><span class="line">        self.wmesh_manager = Manager()</span><br><span class="line">        self.task_manager = Manager()</span><br><span class="line">        self.managers = &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>下面以图像管理器为例，看一下动态管理器的运行机制。</p>
<h2 id="创建管理器并添加元素"><a href="#创建管理器并添加元素" class="headerlink" title="创建管理器并添加元素"></a>创建管理器并添加元素</h2><p>动态管理器的创建实际在App类创建时就在初始化时创建。<br>下面是添加图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">app = wx.App(<span class="literal">False</span>)</span><br><span class="line">frame = ImagePy(<span class="literal">None</span>)</span><br><span class="line">frame.Show()</span><br><span class="line">frame.show_img([np.zeros((<span class="number">512</span>, <span class="number">512</span>), dtype=np.uint8)], <span class="string">&#x27;zeros&#x27;</span>)</span><br><span class="line">frame.show_img([np.ones((<span class="number">512</span>, <span class="number">512</span>), dtype=np.uint8)], <span class="string">&#x27;ones&#x27;</span>)</span><br><span class="line"></span><br><span class="line">app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>这里我们通过ImagePy框架中的show_img添加了两张图像，一张名为zeros，一张名为ones。<br>实际查看该函数的源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_show_img</span>(<span class="params">self, img, title=<span class="literal">None</span></span>):</span></span><br><span class="line">    canvas = self.canvasnb.add_canvas()</span><br><span class="line">    self.remove_img(canvas.image)</span><br><span class="line">    self.remove_img_win(canvas)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> title <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        canvas.set_imgs(img)</span><br><span class="line">        canvas.image.name = title</span><br><span class="line">    <span class="keyword">else</span>: canvas.set_img(img)</span><br><span class="line">    self.add_img(canvas.image)</span><br><span class="line">    self.add_img_win(canvas)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_img</span>(<span class="params">self, img, title=<span class="literal">None</span></span>):</span></span><br><span class="line">    wx.CallAfter(self._show_img, img, title)</span><br></pre></td></tr></table></figure>
<p>可以看出，它是调用了App类的add_img和add_img_win来添加图像及图像窗口（画布）。<br>注意，这里之所以frame能调用App类中的这两个方法，因为frame是ImagePy类的实例对象，而ImagePy类既继承了wx.Frame类，也继承了App类。</p>
<h2 id="读取管理器-1"><a href="#读取管理器-1" class="headerlink" title="读取管理器"></a>读取管理器</h2><p>在插件中如果需要获取图像或其窗口，则可以使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.app.get_img_win()</span><br><span class="line">self.app.get_img()</span><br></pre></td></tr></table></figure>
<p>这里之所以这样调用，是因为App类的实例化对象app是贯穿全局的，任何一个tool或menu在start()启动的时候都需要传入app，所以app能统领全局。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2020/06/14/ImagePy_20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Be interesting!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="亓欣波">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/14/ImagePy_20/" class="post-title-link" itemprop="url">ImagePy解析：20 -- 几何矢量Shape</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-14 00:00:00" itemprop="dateCreated datePublished" datetime="2020-06-14T00:00:00+08:00">2020-06-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-25 15:20:30" itemprop="dateModified" datetime="2021-03-25T15:20:30+08:00">2021-03-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/computational-material-science/" itemprop="url" rel="index"><span itemprop="name">computational material science</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/06/14/ImagePy_20/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/06/14/ImagePy_20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>ImagePy中表示几何矢量的结构类是Shape，最直观的一个应用就是各种ROI操作，这里通过一个小例子看看各种几何图形是怎样操纵和显示的。</p>
<h1 id="最小demo"><a href="#最小demo" class="headerlink" title="最小demo"></a>最小demo</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sciapp.<span class="built_in">object</span> <span class="keyword">import</span> mark2shp</span><br><span class="line"><span class="keyword">from</span> sciwx.canvas <span class="keyword">import</span> VCanvas <span class="keyword">as</span> Canvas</span><br><span class="line"><span class="keyword">import</span> wx</span><br><span class="line"></span><br><span class="line">circle = &#123;<span class="string">&#x27;type&#x27;</span>:<span class="string">&#x27;circle&#x27;</span>, <span class="string">&#x27;color&#x27;</span>:(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>), <span class="string">&#x27;fcolor&#x27;</span>:(<span class="number">255</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="string">&#x27;fill&#x27;</span>:<span class="literal">False</span>, <span class="string">&#x27;body&#x27;</span>:(<span class="number">100</span>,<span class="number">100</span>,<span class="number">50</span>)&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mark_test</span>(<span class="params">mark</span>):</span></span><br><span class="line">    frame = wx.Frame(<span class="literal">None</span>, title=<span class="string">&#x27;gray test&#x27;</span>)</span><br><span class="line">    canvas = Canvas(frame, autofit=<span class="literal">False</span>, up=<span class="literal">True</span>)</span><br><span class="line">    canvas.set_shp(mark2shp(mark))</span><br><span class="line">    frame.Show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app = wx.App()</span><br><span class="line">    mark_test(circle)</span><br><span class="line">    app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>上述是个可运行的最小demo，运行结果为：<br><img src="https://user-images.githubusercontent.com/6218739/84457021-2985e700-ac94-11ea-90e7-b38f2869b1ed.png" alt="shape"><br>可以看出，成功绘制出了一个红色轮廓的圆形。<br>下面逐步解析一下。</p>
<h1 id="mark格式"><a href="#mark格式" class="headerlink" title="mark格式"></a>mark格式</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">circle = &#123;<span class="string">&#x27;type&#x27;</span>:<span class="string">&#x27;circle&#x27;</span>, <span class="string">&#x27;color&#x27;</span>:(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>), <span class="string">&#x27;fcolor&#x27;</span>:(<span class="number">255</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="string">&#x27;fill&#x27;</span>:<span class="literal">False</span>, <span class="string">&#x27;body&#x27;</span>:(<span class="number">100</span>,<span class="number">100</span>,<span class="number">50</span>)&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出，这里的圆形是通过imagepy的mark格式来定义的，即通过一个特定的字典来定义，具体写法见之前的关于mark的解析，在<a target="_blank" rel="noopener" href="https://qixinbo.info/2020/03/28/imagepy_19/">这里</a>。</p>
<p>之所以使用mark格式，是因为它的可读性非常高，如果直接写Shape类会非常不直观。</p>
<h1 id="mark转Shape"><a href="#mark转Shape" class="headerlink" title="mark转Shape"></a>mark转Shape</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mark2shp(mark)</span><br></pre></td></tr></table></figure>
<p>这一步是将上面的mark格式的定义转为imagepy内置的Shape类型的对象。具体的函数定义为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mark2shp</span>(<span class="params">mark</span>):</span></span><br><span class="line">    style = mark.copy()</span><br><span class="line">    style.pop(<span class="string">&#x27;body&#x27;</span>)</span><br><span class="line">    keys = &#123;<span class="string">&#x27;point&#x27;</span>:Point, <span class="string">&#x27;points&#x27;</span>:Points, <span class="string">&#x27;line&#x27;</span>:Line, <span class="string">&#x27;lines&#x27;</span>:Lines,</span><br><span class="line">            <span class="string">&#x27;polygon&#x27;</span>:Polygon, <span class="string">&#x27;polygons&#x27;</span>:Polygons, <span class="string">&#x27;circle&#x27;</span>:Circle,</span><br><span class="line">            <span class="string">&#x27;circles&#x27;</span>:Circles, <span class="string">&#x27;rectangle&#x27;</span>:Rectangle, <span class="string">&#x27;rectangles&#x27;</span>:Rectangles,</span><br><span class="line">            <span class="string">&#x27;ellipse&#x27;</span>:Ellipse, <span class="string">&#x27;ellipses&#x27;</span>:Ellipses, <span class="string">&#x27;text&#x27;</span>:Text, <span class="string">&#x27;texts&#x27;</span>:Texts&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mark[<span class="string">&#x27;type&#x27;</span>] <span class="keyword">in</span> keys: <span class="keyword">return</span> keys[mark[<span class="string">&#x27;type&#x27;</span>]](mark[<span class="string">&#x27;body&#x27;</span>], **style)</span><br><span class="line">    <span class="keyword">if</span> mark[<span class="string">&#x27;type&#x27;</span>]==<span class="string">&#x27;layer&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> Layer([mark2shp(i) <span class="keyword">for</span> i <span class="keyword">in</span> mark[<span class="string">&#x27;body&#x27;</span>]], **style)</span><br><span class="line">    <span class="keyword">if</span> mark[<span class="string">&#x27;type&#x27;</span>]==<span class="string">&#x27;layers&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> Layers(<span class="built_in">dict</span>(<span class="built_in">zip</span>(mark[<span class="string">&#x27;body&#x27;</span>].keys(),</span><br><span class="line">            [mark2shp(i) <span class="keyword">for</span> i <span class="keyword">in</span> mark[<span class="string">&#x27;body&#x27;</span>].values()])), **style)</span><br></pre></td></tr></table></figure>
<p>里面的Point、Circle、Rectangle、Ellipse就是ImagePy内置的几何类，它们有一个共同的基类，即Shape类，里面有三个重要的属性和方法，这也是它的不同子类之间需要进行的重载实现（以Circle为例）：</p>
<p>（1）body属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.body = np.array(body, dtype=np.float32)</span><br></pre></td></tr></table></figure>
<p>将mark格式的body传入numpy的array数组中，然后赋给Shape对象的body属性。<br>这里使用numpy数组的原因有如下几个（源自龙哥的答疑）：</p>
<ul>
<li>在draw的时候，需要根据canvas的位移和比例，进行一个加乘运算，得到最后需要draw的画布坐标，即下面代码：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> pts.dtype == <span class="string">&#x27;circles&#x27;</span>:</span><br><span class="line">	lst = []</span><br><span class="line">	x, y, r = pts.body.T</span><br><span class="line">	x, y = f(x, y)</span><br><span class="line">	r = r * key[<span class="string">&#x27;k&#x27;</span>]</span><br><span class="line">	lst = np.vstack([x-r, y-r, r*<span class="number">2</span>, r*<span class="number">2</span>]).T</span><br><span class="line">	dc.DrawEllipseList(lst)</span><br></pre></td></tr></table></figure></li>
<li>Shape也可以自动计算边界，就用数组的min、max，带上axis参数就可以实现</li>
<li>编辑的时候，一个snap，其实也要判断所有的点，距离鼠标最近的，也有必要用numpy广播。</li>
</ul>
<p>另外，Shape类的style也是mark传入，也可以转为JSON格式的数据，具体可以详看Shape类的代码。<br>（2）转为mark格式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_mark</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> Shape.to_mark(self, <span class="built_in">tuple</span>(self.body.tolist()))</span><br></pre></td></tr></table></figure>
<p>即将Shape的body转为mark格式。</p>
<p>（3）转为shapely的geom格式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_geom</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> geom.Point(self.body[:<span class="number">2</span>]).buffer(self.body[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<p>shapely是一个对几何矢量几何进行操作和分析的python库。<br>上面这条语句就是将body的前两个数作为点的坐标生成shapely中的Point，然后将body的第三个数（即半径）生成该Point的缓冲区，即形成一个圆形区域。<br>转为shapely的geometry结构后，就可以进行复杂的几何运算。比如编辑时候的拖拽，判断鼠标是否点击在图形的内部，就需要将Shape转成shapely的geometry。</p>
<p>关于shapely的教程可以参考：<br><a target="_blank" rel="noopener" href="https://www.osgeo.cn/pygis/shapely.html">矢量数据的空间分析：使用Shapely</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/24782733">基于Python的缓冲区分析</a></p>
<h1 id="将Shape传入画布"><a href="#将Shape传入画布" class="headerlink" title="将Shape传入画布"></a>将Shape传入画布</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">canvas.set_shp(mark2shp(mark))</span><br></pre></td></tr></table></figure>
<p>注意，这里的canvas对象其实是VCanvas类的对象，实际做的是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VCanvas</span>(<span class="params">Canvas</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, parent, autofit=<span class="literal">False</span>, ingrade=<span class="literal">True</span>, up=<span class="literal">True</span></span>):</span></span><br><span class="line">        Canvas.__init__(self, parent, autofit, ingrade, up)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_shp</span>(<span class="params">self, shp</span>):</span></span><br><span class="line">        self.marks[<span class="string">&#x27;shape&#x27;</span>] = shp</span><br><span class="line">        self.update()</span><br></pre></td></tr></table></figure>
<p>可以看出，VCanvas继承了Canvas，所以就是将Circle对象传给了Canvas的marks属性（这个属性是个字典）的shape这个key。<br>并且调用Canvas的update进行更新。</p>
<h1 id="画布绘制几何图形"><a href="#画布绘制几何图形" class="headerlink" title="画布绘制几何图形"></a>画布绘制几何图形</h1><p>那么在画布中是怎样绘制几何图形的呢？<br>具体代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> self.marks.values():</span><br><span class="line">    <span class="keyword">if</span> i <span class="keyword">is</span> <span class="literal">None</span>: <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">callable</span>(i):</span><br><span class="line">        i(dc, self.to_panel_coor, k = self.scale)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        drawmark(dc, self.to_panel_coor, i, k=self.scale, cur=<span class="number">0</span>,</span><br><span class="line">            winbox=self.winbox, oribox=self.oribox, conbox=self.conbox)</span><br></pre></td></tr></table></figure>
<p>首先通过字典的values方法返回marks属性中的所有的值values，以这里绘制Circle为例，那么返回的i就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">sciapp</span>.<span class="title">object</span>.<span class="title">shape</span>.<span class="title">Circle</span>&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<p>即i是Circle对象。注意这里不要使用print来直接打印i，而需要使用type来显示，因为在Shape类中有一个方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">str</span>(self.to_mark())</span><br></pre></td></tr></table></figure>
<p>即如果想看i的值时，会先将其转为mark格式再打印出来，但实际i是个Shape对象。<br>因为i不是callable的，所以就会调用drawmark来显示，最终是使用dc的DrawCircle来绘图。具体绘制过程也可以参见mark模式解析那一篇。</p>
<h1 id="添加shape动作"><a href="#添加shape动作" class="headerlink" title="添加shape动作"></a>添加shape动作</h1><p>这一部分是shape对象进阶，主要看怎样在画布中实时绘制shape，涉及了shape动作和鼠标事件。<br>最小可用的demo如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sciapp.<span class="built_in">object</span> <span class="keyword">import</span> mark2shp</span><br><span class="line"><span class="keyword">from</span> sciapp.action <span class="keyword">import</span> EllipseEditor</span><br><span class="line"><span class="keyword">from</span> sciwx.canvas <span class="keyword">import</span> VCanvas <span class="keyword">as</span> Canvas</span><br><span class="line"><span class="keyword">import</span> wx</span><br><span class="line"></span><br><span class="line">circle = &#123;<span class="string">&#x27;type&#x27;</span>:<span class="string">&#x27;circle&#x27;</span>, <span class="string">&#x27;color&#x27;</span>:(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>), <span class="string">&#x27;fcolor&#x27;</span>:(<span class="number">255</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="string">&#x27;fill&#x27;</span>:<span class="literal">False</span>, <span class="string">&#x27;body&#x27;</span>:(<span class="number">100</span>,<span class="number">100</span>,<span class="number">50</span>)&#125;</span><br><span class="line"></span><br><span class="line">layer = &#123;<span class="string">&#x27;type&#x27;</span>:<span class="string">&#x27;layer&#x27;</span>, <span class="string">&#x27;num&#x27;</span>:-<span class="number">1</span>, <span class="string">&#x27;color&#x27;</span>:(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>), <span class="string">&#x27;fcolor&#x27;</span>:(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>), <span class="string">&#x27;fill&#x27;</span>:<span class="literal">False</span>, <span class="string">&#x27;body&#x27;</span>:[circle]&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mark_test</span>(<span class="params">mark</span>):</span></span><br><span class="line">    frame = wx.Frame(<span class="literal">None</span>, title=<span class="string">&#x27;gray test&#x27;</span>)</span><br><span class="line">    canvas = Canvas(frame, autofit=<span class="literal">False</span>, up=<span class="literal">True</span>)</span><br><span class="line">    canvas.set_shp(mark2shp(mark))</span><br><span class="line">    frame.Show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app = wx.App()</span><br><span class="line">    EllipseEditor().start(<span class="literal">None</span>)</span><br><span class="line">    mark_test(layer)</span><br><span class="line">    app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>可以看出，就是在最上面例子上添加了一个自由绘制椭圆的动作。<br>具体分析一下：<br>首先添加椭圆编辑器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sciapp.action <span class="keyword">import</span> EllipseEditor</span><br><span class="line">EllipseEditor().start(<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>这个椭圆编辑器实际是一个工具Tool，它最开始的源头可视为Tool类，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tool</span>(<span class="params">SciAction</span>):</span></span><br><span class="line">    title = <span class="string">&#x27;Base Tool&#x27;</span></span><br><span class="line">    default = <span class="literal">None</span></span><br><span class="line">    cursor = <span class="string">&#x27;arrow&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_down</span>(<span class="params">self, canvas, x, y, btn, **key</span>):</span> <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_up</span>(<span class="params">self, canvas, x, y, btn, **key</span>):</span> <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_move</span>(<span class="params">self, canvas, x, y, btn, **key</span>):</span> <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_wheel</span>(<span class="params">self, canvas, x, y, d, **key</span>):</span> <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span>(<span class="params">self, app</span>):</span></span><br><span class="line">        self.app, self.default = app, self</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> app <span class="keyword">is</span> <span class="literal">None</span>: app.tool = self</span><br></pre></td></tr></table></figure>
<p>Tool中定义了鼠标动作，最原始的Tool中只是提供了鼠标动作定义入口，并没有具体的动作。<br>Tool派生了DefaultTool，可以实现最朴素的移动画布和缩放画布功能（具体见DefaultTool代码）。<br>DefaultTool派生了ShapeTool，不过这个派生并没有实质性的扩展，只是为了与ImageTool、TableTool进行区分。<br>ShapeTool派生了BaseEditor，该工具对Shape对象进行了深度的动作定制：<br>（1）鼠标中键拖动；<br>（2）alt+右键：删除一个shape<br>（3）shift+右键：合并shape<br>（4）右键：将shape根据当前区域大小缩放<br>（5）alt+ctrl：显示锚点（注意这个地方得移动一下鼠标，因为没有定义单独的键盘事件，否则无法触发动作）<br>（6）alt+ctrl+鼠标拖动锚点：改变shape</p>
<p>BaseEditor派生了EllipseEditor，该工具又对椭圆形状进行了自定义：<br>（1）鼠标左键按下并拖动：新建一个椭圆；<br>（2）alt+新建椭圆：两者做差；<br>（3）shift+新建椭圆：两者取并集<br>（4）alt+shift+新建椭圆：两者取交集</p>
<p>那么，画布是怎样获取shape和tool的呢？答案就在Canvas中的这两行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">obj, tol = self.get_obj_tol()</span><br><span class="line">btn, tool = me.GetButton(), self.tool <span class="keyword">or</span> tol</span><br></pre></td></tr></table></figure>
<p>第一行得到了当前的对象，对于shape对象，注意VCanvas的这两个方法和属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VCanvas</span>(<span class="params">Canvas</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, parent, autofit=<span class="literal">False</span>, ingrade=<span class="literal">True</span>, up=<span class="literal">True</span></span>):</span></span><br><span class="line">        Canvas.__init__(self, parent, autofit, ingrade, up)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_obj_tol</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.shape, ShapeTool.default</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_shp</span>(<span class="params">self, shp</span>):</span></span><br><span class="line">        self.marks[<span class="string">&#x27;shape&#x27;</span>] = shp</span><br><span class="line">        self.update()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_tool</span>(<span class="params">self, tool</span>):</span> self.tool = tool</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shape</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="string">&#x27;shape&#x27;</span> <span class="keyword">in</span> self.marks: <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> self.marks[<span class="string">&#x27;shape&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>即VCanvas重载了Canvas的获取对象的方法，同时shape属性又获得了之前的Shape对象。<br>另外需要注意的是在EllipseEditor中添加椭圆时用的是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shp.body.append(self.obj)</span><br></pre></td></tr></table></figure>
<p>所以这也就是为什么在程序中又新加了一个layer，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layer = &#123;<span class="string">&#x27;type&#x27;</span>:<span class="string">&#x27;layer&#x27;</span>, <span class="string">&#x27;num&#x27;</span>:-<span class="number">1</span>, <span class="string">&#x27;color&#x27;</span>:(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>), <span class="string">&#x27;fcolor&#x27;</span>:(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>), <span class="string">&#x27;fill&#x27;</span>:<span class="literal">False</span>, <span class="string">&#x27;body&#x27;</span>:[circle]&#125;</span><br></pre></td></tr></table></figure>
<p>否则body中无法append进去。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2020/04/09/book-refactoring/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Be interesting!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="亓欣波">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/09/book-refactoring/" class="post-title-link" itemprop="url">《重构：数字化转型的逻辑》读书笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-09 00:00:00" itemprop="dateCreated datePublished" datetime="2020-04-09T00:00:00+08:00">2020-04-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-25 15:20:30" itemprop="dateModified" datetime="2021-03-25T15:20:30+08:00">2021-03-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/IIoT/" itemprop="url" rel="index"><span itemprop="name">IIoT</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/04/09/book-refactoring/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/04/09/book-refactoring/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>以下是对安筱鹏博士的《重构：数字化转型的逻辑》一书的笔记摘抄。<br>逐字摘抄能够加深自己的理解，防止“水过地皮湿”，强烈推荐这种读书方法。</p>
<h1 id="不重构，无未来：拥抱数据驱动的智能-新时代"><a href="#不重构，无未来：拥抱数据驱动的智能-新时代" class="headerlink" title="不重构，无未来：拥抱数据驱动的智能+新时代"></a>不重构，无未来：拥抱数据驱动的智能+新时代</h1><p>伴随着<strong>新一代信息通信技术</strong>（以互联网、大数据、人工智能、5G为代表）的持续创新和渗透扩散，新一轮工业革命正在全球范围孕育兴起，制造业正迈向<strong>体系重构</strong>、<strong>动力变革</strong>、<strong>范式迁移</strong>的新阶段，加速向<strong>数字化</strong>、<strong>网络化</strong>、<strong>智能化</strong>方向延伸扩展，<strong>万物互联</strong>、<strong>数据驱动</strong>、<strong>软件定义</strong>、<strong>平台支撑</strong>、<strong>组织重构</strong>、<strong>智能主导</strong>正在构建制造业的新体系，它也成为了全球新一轮产业竞争的制高点。</p>
<h2 id="体系重构"><a href="#体系重构" class="headerlink" title="体系重构"></a>体系重构</h2><p>（1）谁来生产（Who）在变：<strong>生产主体</strong>从生产者向产消者Prosumer演进，个性化定制模式的兴起让消费者全程参与到生产过程中；<br>（2）生产什么（What）在变：<strong>生产对象</strong>从功能产品向智能互联产品演进，可动态感知并实时响应消费需求的无人驾驶、服务机器人等智能化产品的商业化步伐不断加快；<br>（3）用何工具（Which）在变：<strong>生产工具</strong>从以工业社会传统的以能量转换为特征的工具向智能工具演进，即具备对信息进行采集、传输、处理、执行的工具，3D打印、数控机床、智能机器人等智能装备快速涌现；<br>（4）如何生产（How）在变：<strong>生产方式</strong>从传统制造的“试错法”向基于数字仿真的“模拟择优法”转变，构建制造业快速迭代、持续优化、数据驱动的新生产方式；<br>（5）在哪生产（Where）在变：网络化协同制造、分享制造等制造业新模式推动<strong>生产地点</strong>从集中化走向分散化，跨部门、跨企业、跨地域的协同成为常态，尤其是分享制造的发展，构建起了检测、加工、认证、配送等制造能力标准化封装、在线化交易的新体系，推动制造能力在全社会范围内进行协同。</p>
<h2 id="动力变革"><a href="#动力变革" class="headerlink" title="动力变革"></a>动力变革</h2><p>制造业迈向转型升级的新阶段——数据驱动的新阶段。<br>（1）资源优化是目标，即不断优化制造资源的配置效率，就是要实现更好的质量、更低的成本、更快的交付、更高的满意度，就是要提高制造业全要素生产率；<br>（2）数据流动是关键，即能够把正确的数据在正确的时间以正确的方式传递给正确的人和机器，把数据转化为信息，把信息转化为知识，把知识转化为决策，以信息流带动技术流、资金流、人才流、物资流，以应对和解决制造过程的复杂性和不确定性等问题，提高制造资源的配置效率；<br>（3）工业软件是核心，软件本质上是人类隐形知识显性化的载体，是一套数据自动流动的规则体系。</p>
<h2 id="范式迁移"><a href="#范式迁移" class="headerlink" title="范式迁移"></a>范式迁移</h2><p>制造范式指在一定时期、在特定技术条件下对制造业价值观、方法论、发展模式和运行规律的认识框架。人类认识和改造世界的方法正从传统的理论推理（以牛顿定律、爱因斯坦相对论为代表，以“观察+抽象+数学”为关键要素，是人类认识世界最根本的方法，依赖于少数天才科学家，具备严密的逻辑关系，是试验验证和模拟择优的基础）、试验验证（以爱迪生发明灯泡为代表，以“假设+试验+归纳”为关键要素，依赖于设备材料的高投入，实验过程大协作、长周期，验证结果直观）向模拟择优（以波音777研发为代表，以“样本数据+机理模型”为关键要素，依赖于高质量机理模型的支撑，和传统试错法相比，投入少、周期短，可推动产品研发、验证、制造、服务业务在赛博空间的快速迭代，实现更短的研发周期、更低的制造成本、更高的产品质量和更好的客户体验）和大数据分析（以GE通过平台优化风电设备性能为代表，以“海量数据+大数据分析模型”为关键要素，依赖于海量数据的获取，以及计算、存储资源的低成本和高效利用，是一种基于数据驱动的价值创造范式）转变。</p>
<h2 id="智能制造和工业互联网"><a href="#智能制造和工业互联网" class="headerlink" title="智能制造和工业互联网"></a>智能制造和工业互联网</h2><p>20世纪80年代提出的智能制造和2012年提出的工业互联网是面对制造转型升级需求，基于不同时代的技术体系、需求结构、竞争结局提出的解决方案，既有联系又有区别，从智能制造和工业互联网，是<strong>信息技术体系从传统架构向云架构的迁移</strong>，是<strong>制造资源从局部优化到全局优化的演进</strong>，是<strong>业务协同从企业内部到产业链的扩展</strong>，是<strong>竞争模式从单一企业竞争到生态体系竞争的升级</strong>，是<strong>产业分工从基于产品的分工到基于知识的分工深化</strong>，但其内在逻辑是一致的——<strong>以数据的自动流动化解复杂系统的不确定性</strong>。</p>
<h2 id="重构"><a href="#重构" class="headerlink" title="重构"></a>重构</h2><p>（1）思维重构：以大视野、大科学、大融合维度视角，审视新一轮科技革命和产业变革机理，打通穿透数字化转型的技术、产业、经济、商业、政策的语境与逻辑，在数据+算法定义的世界中，探索<strong>升维思考</strong>之后的<strong>降维落地</strong>之路；<br>（2）战略重构：数字化转型带来了工具革命和决策革命，人们要重新思考战略的形成、演化与落地；<br>（3）技术重构：大科学、大技术交叉融合的时代，技术体系的解耦、分化、再封装正在构建新技术体系，如何洞察技术变局，以OT与IT融合、云架构升级、微服务落地，粉碎僵化开发模式和陈规桎梏，重建技术支撑体系；<br>（4）能力重构：技术赋能时代，传统能力升级与新型能力培育相互交织激荡，企业竞争力体系正在加速重构；<br>（5）组织重构：企业组织迎来了异常转基因工程，无边界的<strong>液态组织</strong>正在激活企业的内生动力。</p>
<h1 id="智能制造的逻辑：从生产装备自动化到数据流动自动化"><a href="#智能制造的逻辑：从生产装备自动化到数据流动自动化" class="headerlink" title="智能制造的逻辑：从生产装备自动化到数据流动自动化"></a>智能制造的逻辑：从生产装备自动化到数据流动自动化</h1><p>人类社会的发展史就是一部<strong>应对不确定性</strong>、<strong>寻求确定性</strong>的历史，克服对不确定性的恐惧是人类认知深化的重要动力，<strong>对客观世界的理解、预测、控制</strong>是人类化解不确定性恐惧的三步曲。<br>信息的价值在于减少认知的不确定性，个性化定制、产品智能化、产业分工深化及竞争格局加剧不断提升制造系统的复杂性及生产过程的不确定性，<strong>智能制造的本质就在于以数据的自动流动化解复杂系统的不确定性，提高智能资源配置效率。</strong></p>
<h1 id="智能制造的本质"><a href="#智能制造的本质" class="headerlink" title="智能制造的本质"></a>智能制造的本质</h1><p><strong>智能制造的本质在于以数据的自动流动化化解复杂制造系统的不确定性，优化制造资源配置效率。</strong></p>
<h2 id="信息、不确定性与人类社会发展"><a href="#信息、不确定性与人类社会发展" class="headerlink" title="信息、不确定性与人类社会发展"></a>信息、不确定性与人类社会发展</h2><h3 id="认知的分野：认知规律中的不确定性"><a href="#认知的分野：认知规律中的不确定性" class="headerlink" title="认知的分野：认知规律中的不确定性"></a>认知的分野：认知规律中的不确定性</h3><p>（1） 哲学视角<br>人们对客观世界的认知并非在确定性和不确定性之间二选一，而是两种思维方式在不断相互转化、交叉融合（灰度思维）。对于现实世界，人们的未知远大于已知，<strong>人类不懈追求认知的绝对确定性而逐步显现出其不确定性</strong>。<br>（2） 科学视角<br>以牛顿三大定律和万有引力定律为核心的牛顿力学作为经典科学，完美地解释了确定性运动学规律和现象。由此而来，近现代科学成就不断强化人们基于确定性逻辑规律的认知，人们不自觉地把科学性和确定性等同起来。<br>对不确定性的重新认识，是现代科学对于人类思想的重要贡献，是20世纪的重大进步。海森堡的测不准原则、哥德尔的不完全性定力、阿罗的社会选择理论、埃弗雷特的平行宇宙理论等不确定性的发现，促使我们的观念发生了根本变化。（<strong>用科学的理论认识总结这种不确定性，是区别被动接受和主动接受不确定性的判据，是不确定性和确定性的融合</strong>）<br>（3） 经济学视角<br>经济学通过研究人的经济行为来分析经济现象，又将人的行为过程描述为决策过程，经济学的一个基本问题是在不确定性条件下人们的决策原则是什么。<br>1972年诺贝尔经济学奖获得者阿罗认为，所谓信息就是根据条件概率原则有效地改变概率的任何观察结果，<strong>不确定就意味着成本，信息的价值就在于降低了经济的不确定性</strong>。</p>
<h3 id="信息的价值：减少认知的不确定性"><a href="#信息的价值：减少认知的不确定性" class="headerlink" title="信息的价值：减少认知的不确定性"></a>信息的价值：减少认知的不确定性</h3><p>香农在论文《通信的数学理论》中指出：<strong>信息是用来减少随机不确定性的东西，信息的价值是确定性的增加</strong>。<strong>信息就是两次不确定性之差</strong>，<strong>信息就是传递中的知识差</strong>。</p>
<h3 id="社会的演进：基于信息能力拓展的分工与协作"><a href="#社会的演进：基于信息能力拓展的分工与协作" class="headerlink" title="社会的演进：基于信息能力拓展的分工与协作"></a>社会的演进：基于信息能力拓展的分工与协作</h3><p>工业革命孕育的市场经济本质是如何在高度不确定性的环境中实现科学决策，哈耶克（1974年诺贝尔经济学奖获得者）认为，<strong>市场经济就是一个信息处理系统</strong>，大量独立个体通过价格发现机制，基于各种有限、当地化、碎片化的信息进行决策，优化资源配置。<br>进入数字经济时代，人类大规模协作的广度、深度、频率进入了一个新阶段，企业边界正在被重新定义，科层组织正在被瓦解，产消者不断涌现，微粒社会正在来临，平台经济体迅速崛起，人类社会已经从工业社会百万人量级的协作生产体系演进到数千万、数亿人的合作，这也带来了产业分工不断深化。</p>
<h2 id="企业竞争的本质：优化资源配置效率的竞争"><a href="#企业竞争的本质：优化资源配置效率的竞争" class="headerlink" title="企业竞争的本质：优化资源配置效率的竞争"></a>企业竞争的本质：优化资源配置效率的竞争</h2><h3 id="企业竞争的本质"><a href="#企业竞争的本质" class="headerlink" title="企业竞争的本质"></a>企业竞争的本质</h3><p>罗纳德.科斯指出：“企业的本质是一种资源配置的机制，是替代市场进行资源配置的组织“。市场和企业是配置资源的两种可相互替代的手段，在市场上资源的配置由价格机制来调节，在企业内则通过管理协调来完成，企业的边界由交易费用决定。当企业内的交易费用低于在市场上的交易费用时，企业的边界则得以扩展，直至两者的交易费用相等为止。<br>企业竞争的本质是在不确定市场环境下企业资源配置效率的竞争。对于制造企业而言，在研发、设计、采购、生产、配送、服务的每个环节，都面临着如何优化资源配置效率的问题。</p>
<h3 id="不确定性的来源"><a href="#不确定性的来源" class="headerlink" title="不确定性的来源"></a>不确定性的来源</h3><p>（1）产品本身的复杂性：现代产品是集软件、电子、机械、液压、控制于一体的技术系统，产品的设计、生产、维护难度越来越高，产品的研发组织充满了不确定性；<br>（2）生产过程的复杂性：制造是一个涉及企业内外部多主体、多设备、多环节、多学科、多工艺、跨区域协同的复杂系统工程。伴随着产业分工深化、个性化消费兴起、智能化步伐加快，生产过程的复杂性不断提高；<br>（3）市场需求的复杂性：制造企业正从传统的大规模标准化生产向适应用户个性化定制和体验式消费的新型生产方式演进；<br><img src="https://user-images.githubusercontent.com/6218739/78452523-431c1880-76be-11ea-97f3-0e8ee957cdbc.png" alt="1-1"><br>（4）供应链协同的复杂性：随着全球化的发展，企业制造分工日趋细化，产品供应链体系也随之越来越庞大。</p>
<h2 id="智能制造的本质：以数据的自动流动化解复杂系统的不确定性"><a href="#智能制造的本质：以数据的自动流动化解复杂系统的不确定性" class="headerlink" title="智能制造的本质：以数据的自动流动化解复杂系统的不确定性"></a>智能制造的本质：以数据的自动流动化解复杂系统的不确定性</h2><h3 id="智能的演化"><a href="#智能的演化" class="headerlink" title="智能的演化"></a>智能的演化</h3><p>杨学山在《智能原理》一书中指出“智能是主体<strong>适应</strong>、<strong>改变</strong>、<strong>选择</strong>环境的各种行为能力”。<br>（1）<strong>生物智能</strong>的演化：从单细胞到多系统、从低级到高级、从单个生命体向物种群落的演化。<br>（2）<strong>非生物智能</strong>的演化：对于非生物智能的探索有三大主流学派，即基于逻辑推理算法的符号主义学派、基于神经网络及网络间联结机制与学习算法的连接主义学派、基于“感知—行动”行为模拟算法的行为主义学派。</p>
<h2 id="数据的自动流动"><a href="#数据的自动流动" class="headerlink" title="数据的自动流动"></a>数据的自动流动</h2><p><strong>完整、准确的数据采集</strong>（智能装备和终端、各种传感器）是数据自动流动的起点，<strong>及时、可靠的数据传输网络</strong>（5G、物联网、时间敏感网络）是数据自动流动的通道，<strong>科学、合理的数据分析</strong>（算法、软件）是数据自动流动的核心，<strong>精准、有效的数据决策</strong>（分布式控制系统DCS、可编程逻辑控制器PLC）是数据自动流动的终点。</p>
<h2 id="信息化与资源优化配置"><a href="#信息化与资源优化配置" class="headerlink" title="信息化与资源优化配置"></a>信息化与资源优化配置</h2><p>信息物理系统是一种非生物智能与生物智能的集成系统，其本质是通过信息化手段实现数据自动流动，<strong>以信息流带动技术流、资金流、人才流、物资流</strong>，进而解决复杂制造系统的不确定性问题，不断优化资源配置效率。<br>在实际生产过程中企业生产运行时间损失、生产制造资源浪费情况十分严重，通过全面优化企业生产全流程、各环节资源配置提升企业生产效率，提高生产有效可用时间的潜力巨大。<br><img src="https://user-images.githubusercontent.com/6218739/78453057-ede20600-76c1-11ea-993a-36ad1cb2347d.png" alt="1-2"><br>（1）从资源优化配置的系统性来看，将从局部优化向全局优化演进。智能制造系统从单机设备、单一环节、单一场景、单一要素的局部小系统不断向大系统、巨系统演进，从部门级到企业级，再到产业链级，乃至产业生态级系统演进，不断突破地域、组织、机制的界限，实现对人才、技术、资金等资源和要素的高效整合，从而带动产品、模式和业态创新。<br>（2）从资源优化配置的时效性来看，将从静态优化向动态优化演进。传统制造理念是以不变应万变、以确定性应对不确定性，用各种冗余应对可能出现的不确定性，传统制造走向智能制造，就是摒弃冗余思维、静态思维，走向精准思维、动态思维，实时响应变化、拥抱变化，以动态优化策略应对各种不确定性。</p>
<h1 id="制造业智能化转型的趋势"><a href="#制造业智能化转型的趋势" class="headerlink" title="制造业智能化转型的趋势"></a>制造业智能化转型的趋势</h1><p>高效率、低成本、高质量是制造业不变的追求。当前，互联网、大数据、人工智能等新技术持续创新和高速发展，为制造业发展注入新的活力，使得制造业加速迈向万物互联、数据驱动、软件定义、平台支撑、组织重构的新时代。</p>
<h2 id="万物互联：互联一切可数字化的事物"><a href="#万物互联：互联一切可数字化的事物" class="headerlink" title="万物互联：互联一切可数字化的事物"></a>万物互联：互联一切可数字化的事物</h2><p>所谓万物互联，就是人、物、数据和应用通过互联网连接在一起，实现所有人和人、人和物及物和物之间的互联，重构整个社会的生产工具、生产方式和生活场景。在万物互联的角度下，信息化就是物理设备不断成为网络终端并引发整个社会变革的过程，信息技术发展的终极目标是基于物联网平台实现设备无所不在的连接，开发各类应用，提供多种数据支撑和服务，未来所有产品都将成为<strong>可监测</strong>、<strong>可控制</strong>、<strong>可优化</strong>、<strong>自主性</strong>的智能产品。<br><img src="https://user-images.githubusercontent.com/6218739/78453567-2df6b800-76c5-11ea-9ee8-fbd1b2e58661.png" alt="2-1"><br><img src="https://user-images.githubusercontent.com/6218739/78453713-0f44f100-76c6-11ea-9d1b-3d52db6934a1.png" alt="2-1"><br><img src="https://user-images.githubusercontent.com/6218739/78453838-f12bc080-76c6-11ea-8584-de60289ff0c1.png" alt="2-2"><br>可监测、可控制、可优化、自主性的智能产品将感知客户需求、推送客户服务，推动企业从<strong>产品生产商</strong>到<strong>客户运营商</strong>的转变。</p>
<h2 id="数据驱动：驱动制造资源的优化配置"><a href="#数据驱动：驱动制造资源的优化配置" class="headerlink" title="数据驱动：驱动制造资源的优化配置"></a>数据驱动：驱动制造资源的优化配置</h2><p>数据驱动的本质就是通过生产制造全过程、全产业链、产品全生命周期数据的自动流动不断优化制造资源的配置效率，就是要实现更好的质量、更低的成本、更快的交付、更高的满意度，就是要提高制造业全要素生产率，这将带来<strong>数据驱动的服务</strong>（智能互联产品正演变为一个客户需求数据实时感知的平台，演变为基于实时数据的客户服务平台）、<strong>数据驱动的创新</strong>（企业对客户现实需求和潜在需求的深度挖掘、实时感知、快速响应、及时满足，越来越依赖于需求—功能—创意—产品链条数据联动的速度、节奏和效率）、<strong>数据驱动的生产</strong>（数字化模型普遍存在于生产体系各个环节，构建了面向设计、生产、运营、服务和管理的产品库、知识库、专家库，衍生出个性化定制、极少量生产、服务型制造和云制造等新的生产模式）和<strong>数据驱动的决策</strong>（企业内部的横向集成和企业间的纵向集成实现了数据的及时性、完整性、准确性和可执行性，推动数据—信息—知识—决策持续转化，构建企业运营新机制）。</p>
<h2 id="软件定义：定义数据自动流动的准则"><a href="#软件定义：定义数据自动流动的准则" class="headerlink" title="软件定义：定义数据自动流动的准则"></a>软件定义：定义数据自动流动的准则</h2><p>软件的本质是构建一套数据自动流动的规则体系，基于软件打造“<strong>状态感知</strong>—<strong>实时分析</strong>—<strong>科学决策</strong>—<strong>精准执行</strong>”的数据闭环，解决研发设计、生产制造、运营管理乃至生产制造全过程中的复杂性和不确定性问题，提高资源配置效率。<br>（1）软件定义实现了软硬件的解耦分离：其核心是利用分层思想将<strong>软硬件分离</strong>，通过打破过去的一体化硬件设施，实现“硬件资源的虚拟化”和“服务任务的可编程”，即将传统的“单体式”（Monolithic）硬件设施分解为“基础硬件虚拟化及其API+管控软件”两部分：基础硬件通过API提供标准化的基本功能，进而在其上新增一个软件层替换“单体式”硬件中实现管控的“硬”逻辑，为用户提供更开放、灵活的系统管理服务。这一思想以虚拟化技术为基础，既解决了资源的效率过低的问题，也极大地提升了资源的弹性和灵活性。<br>（2）软件定义重构生产流程和控制模式：软件定义了生产流程，打破了传统的“设计—制造—测试—再设计”的过程，重构一个与实物制造相对应的<strong>虚拟制造空间</strong>，实现了研发设计、仿真、试验、制造、服务在虚拟空间并行运行，通过软件定义设计、产品、生产和管理等制造全环节的方式，推动制造过程快速迭代、持续优化和效率提升。软件定义了控制模式，在工业革命300年的历史进程中，控制装置作为技术完备系统（动力装置、传动装置、执行装置、控制装置）重要子系统之一发展最为迅猛，从珍妮纺织机到继电器开关，从电流调节器到数控机床，从嵌入式控制到基于云平台的远程控制，控制系统在核心技术上走过了一条“机械—机电—电子—数字—软件”的技术发展路线，软件技术的发展促使装备控制模式实现从物理控制到数字控制的革命性变迁。</p>
<h2 id="平台支撑：支撑制造业生态体系的构建"><a href="#平台支撑：支撑制造业生态体系的构建" class="headerlink" title="平台支撑：支撑制造业生态体系的构建"></a>平台支撑：支撑制造业生态体系的构建</h2><p>平台是基于信息技术构建的连接多个参与方的虚拟空间，是提供信息汇聚（信息门户平台）、产品交易（电商平台）和知识交易（工业互联网平台）的互联网信息服务载体。<br><img src="https://user-images.githubusercontent.com/6218739/78454309-dc046100-76c9-11ea-98a6-01cf8d5e577f.png" alt="2-3"><br>工业互联网平台是工业全要素连接的枢纽，是工业资源配置的核心。工业互联网的本质是推动工业资源和要素的解耦、整合和重构，构建新的发展理念、生产体系、组织架构和商业模式，宏观上提升国家资源高效配置能力，中观上培育产业生态构建能力，微观上打造企业新型能力。工业互联网平台推动资源优化的范围从单机、产线、车间、企业拓展到跨企业、跨区域，正在重塑制造业研发体系、生产范式和商业模式，推动企业研发实现研发主体跨部门协同化、研发流程并行化、研发模式闭环化，不断提升研发效率、缩短研发周期、降低研发成本，推动企业智能制造在更广的范围、更深的领域优化制造资源配置效率，实现价值创造从封闭的价值链向开放的价值网络拓展，推动企业从产品生产商向客户运营商转变，基于平台开展状态监测、故障诊断、预测预警、健康优化等各种新型智能服务。</p>
<h2 id="组织重构：重构社会分工协作体系"><a href="#组织重构：重构社会分工协作体系" class="headerlink" title="组织重构：重构社会分工协作体系"></a>组织重构：重构社会分工协作体系</h2><p>组织重构的本质就是进入数字经济时代后，数据作为一种新管理要素与传统技术、业务流程、组织结构相互影响、相互作用，极大地变革了不同群体的交流方式、交易方式，有效提升交易速率和质量，从而使得企业内外部交易成本呈现明显下降趋势，推动了组织向<strong>扁平化</strong>（极小化的自组织）、<strong>平台化</strong>（极大化的平台）和<strong>联盟化</strong>（生态化的产业联盟）方向发展。</p>
<h1 id="信息物理系统（CPS）：智能制造技术体系"><a href="#信息物理系统（CPS）：智能制造技术体系" class="headerlink" title="信息物理系统（CPS）：智能制造技术体系"></a>信息物理系统（CPS）：智能制造技术体系</h1><p>信息物理系统集成先进的信息通信和自动控制等技术构建了一个物理空间与赛博空间相互映射、实时交互、高效协同的复杂系统，是智能制造发展的关键技术支撑。<strong>信息物理系统的核心在于构建一套基于数据自动流动的状态感知、实时分析、科学决策、精准执行的闭环赋能体系，从而解决生产制造过程中的复杂性、不确定性，提高资源配置效率。</strong></p>
<h2 id="CPS的总体定位：支撑智能制造的综合技术体系"><a href="#CPS的总体定位：支撑智能制造的综合技术体系" class="headerlink" title="CPS的总体定位：支撑智能制造的综合技术体系"></a>CPS的总体定位：支撑智能制造的综合技术体系</h2><p>CPS打通状态感知、实时分析、科学决策、精准执行四个环节，连接了物理空间和赛博空间，构筑起数据自动流动的闭环赋能体系，通过隐性数据显性化、隐性知识显性化，实现由数据转化为信息、信息提炼成知识、知识转化决策，在这一过程中，解决了物理世界四个基本问题：首先是描述（Descriptive）物理世界发生了什么（What happened）；其次是诊断（Diagnostic）为什么会发生（Why it happened）；再次是预测（Predictive）接下来会怎样（What will happen）；最后是决策（Decision）应该怎么办（How to do），决策完成之后就可以驱动物理世界执行（Action），最终实现制造资源的优化配置。<br><img src="https://user-images.githubusercontent.com/6218739/78532410-c8681000-7819-11ea-9344-ee9f292123db.png" alt="3-1"></p>
<h2 id="CPS的技术要素：一硬、一软、一网、一平台"><a href="#CPS的技术要素：一硬、一软、一网、一平台" class="headerlink" title="CPS的技术要素：一硬、一软、一网、一平台"></a>CPS的技术要素：一硬、一软、一网、一平台</h2><p>状态感知就是通过各种各样的传感器感知物质世界的运行状态，实时分析就是通过工业软件实现数据、信息、知识的转化，科学决策就是通过大数据平台实现异构系统数据的流动与知识的分享，精准执行就是通过控制器、执行器等机械硬件实现对决策的反馈响应，这一切都依赖于一个实时、可靠、安全的网络。<br><img src="https://user-images.githubusercontent.com/6218739/78534790-9b1d6100-781d-11ea-9a17-31a872521639.png" alt="3-2"><br>可以把这一闭环赋能体系概括为“一硬”（感知和自动控制）（感知的本质是物理世界的数字化，通过各种芯片、传感器等智能硬件实现生产制造全流程中人、设备、物料、环境等隐性信息的显性化，自动控制体现为一系列动作或行为，作用于人、设备、物料和环境上，如分布式控制系统DCS、可编程逻辑控制器PLC及数据采集与监视控制系统SCADA等）、“一软”（工业软件）（工业软件是对工业研发设计、生产制造、经营管理、服务等全生命周期环节规律的模型化、代码化、工具化，是工业知识、技术积累和经验体系的载体，是实现工业数字化、网络化、智能化的核心）、“一网”（工业网络）（工业网络是连接工业生产系统和工业产品各要素的信息网络，通过工业现场总线、工业以太网、工业无线网络和异构网络集成等技术，能够实现工厂内各类装备、控制系统和信息系统的互联互通，以及物料、产品与人的无缝集成，并呈现扁平化、无线化、灵活组网的发展趋势）、“一平台”（工业互联网平台）（工业互联网平台是高度集成、开放和共享的数据服务平台，是跨系统、跨平台、跨领域的数据集散中心、数据存储中心、数据分析中心和数据共享中心，基于工业互联网平台推动专业软件库、应用模型库、产品知识库、测试评估库、案例专家库等基础数据和工具的开发集成和开放共享，实现生产全要素、全流程、全产业链、全生命周期管理的资源配置优化，以提升生产效率、创新模式业态，构建全新产业生态），即<strong>“新四基”</strong>。</p>
<h2 id="CPS的层级体系：单元级、系统级、系统之系统级"><a href="#CPS的层级体系：单元级、系统级、系统之系统级" class="headerlink" title="CPS的层级体系：单元级、系统级、系统之系统级"></a>CPS的层级体系：单元级、系统级、系统之系统级</h2><p>信息物理系统建设的过程就是从单一部件、单机设备、单一环节、单一场景的<strong>局部小系统</strong>不断向<strong>大系统</strong>、<strong>巨系统</strong>演进的过程，是从部门级到企业级，再到产业链级，乃至产业生态级演进的过程，是数据流闭环体系不断延伸和扩展的过程，并逐步形成相互作用的复杂系统网络，突破地域、组织、机制的界限，实现对人才、技术、资金等资源和要素的高效整合，从而带动产品、模式和业态创新。<br>CPS可以分为单元级、系统级、系统之系统（SoS）级三个层级：<br><img src="https://user-images.githubusercontent.com/6218739/78535244-665dd980-781e-11ea-962a-a8be050286a2.png" alt="3-3"><br>（1）<strong>单元级</strong>是具有不可分割性的信息物理系统的最小单元。它可以是一个部件或一个产品，通过“一硬”（如具备传感、控制功能的机械臂和传动轴承等）和“一软”（如嵌入式软件）就可构成“感知—分析—决策—执行”的数据闭环，具备了可感知、可计算、可交互、可延展、自决策的功能，典型的单元级最小单元如智能轴承、智能机器人、智能数控机床等。每个最小单元都是一个<strong>可被识别、定位、访问、联网的信息载体</strong>，通过在赛博空间中对物理实体的身份信息、几何形状、功能信息、运行状态等进行描述和建模，在虚拟空间也可以映射形成一个最小的数字化单元，并伴随着物理实体单元的加工、组装、集成不断叠加、扩展、升级，这一过程也是最小单元在虚拟和实体两个空间不断向系统级和系统之系统级同步演进的过程。<br>（2）<strong>系统级</strong>是“一硬、一软、一网”的有机组合。信息物理系统的多个最小单元（单元级）通过工业网络（如工业现场总线、工业以太网等，简称“一网”），实现更大范围、更宽领域的数据自动流动，构成智能生产线、智能车间、智能工厂，实现了多个单元级CPS的互联、互通和互操作，进一步提高制造资源优化配置的广度、深度和精度。系统级CPS基于多个单元级最小单元的状态感知、信息交互、实时分析，实现了局部制造资源的自组织、自配置、自决策、自优化。由传感器、控制终端、组态软件、工业网络等构成的分布式控制系统（DCS）和数据采集与监控系统（SCADA）是系统级CPS，由数控机床、机器人、AGV小车、传送带等构成的智能生产线是系统级CPS，通过制造执行系统（MES）对人、机、物、料、环等生产要素进行生产调度、设备管理、物料配送、计划排产和质量监控而构成的智能车间也是系统级CPS。<br>（3）<strong>系统之系统级（SoS级）</strong>是多个系统级CPS的有机组合，涵盖了“一硬、一软、一网、一平台”四大要素。SoS级CPS通过工业互联网平台，实现了跨系统、跨平台的互联、互通和互操作，促成了多源异构数据的集成、交换和共享的闭环自动流动，在全局范围内实现信息全面感知、深度分析、科学决策和精准执行。基于工业互联网平台，通过丰富开发工具、开放应用接口、共享数据资源、建设开发社区，加快各类工业App和平台软件的快速发展，形成一个赢者通吃的多边市场，构建一个新的产业生态。<br><img src="https://user-images.githubusercontent.com/6218739/78535482-d40a0580-781e-11ea-826e-1325d5b27d49.png" alt="3-4"><br><img src="https://user-images.githubusercontent.com/6218739/78535977-a7a2b900-781f-11ea-8488-c6ee4a0d421d.png" alt="3-5"></p>
<p>信息化的终极版图就是要在赛博空间构建起一个与物理空间泛在连接、虚实映射、实时联动、精准反馈、系统自治的<strong>数字孪生体</strong>。伴随着新技术、新方法、新模式的持续创新，物理空间与数字孪生的交互将实现从静态、动态向实时不断演进，这将驱动着赛博空间的数字孪生无限逼近真实物理空间，实现在单元级、系统级、SoS级等不同层次上的感知、分析、决策、控制。<br><img src="https://user-images.githubusercontent.com/6218739/78536476-7c6c9980-7820-11ea-8eb6-d89267a3afe3.png" alt="3-6"></p>
<h2 id="建设CPS的思路：数据自动流动是关键"><a href="#建设CPS的思路：数据自动流动是关键" class="headerlink" title="建设CPS的思路：数据自动流动是关键"></a>建设CPS的思路：数据自动流动是关键</h2><h3 id="资源优化是目标"><a href="#资源优化是目标" class="headerlink" title="资源优化是目标"></a>资源优化是目标</h3><p>（1）在资源优化的频率上，从静态优化走向动态优化，摒弃传统的以不变应万变的思维模式，根据需求和环境的变化实时调整资源配置方式（柔性生产）；<br>（2）在资源优化的范围上，从单点局部走向全局优化；<br>（3）在资源优化方法论上，从实体优化走向虚实结合优化，从传统的“试错法”向基于数字仿真的“模拟择优法”演变。<br>总体来看，基于CPS的资源优化过程是一个“螺旋式”上升的过程：<br><img src="https://user-images.githubusercontent.com/6218739/78536996-5398d400-7821-11ea-8979-85e9c9c391bf.png" alt="3-7"></p>
<h3 id="数据自动流动是关键"><a href="#数据自动流动是关键" class="headerlink" title="数据自动流动是关键"></a>数据自动流动是关键</h3><p>如果机器人、数控机床、立体仓库等生产设备的自动化替代的是体力劳动者，那么<strong>数据流动的自动化将替代脑力劳动者</strong>；如果生产设备的自动化是工业3.0，那么<strong>数据流动的自动化才是工业4.0的本质</strong>。<br>信息物理系统的本质就是构建一套数据自动流动的运行体系，即将正确的数据（所承载知识）在正确的时间传递给正确的人和机器，以信息流带动技术流、资金流、人才流、物资流，进而不断优化制造资源的配置效率。<br><strong>从数据流动的视角看，数字化解决了“有数据”的问题，网络化解决了“能流动”的问题，智能化解决了“自动流动”的问题。</strong><br><img src="https://user-images.githubusercontent.com/6218739/78537822-a2933900-7822-11ea-8884-7b067027aa04.png" alt="3-8"></p>
<h3 id="工业软件是核心"><a href="#工业软件是核心" class="headerlink" title="工业软件是核心"></a>工业软件是核心</h3><p>产品设计和全生命周期管理软件（如CAX、PLM等）建立了高度集成的数字化模型及研发工艺仿真体系，生产制造执行系统（MES）是企业实现纵向整合的核心，联通了设备、原料、订单、排产、配送等各主要生产环节和生产资源，企业管理系统（如ERP、WMS、CRM）为企业的业务活动进行科学管理，改变了企业管理模式和管理理念。</p>
<h3 id="新型能力培育是主线"><a href="#新型能力培育是主线" class="headerlink" title="新型能力培育是主线"></a>新型能力培育是主线</h3><p>企业推进信息物理系统建设，不能只单纯强调信息技术的先进性，而要围绕企业新型能力不断推进数据、技术、业务流程、组织结构的互动创新和持续优化，将技术的进步、组织结构的变革、业务流程的优化转化成企业的新型能力，诸如个性化定制、精益管理、风险管控、供应链协同、市场快速响应等新型能力，进而重构企业生产方式、服务模式和组织形态，不断获取差异化的可持续竞争优势。<br>当前我国企业关注信息化环境下的六大类能力，包括<strong>研发创新类</strong>（主要关注基于客户需求的数字化快速定制研发、产品研发、工艺设计、生产制造一体化，以及在线、异地协同研发）、<strong>生产管控类</strong>（重点关注大规模个性化定制生产管控、基于用户订单的柔性生产、服务型制造等）、<strong>供应链管理类</strong>、<strong>财务管控类</strong>、<strong>经营管控类</strong>（主要关注基于数据分析的智能决策、企业资源集中共享与协同运营等）及<strong>用户服务类</strong>（主要关注远程诊断与服务、客户互动与敏捷服务、产品全生命周期追溯等）能力。<br><img src="https://user-images.githubusercontent.com/6218739/78538651-e5094580-7823-11ea-9705-21bc8c68f0df.png" alt="3-9"></p>
<h3 id="系统解决方案是重点"><a href="#系统解决方案是重点" class="headerlink" title="系统解决方案是重点"></a>系统解决方案是重点</h3><p>推动信息物理系统的应用与发展既需要核心关键技术的突破，也需要一批具有广泛应用前景的行业系统解决方案。</p>
<h1 id="软件定义的未来工业"><a href="#软件定义的未来工业" class="headerlink" title="软件定义的未来工业"></a>软件定义的未来工业</h1><p>“软件定义制造”的本质是研发设计、生产制造、经营管理、运维服务等全生命周期业务环节规律的模型化、代码化、工具化，从根本上优化制造业产品装备、生产方式、组织管理和产业生态，是实现智能制造的核心。</p>
<h2 id="软件定义的本质"><a href="#软件定义的本质" class="headerlink" title="软件定义的本质"></a>软件定义的本质</h2><p>西门子2014年成立数字化工厂集团——“全球智能制造软硬件整合解决方案提供商”。<br>罗兰贝格公司的专家在谈到工业4.0时曾指出，未来的工业竞争存在两种可能的情景：<strong>软件革命</strong>和<strong>硬件进化</strong>。软件革命的情境是，来自硅谷的国际ICT巨头或新兴企业，以ICT产业领域的技术优势、竞争规则和商业模式重整制造业，通过构建制造业平台、解决方案和产业生态，掌控消费者，从而掌握制造业发展的主导权。硬件进化的情境是，传统制造业进一步强化核心工业技术的竞争优势，更加高效地解决传统工业体系封闭、分散、碎片化的问题，化解软件革命所带来的新生产模式的挑战。</p>
<p><img src="https://user-images.githubusercontent.com/6218739/78539845-ce63ee00-7825-11ea-9513-2201d5f3d36a.png" alt="表4-1"></p>
<p>一部工业革命300多年的发展史，就是一部人类社会如何<strong>创造新工具</strong>，更好地开发资源、不断地解放自己的发展史。信息通信技术牵引的新一轮工业革命，推动了人类生产工具从能量转换工具到知识和智能工具的演进，从开发自然资源到<strong>开发信息资源</strong>拓展，从解放人类体力到解放人类脑力跨越。<br><img src="https://user-images.githubusercontent.com/6218739/78540228-6cf04f00-7826-11ea-98b1-923589a6dab0.png" alt="4-1"></p>
<p>软件开发过程中最重要的逻辑就是if…then…，其核心理念是如何将现实世界可能出现的各种不确定性状态，通过“数据+算法”的分析判断转化为确定性选择，通过对这一逻辑结构的不断组合、嵌套，软件能够在现实世界中建立起一套认识、理解、化解不确定性的方法论。<br><img src="https://user-images.githubusercontent.com/6218739/78540423-b93b8f00-7826-11ea-98f1-358c006e44cd.png" alt="4-2"></p>
<p><strong>工业软件</strong>是人们对工业研发设计、生产制造、经营管理、运维服务等全生命周期业务环节认知规律的模型化、代码化、工具化，是工业知识、技术积累和经验体系的新载体，是实现工业数字化、网络化、智能化的核心。<strong>数字化</strong>正在从研发手段、管理手段、服务手段等环节走向产品、设备本身，各种芯片、传感器、智能微尘等都具有数字化计算内核（嵌入式系统）（有数据）；<strong>网络化</strong>正在从物质（机械，如螺栓、导线）连接向能量（物理场，如传感器、WiFi）连接、信息（数字，如比特）连接，甚至意识（生物场，如意识）连接演进（数据能流动）；进而，当网络无处不在、知识沉淀为数据和软件、信息可以在任何场景下以数字化形式调用时，<strong>智能化</strong>得以实现（数据能自动流动）。<br><img src="https://user-images.githubusercontent.com/6218739/78540816-4979d400-7827-11ea-9d3d-92b91332ded4.png" alt="4-3"></p>
<h2 id="软件定义产品"><a href="#软件定义产品" class="headerlink" title="软件定义产品"></a>软件定义产品</h2><p><img src="https://user-images.githubusercontent.com/6218739/78542746-36b4ce80-782a-11ea-8364-769180408f90.png" alt="表4-2"></p>
<h3 id="软件定义产品功能"><a href="#软件定义产品功能" class="headerlink" title="软件定义产品功能"></a>软件定义产品功能</h3><p><img src="https://user-images.githubusercontent.com/6218739/78542845-59df7e00-782a-11ea-8b44-bc19f6ede24f.png" alt="表4-3"><br>主要表现在：定义产品功能（笔记本电脑、智能手机）、增强产品效能（数控机床、CT机）、拓展产品边界（智能牙刷、智能水杯）。</p>
<h3 id="软件定义产品结构"><a href="#软件定义产品结构" class="headerlink" title="软件定义产品结构"></a>软件定义产品结构</h3><p>每次技术的重大突破，都会推动产品结构持续创新和演进，构建起新的产品结构。以“<strong>蒸汽机</strong>”为代表的第一次工业革命催生了工业文明；以“电力技术”为代表的第二次工业革命催生出的<strong>电动机</strong>，使工业产品从“蒸汽时代”迈向“电气时代”。信息技术和自动化技术的应用，催生出可控制电压电流的<strong>伺服电机</strong>，“电气一代”跃升为“数控一代”。当前，软件正在与机械、电子、控制等传统工业技术相结合，推动产品逐步向“智能一代”进化，并构建新的产品结构，“一代软件、一代产品”的时代正在到来。<br>（1）功能结构的重构<br>信息技术革命带来的重大变革是，信息技术与自动控制、机械制造技术的集成引发机械产品结构和功能的重大变革，以往仅能靠机械和电子元件等物理实体实现的功能，可以通过软件来实现，产品功能不断丰富，同时物理结构复杂性和成本不断降低。<br>在飞机飞行中，基于对状态信息实时感知、计算、控制的<strong>电传操纵系统</strong>取代了复杂、脆弱和笨重的液压式飞行操纵系统，飞控软件和电子系统使得飞行器的结构更简化、更轻巧、更可靠。同时，飞机、汽车、船舶都可以通过<strong>数字驾驶舱</strong>技术，用简洁的显示屏替代传统的数据仪表。<br>（2）性能结构的优化<br>软件的进化正带来产品设计模式及产品性能结构的重大变化，传统的计算机辅助设计（Computer-Aided Design, CAD）正在向计算机主导设计（Computer-Automated Design,CAD）演进。传统的产品结构设计主要依靠设计工程师的经验和水平，而现在计算机辅助设计正在向计算机自动设计转变。设计仿真工具可根据产品的参数要求，通过不断的迭代优化，自动地给出产品结构的最佳方案，并结合材料技术和制造技术的变革制造出性能更加优秀的产品。<br>（3）价值结构的再造<br>产品基于软件功能的增加带来了更高的价值，软件成为产品价值创造的重要来源。</p>
<h2 id="软件定义企业管理流程"><a href="#软件定义企业管理流程" class="headerlink" title="软件定义企业管理流程"></a>软件定义企业管理流程</h2><p>发展和成熟于不同技术时代的ERP、CRM、SCM、PLM等管理软件，是某种管理理论、经验和知识的表达、重现和固化，是一种管理规律认知的代码化、软件化。基于软件的业务流程管理正在成为现代企业核心竞争力的重要组成部分。<br><img src="https://user-images.githubusercontent.com/6218739/78544544-df642d80-782c-11ea-8028-6971e74c9fa3.png" alt="4-4"></p>
<h3 id="软件支撑和定义的研发设计模式"><a href="#软件支撑和定义的研发设计模式" class="headerlink" title="软件支撑和定义的研发设计模式"></a>软件支撑和定义的研发设计模式</h3><p><img src="https://user-images.githubusercontent.com/6218739/78544904-7204cc80-782d-11ea-9ab7-d6aba838924b.png" alt="表4-4"><br><img src="https://user-images.githubusercontent.com/6218739/78545180-ef304180-782d-11ea-9cbe-4aca146792cf.png" alt="4-5"><br><img src="https://user-images.githubusercontent.com/6218739/78545476-6c5bb680-782e-11ea-9489-355744922cd3.png" alt="4-6"></p>
<h3 id="软件支撑和定义的经营管理模式"><a href="#软件支撑和定义的经营管理模式" class="headerlink" title="软件支撑和定义的经营管理模式"></a>软件支撑和定义的经营管理模式</h3><p>企业经营管理是指运用先进科学管理理念、方法和工具对企业资源、供应链、客户关系等业务活动进行科学管理和系统优化。随着信息技术的发展，ERP、SCM、CRM等管理软件成为推动企业资源管理、供应链管理、客户关系管理的有效工具，改变了企业管理模式和管理理念。这些管理软件的本质是管理理念、方法的模型化、代码化、软件化，通过代码集成了数以千计优秀企业的经营管理精粹，提炼总结了行业知识和最佳实践，并不断进行迭代优化。<br><img src="https://user-images.githubusercontent.com/6218739/78545765-ec821c00-782e-11ea-8725-8f4beed947b4.png" alt="表4-5"><br>管理软件把看不见、摸不着的管理思想、企业文化变成了可看、可学、可复制的标准化模块，管理软件的本质是管理思想的代码化。</p>
<h3 id="软件支撑和定义的组织架构"><a href="#软件支撑和定义的组织架构" class="headerlink" title="软件支撑和定义的组织架构"></a>软件支撑和定义的组织架构</h3><p>企业功能平台化、运营决策小型化、多部门协同化。</p>
<h2 id="软件定义企业生产方式"><a href="#软件定义企业生产方式" class="headerlink" title="软件定义企业生产方式"></a>软件定义企业生产方式</h2><h3 id="制造范式的迁移：从实体制造到虚拟制造，以快速迭代、持续优化、数据驱动重建制造效率、成本、质量管控体系"><a href="#制造范式的迁移：从实体制造到虚拟制造，以快速迭代、持续优化、数据驱动重建制造效率、成本、质量管控体系" class="headerlink" title="制造范式的迁移：从实体制造到虚拟制造，以快速迭代、持续优化、数据驱动重建制造效率、成本、质量管控体系"></a>制造范式的迁移：从实体制造到虚拟制造，以快速迭代、持续优化、数据驱动重建制造效率、成本、质量管控体系</h3><p><strong>新概念泛滥反映了制造范式的迁移。</strong><br>虚拟制造：制造业数字化、网络化、智能化的过程，是在赛博空间重建制造流程，并基于此不断提升制造效率的过程。<br>数字样机从传统的<strong>几何样机</strong>向<strong>性能样机</strong>、<strong>制造样机</strong>和<strong>维护样机</strong>拓展，并将进一步进化为与实体产品对应的产品<strong>数字孪生</strong>（Digital Twin）。<br><img src="https://user-images.githubusercontent.com/6218739/78629620-29e7b780-78ca-11ea-8afe-80bcc3e79f07.png" alt="4-7"></p>
<h3 id="制造模式的变革：从规模生产到定制生产，以数据的自动流动化解制造系统的不确定性、多样性和复杂性"><a href="#制造模式的变革：从规模生产到定制生产，以数据的自动流动化解制造系统的不确定性、多样性和复杂性" class="headerlink" title="制造模式的变革：从规模生产到定制生产，以数据的自动流动化解制造系统的不确定性、多样性和复杂性"></a>制造模式的变革：从规模生产到定制生产，以数据的自动流动化解制造系统的不确定性、多样性和复杂性</h3><p><strong>智能制造与传统制造的本质区别</strong>在于，在生产制造过程中，人员、机器、产品之间<strong>信息交流的载体、方式、效率</strong>不同。智能制造的基础是数字化，传感器、智能装备终端、工业网络、工业软件的大量使用促进了生产制造全过程的数字化，数据采集、传输、存储、分析和挖掘的手段相比传统制造更加丰富，大量蕴含在生产制造过程中的隐性数据不断被采集、汇聚、加工，形成新的知识、决策，不断优化制造资源的配置效率，数据的自动有序流动实现了物资流、资金流的高效利用。<br>美国国家标准与技术研究院（NIST）曾经提出，<strong>智能制造就是要解决差异性更大的定制化服务、更小的生产批量和不可预知的供应链变更</strong>。智能制造的一个重要任务就是应对制造复杂系统的不确定性，这种复杂性既来自产品的复杂性，也来自定制化生产等新生产方式所带来的制造成本、质量和效率的挑战。</p>
<h3 id="制造系统的重建：从封闭体系走向开放体系，以网络化协同实现制造资源局部优化向全局优化的演进"><a href="#制造系统的重建：从封闭体系走向开放体系，以网络化协同实现制造资源局部优化向全局优化的演进" class="headerlink" title="制造系统的重建：从封闭体系走向开放体系，以网络化协同实现制造资源局部优化向全局优化的演进"></a>制造系统的重建：从封闭体系走向开放体系，以网络化协同实现制造资源局部优化向全局优化的演进</h3><p>软件产业发展的过程就是持续优化资源配置的过程，从计算机辅助设计（CAD）到基于模型的设计（MBD）、基于模型的企业（MBE），从物资需求计划（MRP）到企业资源计划（ERP），从制造执行系统（MES）到制造运营管理（MOM），资源优化的范围越来越广。<br><img src="https://user-images.githubusercontent.com/6218739/78644935-4810e000-78e9-11ea-924d-052af67288e3.png" alt="4-8"></p>
<h2 id="软件定义企业创新能力"><a href="#软件定义企业创新能力" class="headerlink" title="软件定义企业创新能力"></a>软件定义企业创新能力</h2><p>包括产品研发创新能力、精益及柔性生产能力（精益生产的核心思想是：通过持续改进，杜绝一切无效作业与浪费）、市场需求实时响应能力、全生命周期服务能力（可预测性维护能力额制造资源分享能力）。</p>
<h2 id="软件定义产业生态"><a href="#软件定义产业生态" class="headerlink" title="软件定义产业生态"></a>软件定义产业生态</h2><p>智能制造发展的一个重要目标就是在商务系统、制造工厂和供应商之间实现企业生态联盟的集成。而这种生态构成，是以集成化软件产品为核心、跨界合作为重要发展模式，其具体表现在以下3个方面。<br>（1）构建基于智能机器的数据采集系统。通过将不同标准接口的设备进行统一集成，将不同类型的机械设备（不同年代、不同生产商）进行连接打通，实现对<strong>OT层数据</strong>的打通及采集，进而将设备厂商、元器件厂商纳入生态系统中。<br>（2）形成智能分析工具。通过整合<strong>IT软件</strong>企业、大数据专业服务商及互联网企业的云服务能力，提升数据分析速度和精度，从而支撑设备、资产、流程优化。<br>（3）搭建开放平台实现工业App的开发。随着新一代信息技术和制造技术的融合，智能制造新型集成化产品不断涌现，显现出以开放化平台为核心，向下整合硬件资源、向上承载软件应用的发展趋势，集成了多种网络通信协议、应用协议、数据协议的工业IoT平台，成为IT、自动化、制造领域领先企业的新宠。</p>
<h1 id="工业4-0：他山之石的启示"><a href="#工业4-0：他山之石的启示" class="headerlink" title="工业4.0：他山之石的启示"></a>工业4.0：他山之石的启示</h1><h2 id="工业4-0：为什么"><a href="#工业4-0：为什么" class="headerlink" title="工业4.0：为什么"></a>工业4.0：为什么</h2><p>德国工业4.0的概念最大的成功在于，它<strong>把几百年工业发展的历史与现代信息技术趋势进行了完美的集成</strong>，是继承性与创新性的统一、理论性与通俗化的统一、严肃性与时尚性的统一。</p>
<h2 id="工业4-0：是什么"><a href="#工业4-0：是什么" class="headerlink" title="工业4.0：是什么"></a>工业4.0：是什么</h2><p>与国际社会关于第三次工业革命的说法不同，德国学术界和产业界认为，前三次工业革命的发生，分别源于机械化、电力和信息技术。他们<strong>将18世纪引入机械制造设备定义为工业1.0</strong>，<strong>将20世纪初的电气化定义为工业2.0</strong>，<strong>将始于20世纪70年代的生产工艺自动化定义为工业3.0</strong>，而<strong>物联网和制造业服务化迎来了以智能制造为主导第四次工业革命</strong>，或革命性的生产方法，即工业4.0。德国工业4.0战略旨在通过充分利用信息通信技术和信息物理系统（CPS）相结合的手段，推动制造业向智能化转型。</p>
<h3 id="工业4-0是互联"><a href="#工业4-0是互联" class="headerlink" title="工业4.0是互联"></a>工业4.0是互联</h3><p>工业4.0的核心是<strong>连接</strong>，要把设备、生产线、工厂、供应商、产品、客户紧密地连接在一起。“工业4.0”适应了万物互联的发展趋势，将无处不在的传感器、嵌入式终端系统、智能控制系统、通信设施通过信息物理系统（CPS）形成一个智能网络，使产品与生产设备之间、不同的生产设备之间，以及数字世界和物理世界之间能够互联，使机器、工作部件、系统及人类会通过网络保持数字信息的交流。<br>（1）生产设备之间的互联。<strong>从工业2.0到工业3.0时代的重要标志是，单机智能设备的广泛普及。</strong>工业4.0工作组把1969年第一个可编程逻辑控制器Modicon 084的使用作为工业3.0的起点，其核心是各种数控机床、工业机器人自动化设备在生产环节的推广，我们可以把它理解为单机设备智能化水平不断提升并普及推广。<strong>工业4.0的核心是单机智能设备的互联</strong>，不同类型和功能的单机智能设备的互联组成智能生产线，不同智能生产线间的互联组成智能车间，智能车间的互联组成智能工厂，不同地域、行业、企业的智能工厂的互联组成一个制造能力无所不在的信息物理系统，这些单机智能设备、智能生产线、智能车间及智能工厂可以自由地、动态地组合，以满足不断变化的制造需求，这是工业4.0区别于工业3.0的重要特征。<br>（2）设备和产品的互联。工业4.0意味着智能工厂能够自行运转，<strong>零件与机器可以进行交流</strong>。由于产品和生产设备之间能够通信，使产品能理解制造的细节及自己将被如何使用。同时，它们能协助生产过程，回答诸如“我是什么时候被制造的”“哪组参数应该被用来处理我”“我应该被传送到哪”等问题。<br>（3）虚拟和现实的互联。信息物理系统（CPS）是工业4.0的核心，它通过将物理设备连接到互联网上，让物理设备具有计算、通信、控制、远程协调和自治五大功能，从而实现虚拟网络世界与现实物理世界的融合。<br>（4）万物互联（Internet of Everything, IoE）。信息技术发展的终极目标是实现无所不在的连接，所有产品都将成为一个网络终端。万物互联就是人、物、数据和程序通过互联网连接在一起，实现人类社会所有人和人、人和物及物和物之间的互联，重构整个社会的生产工具、生产方式和生活场景。</p>
<h3 id="工业4-0是集成"><a href="#工业4-0是集成" class="headerlink" title="工业4.0是集成"></a>工业4.0是集成</h3><p>工业4.0将无处不在的传感器、嵌入式终端系统、智能控制系统、通信设施通过CPS形成一个智能网络，使人与人、人与机器、机器与机器及服务与服务之间能够互联，从而实现横向、纵向和端对端的高度集成。<br>（1）纵向集成。企业信息化在各个部门发展阶段的里程碑，就是企业内部信息流、资金流和物流的集成，是在哪个层次、哪个环节、哪个水平上，是在生产环节上的集成（如研发设计内部信息集成），还是跨环节的集成（如研发设计与制造环节的集成），还是产品全生命周期的集成（如从产品研发、设计、计划、工艺到生产、服务等全生命周期的信息集成）。工业4.0所要追求的就是在企业内部实现所有环节信息无缝链接，这是所有智能化的基础。<br>（2）横向集成。在市场竞争牵引和信息技术创新驱动下，每个企业都在追求生产过中的信息流、资金流、物流无缝链接与有机协同，在过去，这一目标主要集中在企业内部，但现在企业要实现新的目标：从企业内部的信息集成走向产业链信息集成，从企业内部协同研发体系走向企业间的研发网络，从企业内部的供应链管理走向企业间的协同供应链管理，从企业内部的价值链重构走向企业间的价值链重构。横向集成是企业之间通过价值链及信息网络所实现的一种资源整合，为实现各企业间的无缝合作，提供实时产品与服务，推动企业间研产供销、经营管理与生产控制、业务与财务全流程的无缝衔接和综合集成，实现产品开发、生产制造、经营管理等在不同企业间的信息共享和业务协同。<br>（3）端到端集成。所谓端到端就是围绕产品全生命周期的价值链创造，通过价值链上不同企业资源的整合，实现从产品设计、生产制造、物流配送、使用维护的产品全生命周期的管理和服务，它以产品价值链创造来集成优化供应商（一级、二级、三级……）、制造商（研发、设计、加工、配送）、分销商（一级、二级、三级……），以及客户信息流、物流和资金流，在为客户提供更有价值的产品和服务的同时，重构产业链各环节的价值体系。</p>
<h3 id="工业4-0是数据"><a href="#工业4-0是数据" class="headerlink" title="工业4.0是数据"></a>工业4.0是数据</h3><p>据将会呈现爆炸式增长态势。伴随着工业互联网、工业大数据、信息物理系统（CPS）的推广，智能装备、智能终端的普及，以及各种各样传感器加速普及使用，将会带来无所不在的感知和无所不在的连接，所有的生产装备、感知设备、联网终端，包括生产者本身都在源源不断地产生数据，这些数据将会渗透到企业运营、价值链乃至产品的整个生命周期，是工业4.0和制造革命的基石。（产品数据、运营数据、价值链数据、外部数据）</p>
<h3 id="工业4-0是创新"><a href="#工业4-0是创新" class="headerlink" title="工业4.0是创新"></a>工业4.0是创新</h3><p>技术创新（新型传感器、集成电路、人工智能、移动互联、CPS、虚拟制造等）、产品创新（智能产品、智能生产线、智能车间、智能工厂等）、模式创新（生产模式：机器分析判断+机器生产制造，商业模式：网络众包、异地协同设计、大规模个性化定制、精准供应链管理）、业态创新（工业云服务、工业大数据应用、物联网应用）、组织创新（业务流程重组和企业组织再造）</p>
<h3 id="工业4-0是转型"><a href="#工业4-0是转型" class="headerlink" title="工业4.0是转型"></a>工业4.0是转型</h3><p>（1）从大规模生产向<strong>个性化定制</strong>转型。<br>（2）从生产型制造向<strong>服务型制造</strong>转型。<br>（3）从要素驱动向<strong>创新驱动</strong>转型。</p>
<h2 id="工业4-0：如何看"><a href="#工业4-0：如何看" class="headerlink" title="工业4.0：如何看"></a>工业4.0：如何看</h2><p>德国工业4.0战略与中国的<strong>信息化和工业化深度融合</strong>战略在核心理念、主要内容和具体做法等诸多方面殊途同归。</p>
<h2 id="工业4-0：怎么干"><a href="#工业4-0：怎么干" class="headerlink" title="工业4.0：怎么干"></a>工业4.0：怎么干</h2><p>可以从五个方面认识和理解智能制造，即<strong>产品的智能化</strong>（即把传感器、处理器、存储器、通信模块、传输系统融入到各种产品中，使产品具备动态存储、感知和通信能力，实现产品的可追溯、可识别、可定位）、<strong>装备的智能化</strong>（指通过先进制造、人工智能等技术的集成融合，形成具有感知、决策、执行、自主学习及维护等自组织、自适应功能的智能生产系统，以及网络化、协同化的生产设施。装备的智能化至少是在两个维度上进行的，即单机智能化及单机设备互联形成的智能生产线、智能车间、智能工厂）、<strong>生产的智能化</strong>（重组客户、供应商、销售商及企业内部组织的关系，重构生产体系中信息流、产品流、资金流的运行模式，重建新的产业价值链、生态系统和竞争格局）、<strong>管理的智能化</strong>（通过将信息技术与现代管理理念融入企业管理，实现企业流程再造、信息集成、智能管控、组织优化，以形成数据驱动型的企业，从而不断提升信息化背景下企业的核心竞争力）和<strong>服务的智能化</strong>（既体现为企业如何高效、准确、及时挖掘客户的潜在需求并实时响应，也体现为产品交付后对产品实现线上线下（O2O）服务，实现产品的全生命周期管理）。</p>
<h1 id="工业互联网：从基于产品的分工到基于知识的分工"><a href="#工业互联网：从基于产品的分工到基于知识的分工" class="headerlink" title="工业互联网：从基于产品的分工到基于知识的分工"></a>工业互联网：从基于产品的分工到基于知识的分工</h1><p>从智能制造到工业互联网，是信息技术体系从传统架构向云架构的迁移，是制造资源从局部优化到全局优化的演进，是业务协同从企业内部到产业链的扩展，是竞争模式从单一企业竞争到生态体系竞争的升级，是产业分工从基于产品的分工到基于知识的分工深化，但其内部逻辑是一致的——以数据的自动流动化解复杂制造系统的不确定性，提高制造资源的配置效率。</p>
<h1 id="探索制造业与互联网融合发展之路"><a href="#探索制造业与互联网融合发展之路" class="headerlink" title="探索制造业与互联网融合发展之路"></a>探索制造业与互联网融合发展之路</h1><p>李克强总理强调：“我国经济保持中高速增长、迈向中高端水平必须要有基本依托，这个基本依托就是推动形成<strong>大众创业</strong>、<strong>万众创新</strong>的新动能。”大型制造企业、电信企业和互联网企业积极构建基于互联网的开放式“双创平台”，在推动制造业转型升级方面发挥了重要作用。<br><img src="https://user-images.githubusercontent.com/6218739/78683279-ac4f9600-7921-11ea-9831-d0751f1f496f.png" alt="6-1"><br><img src="https://user-images.githubusercontent.com/6218739/78684660-511ea300-7923-11ea-9297-0a87a3cb6e9a.png" alt="表6-1"><br>（1）网络化协同制造<br>网络化协同制造是指企业借助互联网或工业云平台，发展企业间协同研发、众包设计、供应链协同等新模式，以此能有效降低资源获取成本，大幅延伸资源利用范围，打破封闭疆界，加速从单打独斗向产业协同转变，促进产业整体竞争力提升。<br><img src="https://user-images.githubusercontent.com/6218739/78685367-3d277100-7924-11ea-9644-1b0595bf5faa.png" alt="6-2"></p>
<p>（2）个性化定制<br>个性化定制是从传统工业过渡到智能制造阶段的重要标志。其本质是利用互联网平台和智能工厂建设，<strong>将用户需求直接转化为生产排单</strong>，开展以用户为中心的个性定制与按需生产，以有效满足市场多样化需求，解决制造业长期存在的库存和产能问题，从而实现产销动态平衡，满足成本、质量和效率等多方面需求。<br><img src="https://user-images.githubusercontent.com/6218739/78866121-643f8900-7a71-11ea-8ddb-770ce9d82135.png" alt="6-3"><br><img src="https://user-images.githubusercontent.com/6218739/78866244-99e47200-7a71-11ea-96a1-d72b0b5c1590.png" alt="6-4"><br><img src="https://user-images.githubusercontent.com/6218739/78866340-cf895b00-7a71-11ea-971d-96ec659306d9.png" alt="表6-2"></p>
<p>（3）服务型制造<br>制造业竞争正面临以下四个转变：一是市场需求正从产品导向向产品服务系统导向转变；二是高价值环节从制造环节为主向服务环节为主转变；三是基于产品服务的竞争正成为增强产品竞争优势的重要途径；四是市场交易正从短期交易向长期交易转变。<br>（4）制造业分享经济<br>《国务院关于深化制造业与互联网融合发展的指导意见》强调：“推动中小企业制造资源与互联网平台全面对接，<strong>实现制造能力的在线发布、协同和交易</strong>，积极发展面向制造环节的分享经济，打破企业界限，共享技术、设备和服务，提升中小企业快速响应和柔性高效的供给能力。”</p>
<p>“新四基”（感知和自动控制（一硬）、工业软件（一软）、工业网络（一网）、工业互联网平台（一平台））是深化制造业与互联网融合的关键支撑。<br><img src="https://user-images.githubusercontent.com/6218739/78867261-660a4c00-7a73-11ea-97e6-ebaa77872296.png" alt="6-5"></p>
<h1 id="工业互联网平台：为什么，是什么，怎么看？"><a href="#工业互联网平台：为什么，是什么，怎么看？" class="headerlink" title="工业互联网平台：为什么，是什么，怎么看？"></a>工业互联网平台：为什么，是什么，怎么看？</h1><h2 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h2><p>工业互联网是<strong>新一代信息通信技术与现代工业技术深度融合的产物</strong>，是<strong>制造业数字化、网络化、智能化的重要载体</strong>，也是全球新一轮产业竞争的制高点。工业互联网通过构建<strong>连接机器、物料、人、信息系统</strong>的基础网络，实现工业数据的<strong>全面感知</strong>、<strong>动态传输</strong>、<strong>实时分析</strong>，从而形成<strong>科学决策与智能控制</strong>，以提高制造资源配置效率。</p>
<p><strong>5G</strong>、<strong>窄带物联网（NB-IoT）</strong>、<strong>时间敏感网络（TSN）</strong>、<strong>OPCUA</strong>等网络技术及<strong>工业以太网</strong>、<strong>工业总线</strong>等通信协议的应用，为制造企业系统和设备数据的互联汇聚创造了条件，构建了<strong>低延时、高可靠、广覆盖的工业网络</strong>，实现了制造系统各类数据便捷、高效、低成本的汇聚。<strong>大数据和人工智能</strong>技术的发展，实现了不同来源、不同结构工业数据的采集与集成、高效处理分析，进而帮助制造企业提升价值。各领域技术的不断发展，并与工业技术融合，构建起了工业互联网平台综合技术体系，工业互联网平台应运而生。<br>在这一进程中，尤其值得关注的是<strong>云计算</strong>技术的发展。云计算技术的发展正在重构软件架构体系和商业模式。高弹性、低成本的IT基础设施日益普及，软件部署由本地化逐渐向云端迁移，软件形态从单体式向微服务不断演变。<strong>开源云架构</strong>、<strong>容器</strong>技术为可重构、可移植、可伸缩的应用服务敏捷地开发和快速部署提供保障，各类新型工业App逐步推广应用，推动了制造资源优化配置。<br><img src="https://user-images.githubusercontent.com/6218739/78869528-512fb780-7a77-11ea-9675-14841707c882.png" alt="7-1"></p>
<h2 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h2><h3 id="工业互联网的架构"><a href="#工业互联网的架构" class="headerlink" title="工业互联网的架构"></a>工业互联网的架构</h3><p><img src="https://user-images.githubusercontent.com/6218739/78872958-a1f5df00-7a7c-11ea-8232-3194d614d2a8.png" alt="7-2"><br>（1）数据采集（边缘层）是基础<br>数字采集的本质是利用泛在感知技术对多源设备、异构系统、运营环境、人等要素信息进行实时高效采集和云端汇聚。核心就是要构建一个精准、实时、高效的数据采集体系，把数据采集上来，通过协议转换和边缘计算，将一部分数据在<strong>边缘侧</strong>进行处理，这适用于对实时性、短周期数据的快速处理，处理结果将直接返回到机器设备；将另一部分数据传到<strong>云端</strong>，通过云计算更强大的数据运算能力和更快的处理速度，对非实时、长周期数据进行综合利用分析，从而进一步优化形成决策。</p>
<p>工业现场数据云端汇聚面临的突出问题可以总结为“三不”：<strong>不敢传（数据安全问题）</strong>、<strong>不需传（本地化和实时性问题）</strong>、<strong>不能传（协议标准不统一）</strong>，即无法支撑实时数据采集和实时分析、智能优化和科学决策。一是工业数据采集存在数据安全隐患。工业数据采集会涉及大量重要工业数据和用户隐私信息，在传输和存储时都会存在一定的数据安全隐患，也存在黑客窃取数据、攻击企业生产系统的风险。因此，急需从技术、管理和法律法规等多方面保障数据安全。二是工业协议标准不统一且数据开放性不够。目前在工业数据采集领域，存在Modbus、CAN、ControlNet、DeviceNet、Profibus、Zigbee等各种工业协议标准，各个自动化设备生产及集成商还会自己开发各种私有的工业协议，各种协议标准不统一、互不兼容；同时很多设备和系统的数据开放性不够，缺乏数据接口及文档说明，导致协议适配解析和数据互联互通困难。三是工业数据采集实时性要求难以保证。生产线的高速运转，精密生产和运动控制等场景对数据采集的实时性要求不断提高，传统数据采集技术对于高精度、低时延的工业场景难以保证重要信息实时采集和上传，无法满足生产过程的实时监控需求。<br><img src="https://user-images.githubusercontent.com/6218739/78873281-3102f700-7a7d-11ea-9374-ce31ebd41d69.png" alt="7-3"></p>
<p>当前，突破数据采集瓶颈的主要思路包括以下两个方面：<br>（a）通过协议兼容、转换实现多源设备、异构系统的数据可采集、可交互、可传输。GE通过将数据采集转换模块Predix Machine部署在现场传感器、控制器和网关中，以多种方式实现不同协议的兼容和转换，来完成工业现场数据采集及云端汇聚。西门子通过在设备端部署数据采集模块MindConnect Nano，实现通用协议兼容和私有协议转换及云端汇聚。<br><img src="https://user-images.githubusercontent.com/6218739/78873502-91923400-7a7d-11ea-8db9-267b70d8ea63.png" alt="7-4"></p>
<p>（b）通过边缘计算等技术在设备层进行数据预处理，进而大幅提高数据采集、传输效率，以降低网络接入、存储、计算等成本，提高现场控制反馈的及时性。</p>
<p>（2）IaaS是支撑<br>IaaS是通过虚拟化技术将计算、存储、网络等资源池化，向用户提供可计量、弹性化的资源服务。IaaS是工业互联网平台运行的载体和基础，其实现了工业大数据的存储、计算、分发。</p>
<p>（3）工业PaaS（平台层）是核心<br>工业PaaS本质是一个可扩展的工业云操作系统，它能够实现对软硬件资源和开发工具的接入、控制和管理，为应用开发提供了必要接口及存储计算、工具资源等支持，它为工业应用软件开发提供一个基础平台。</p>
<p>工业PaaS面临的突出问题是开发工具不足、行业算法和模型库缺失、模块化组件化能力较弱，现有通用PaaS平台尚不能完全满足工业级应用需要。当前，工业PaaS建设的总体思路是，通过对通用PaaS平台的深度改造，构造满足工业实时、可靠、安全需求的云平台，采用微服务架构，<strong>将大量工业技术原理、行业知识、基础模型规则化、软件化、模块化，并封装为可重复使用的微服务</strong>，通过对微服务的灵活调用和配置，降低应用程序开发门槛和开发成本，提高开发、测试、部署效率，为海量开发者汇聚、开放社区建设提供技术支撑和保障。</p>
<p>（4）工业App（应用层）是关键<br>工业App主要表现为面向特定工业应用场景，整合全社会资源推动工业技术、经验、知识和最佳实践的模型化、软件化、再封装（工业App），用户通过对工业App的调用实现对特定制造资源的优化配置。工业App由通用云化软件和专用App应用构成，它面向企业客户提供各类软件和应用服务。<br>当前，工业App发展的总体思路包括以下两个方面。<br>（a）传统的CAD、CAE、ERP、MES等研发设计工具和管理软件加快<strong>云化改造</strong>。云化迁移是当前软件产业发展的基本趋势，全球软件产品“云化”步伐不断加快，基于传统集中式架构的软件开发部署模式正在向高可用、易扩展、低成本的分布式云架构转型。<br>（b）围绕多行业、多领域、多场景的云应用需求开发<strong>专用App</strong>应用。大量开发者通过对工业PaaS层微服务的调用、组合、封装和二次开发，将工业技术、工艺知识和制造方法固化和软件化，开发形成了专用App应用。</p>
<p><img src="https://user-images.githubusercontent.com/6218739/78874309-c5ba2480-7a7e-11ea-8040-2e051d5d9eb0.png" alt="7-5"><br><img src="https://user-images.githubusercontent.com/6218739/78874890-ad96d500-7a7f-11ea-990d-54a3a76f58a1.png" alt="表7-1"><br><img src="https://user-images.githubusercontent.com/6218739/78875053-e46ceb00-7a7f-11ea-8d21-f7136ebf183a.png" alt="7-6"></p>
<h3 id="工业互联网平台的本质"><a href="#工业互联网平台的本质" class="headerlink" title="工业互联网平台的本质"></a>工业互联网平台的本质</h3><p>工业互联网平台的本质是一套面向制造业数字化、网络化、智能化的解决方案，这套解决方案与传统方案最本质的区别就是基于<strong>云架构</strong>，这是需求场景、技术演进、生态构建共同作用的必然结果，其基本的逻辑就是“数据+模型=服务”，就是如何采集制造系统海量数据，把来自机器设备、业务系统、产品模型、生产过程及运行环境中的大量数据汇聚到工业PaaS平台上，实现物理世界隐性数据的显性化，实现数据的及时性、完整性、准确性，并将技术、知识、经验和方法以数字化模型的形式沉淀到平台上，形成各种软件化的模型（机理模型、数据分析模型等），基于这些数字化模型对各种数据进行分析、挖掘、展现，以提供产品全生命周期管理、协同研发设计、生产设备优化、产品质量检测、企业运营决策、设备预测性维护等多种多样的服务，从而实现数据—信息—知识—决策的迭代，最终把正确的数据、以正确的方式、在正确的时间传递给正确的人和机器，优化制造资源配置效率。<br><img src="https://user-images.githubusercontent.com/6218739/78875258-30b82b00-7a80-11ea-809c-b5951229453a.png" alt="7-7"><br><img src="https://user-images.githubusercontent.com/6218739/78875362-59d8bb80-7a80-11ea-88b4-d6912e89ce50.png" alt="7-8"><br><img src="https://user-images.githubusercontent.com/6218739/78875620-c18f0680-7a80-11ea-8ba3-25788370fd92.png" alt="7-9"><br><img src="https://user-images.githubusercontent.com/6218739/78876166-8214ea00-7a81-11ea-9a37-3d666c26af9b.png" alt="表7-2"><br><img src="https://user-images.githubusercontent.com/6218739/78876352-cacca300-7a81-11ea-9d60-9f8105ce513e.png" alt="7-10"><br><img src="https://user-images.githubusercontent.com/6218739/78876472-f8195100-7a81-11ea-8910-88ea6ad898aa.png" alt="7-11"><br><img src="https://user-images.githubusercontent.com/6218739/78876566-1aab6a00-7a82-11ea-8f2e-4734b7273063.png" alt="7-12"></p>
<h3 id="工业互联网平台的核心"><a href="#工业互联网平台的核心" class="headerlink" title="工业互联网平台的核心"></a>工业互联网平台的核心</h3><p>工业PaaS中最核心的要素组件是<strong>基于微服务架构的数字化模型</strong>。数字化模型将大量工业技术原理、行业知识、基础工艺、模型工具等规则化、软件化、模块化，并封装为可重复使用的组件。<br><img src="https://user-images.githubusercontent.com/6218739/78877416-64e11b00-7a83-11ea-9f64-56d008966e11.png" alt="7-13"><br>（1）数字化模型是什么？数字化模型可以分为两种，一种是<strong>机理模型</strong>，包括<strong>基础理论模型</strong>（如飞机、汽车、高铁等制造过程中涉及的流体力学、热力学、空气动力学方程等模型），<strong>流程逻辑模型</strong>（如ERP、供应链管理等业务流程中蕴含的逻辑关系）、<strong>部件模型</strong>（如飞机、汽车、工程机械等涉及的零部件三维模型）、<strong>工艺模型</strong>（如集成电路、钢铁、石化等生产过程中涉及的多种工艺、配方、参数模型）、<strong>故障模型</strong>（如设备故障关联、故障诊断模型等）、<strong>仿真模型</strong>（如风洞、温度场模型等）。机理模型本质上是各种经验知识和方法的固化，它更多是从业务逻辑原理出发，强调的是因果关系。随着大数据技术发展，一些<strong>数据分析模型</strong>也被广泛使用，包括<strong>基本的数据分析模型</strong>（如对数据做回归、聚类、分类、降维等基本处理的算法模型）、<strong>机器学习模型</strong>（如利用神经网络等模型对数据进行进一步辨识、预测等）及<strong>智能控制结构模型</strong>，大数据分析模型更多的是从数据本身出发，不过分考虑机理原理，更加强调相关关系。<br>（2）数字化模型从哪来？这些数字化模型一部分来源于物理设备，包括飞机、汽车、高铁制造过程的零件模型，设备故障诊断、性能优化和远程运维等背后的原理、知识、经验及方法；一部分来源于业务流程逻辑，包括ERP、供应链管理、客户关系管理、生产效能优化等这些业务系统中蕴含着的流程逻辑框架；此外还来源于研发工具，包括CAD、CAE、MBD等设计、仿真工具中的三维数字化模型、仿真环境模型等；以及生产工艺中的工艺配方、工艺流程、工艺参数等模型。<br>（3）数字化模型怎么开发？用什么工具开发？所有这些技术、知识、经验、方法、工艺都将通过不同的编程语言、编程方式固化形成一个个数字化模型。这些模型一部分是由具备一定开发能力的编程人员，通过<strong>代码化、参数化的编程方式</strong>直接将数字化模型以源代码的形式表示出来，但对模型背后蕴含的知识、经验了解相对较少；另一部分是由具有深厚工业知识沉淀但不具备直接编程能力的行业专家，将长期积累的知识、经验、方法通过<strong>“拖拉拽”等形象、低门槛的图形化编程方式</strong>（低代码），简易、便捷、高效地固化成一个个数字化模型。<br>（4）数字化模型什么样？采用什么技术架构？当把这些技术、知识、经验、方法等固化成一个个数字化模型沉淀在工业PaaS平台上时，主要以两种方式存在：一种是<strong>单体式架构</strong>，即把一个复杂大型的软件系统直接迁移至平台上；另一种是<strong>微服务架构</strong>，传统的软件架构不断“解耦”成一个个功能单元，并以微服务架构形式呈现在工业PaaS平台上，构成一个微服务池，然后对这些微服务调用，重构成面向角色的App。目前两种架构并存于平台之上，但随着时间的推移，<strong>单体式架构会不断地向微服务架构迁移</strong>。当工业PaaS平台上拥有大量蕴含着工业技术、知识、经验和方法的微服务架构模型时，应用层的工业App可以快速、灵活地<strong>调用多种碎片化的微服务组件</strong>，实现工业App快速开发部署和应用。<br><img src="https://user-images.githubusercontent.com/6218739/78878105-6f4fe480-7a84-11ea-8a5e-4358a2bdca7c.png" alt="7-14"><br>（5）数字化模型怎么用？一旦海量数据都汇聚到工业PaaS平台上，工业技术、知识、经验和方法以数字化模型的形式沉淀在PaaS平台上，当海量数据加载到数字化模型中，进行反复迭代、学习、分析、计算之后，可以解决物理世界四个基本问题：首先是描述（Descriptive）物理世界发生了什么（What happened）；其次是诊断（Diagnostic）为什么会发生（Why ithappened）；再次是预测（Predictive）下一步会发生什么（What will happen）；最后是决策（Decision）该怎么办（How to do），并驱动物理世界执行（Action），优化资源配置效率。概括起来讲，就是状态感知、实时分析、科学决策、精准执行。</p>
<h3 id="工业互联网与消费互联网的区别"><a href="#工业互联网与消费互联网的区别" class="headerlink" title="工业互联网与消费互联网的区别"></a>工业互联网与消费互联网的区别</h3><p><img src="https://user-images.githubusercontent.com/6218739/78878434-fdc46600-7a84-11ea-9866-7f45ad289b94.png" alt="7-15"></p>
<h3 id="制造业数字化架构体系的演进：从传统IT架构到工业互联网架构"><a href="#制造业数字化架构体系的演进：从传统IT架构到工业互联网架构" class="headerlink" title="制造业数字化架构体系的演进：从传统IT架构到工业互联网架构"></a>制造业数字化架构体系的演进：从传统IT架构到工业互联网架构</h3><p><img src="https://user-images.githubusercontent.com/6218739/78879002-e20d8f80-7a85-11ea-8ab1-4050141ae4f3.png" alt="表7-3"></p>
<h3 id="微服务：工业互联网脚骨技术变革的关键"><a href="#微服务：工业互联网脚骨技术变革的关键" class="headerlink" title="微服务：工业互联网脚骨技术变革的关键"></a>微服务：工业互联网脚骨技术变革的关键</h3><p>相比于传统软件开发架构面临的软件代码体积大、更新慢、维护难等问题，微服务具有<strong>轻量化</strong>、<strong>松耦合</strong>、<strong>快部署</strong>、<strong>高灵活度</strong>等特性，其适用于互联网需求变化快、用户群体面广等特点。在工业领域，现有工业软件架构体系越来越难以满足制造业生产体系的复杂性和不确定性需求，微服务架构为各类工业知识、经验、方法、技术等在工业互联网平台上的沉淀创造了条件，实现了工业知识的复用、重构、创造和传播，极大地提高了工业App的开发、测试、部署效率。<br>（1）微服务的本质<br>微服务（Microservice）是一种将复杂应用拆分成多个单一功能组件，通过模块化组合方式实现“松耦合”应用开发的软件架构，也称微服务架构（Microservice Architecture）。每个功能组件都是一个独立的、可部署的业务单元，称之为微服务组件。每个微服务组件可以根据业务逻辑，<strong>选择最适合该微服务组件的语言、框架、工具和存储技术进行开发部署</strong>。因此，微服务架构是一种独立开发、独立测试、独立部署、独立运行、高度自治的架构模式，同时也是一种更灵活、更开放、更松散的演进架构。其本质是一种将整体功能分解到各个离散服务中，实现对原有解决方案解耦，进而提供更加灵活服务的设计思想。<br><img src="https://user-images.githubusercontent.com/6218739/78879741-e4241e00-7a86-11ea-9d73-426026e2683c.png" alt="7-16"></p>
<p>（2）微服务的特征<br><img src="https://user-images.githubusercontent.com/6218739/78879847-0f0e7200-7a87-11ea-8876-6407c181cc62.png" alt="7-17"><br><img src="https://user-images.githubusercontent.com/6218739/78880024-509f1d00-7a87-11ea-90e7-5b3d037a0b24.png" alt="7-18"><br><img src="https://user-images.githubusercontent.com/6218739/78880274-bc818580-7a87-11ea-9063-55f69293d482.png" alt="7-19"><br><img src="https://user-images.githubusercontent.com/6218739/78880323-d02cec00-7a87-11ea-9f42-cb0e4b0ca083.png" alt="7-20"></p>
<p>在工业互联网领域，微服务作为汇聚工业大数据与提供工业智能服务的关键核心，驱动着工业大数据开始“数据产生—数据汇聚—数据处理—数据加工—数据调用—数据展示”的数据之旅，其实现路径是将各种<strong>以数字化模型构成的微服务组件容器化</strong>，通过负载均衡、弹性扩展快速实现微服务的部署、组合及调度，以支撑工业App的应用开发与使用<br><img src="https://user-images.githubusercontent.com/6218739/78880547-2732c100-7a88-11ea-9193-1848856ccd5c.png" alt="7-21"></p>
<p>（3）微服务设计原则<br>单一职责原则、服务自洽原则、轻量级通信原则、接口明确原则。</p>
<h2 id="怎么看"><a href="#怎么看" class="headerlink" title="怎么看"></a>怎么看</h2><h3 id="工业云视角"><a href="#工业云视角" class="headerlink" title="工业云视角"></a>工业云视角</h3><p><img src="https://user-images.githubusercontent.com/6218739/78881297-2c444000-7a89-11ea-8663-11650d909b24.png" alt="7-22"></p>
<p>工业互联网平台就是在传统工业云平台软件工具共享、业务系统集成的基础上，叠加了制造能力开放、知识经验复用和开发者集聚的功能，从而大幅提升工业知识生产、传播和利用的效率，它是一个不断演进的过程。</p>
<h3 id="解决方案视角"><a href="#解决方案视角" class="headerlink" title="解决方案视角"></a>解决方案视角</h3><p><img src="https://user-images.githubusercontent.com/6218739/78881497-7d543400-7a89-11ea-8414-4dad267d847e.png" alt="7-23"><br>工业互联网平台本质上是一套基于云平台的数字化、网络化、智能化解决方案。<br><img src="https://user-images.githubusercontent.com/6218739/78881764-d6bc6300-7a89-11ea-9ea1-166801741f4d.png" alt="7-24"></p>
<h3 id="操作系统视角"><a href="#操作系统视角" class="headerlink" title="操作系统视角"></a>操作系统视角</h3><p>工业互联网平台实质上是一个可拓展的工业操作系统，向下，可以实现对各种软硬件资源接入、控制和管理；自身，承载着蕴含大量工业知识的数字化模型与微服务；向上，提供开发接口、存储计算及工具资源等支持，并以工业App的形式提供各种各样的服务。<br><img src="https://user-images.githubusercontent.com/6218739/78882050-3450af80-7a8a-11ea-87d2-c10c5d16b83e.png" alt="7-25"></p>
<p>操作系统通过分层思想将<strong>软硬件的分离解耦</strong>，以打破过去的一体化硬件设施，进而实现“硬件资源的通用化”和“服务任务的可编程”。“硬件通用化”和“服务可编程”技术演进主要解决的问题是：快速应对名种不确定性。即<strong>让“变化快”的软件摆脱束缚，使得变化“更快”；让“利用率高”的硬件逐渐沉淀趋于统一，使得利用率“更高”</strong>。硬件能够提高资产通用性，其遵循规模经济，可大规模生产标准化产品，降低生产成本。软件能够丰富产品个性化，其遵循范围经济，使企业能从提供同质产品向提供多样化产品转变，以满足市场个性化需求。<br><img src="https://user-images.githubusercontent.com/6218739/78882322-7d086880-7a8a-11ea-9e6c-2ceaff6faf39.png" alt="7-26"></p>
<p>在工业里很多技术、知识、经验、方法创新需要从零开始，知识复用水平较低。而构建一个工业互联网平台，<strong>能够将大量工业技术原理、行业知识、基础工艺、模型工具、业务流程及老专家几十年的经验进行规则化、软件化、模块化，以数字化模型的形式沉淀在这个平台上</strong>。沉淀之后能够减少大部分重复性工作，可以直接调用、复用、传播，重构工业创新体系，进而大幅度降低创新成本和风险，提高研发、生产和服务效率。从这个角度讲，工业互联网平台就是通过提高工业知识复用水平构筑工业知识创造、传播和应用新体系，即重构工业知识新体系。</p>
<h3 id="产业生态视角"><a href="#产业生态视角" class="headerlink" title="产业生态视角"></a>产业生态视角</h3><p><img src="https://user-images.githubusercontent.com/6218739/78882579-d7a1c480-7a8a-11ea-8a9f-a3affbabcf2d.png" alt="7-27"></p>
<h3 id="经济学视角"><a href="#经济学视角" class="headerlink" title="经济学视角"></a>经济学视角</h3><p>工业互联网的价值在于其加快了从基于产品的分工向<strong>基于知识的分工</strong>演进，构建了新的产业分工体系，推动了经济增长。郭朝晖也曾多次强调，工业互联网的应用将会带来新的工作场景：有经验的人离开了生产现场，从事更富有创造性的“<strong>知识生产</strong>”。<br>（1）信息化推动经济增长的机理<br>提高生产率：<br><img src="https://user-images.githubusercontent.com/6218739/78883456-1dab5800-7a8c-11ea-8864-0486c15fe540.png" alt="7-28"><br>提高交易效率：<br><img src="https://user-images.githubusercontent.com/6218739/78883701-6cf18880-7a8c-11ea-91da-d52c08f957bd.png" alt="7-29"></p>
<p>（2）知识创造的专业化是分工深化的新阶段<br>伴随着生产力水平的不断提升，产业分工不断深化，其大致经历了五个阶段：一是<strong>部门专业化</strong>，即农业、手工业和商业之间的分工；二是<strong>产品专业化</strong>，即以完成的最终产品为对象的分工，如汽车、机械、电器产品的生产；三是<strong>零部件专业化</strong>，即一个企业仅仅生产某个最终产品的一部分；四是<strong>工序专业化</strong>，即专门进行产品或零部件生产的一个工艺过程，如铸造、电镀等；五是<strong>生产服务专业化</strong>，即在直接生产过程之外，又提供基于产品的为生产服务，如物流配送、金融服务等。<br>以集成电路产业为例，集成电路产业分工水平明显高于其他行业，其形成了<strong>基于知识的产业分工新体系</strong>。<br><img src="https://user-images.githubusercontent.com/6218739/78884234-2cded580-7a8d-11ea-8f98-458a8543a300.png" alt="7-30"></p>
<p>1991年，英国ARM公司成立，同时逐渐涌现出一批专注于集成电路知识产权包（IntellectualProperty, IP）设计、研发公司，集成电路产业开始兴起架构授权的Chipless新商业模式，这标志着基于知识创造的专业化分工独立出现在集成电路产业链中，<strong>工业知识脱离电路产品的附庸，开始作为独立的产品进行传播、使用和交易</strong>，随后在集成电路各个环节涌现出大量以各类IP包形式存在的设计、仿真、试产、制造等环节的工业知识，这些知识大幅提高了设计效率、产品性能、制造可行性及良品率，基于知识交易的新业态逐渐显现。<br><img src="https://user-images.githubusercontent.com/6218739/78884529-a70f5a00-7a8d-11ea-9877-73f54c102467.png" alt="表7-4"><br><img src="https://user-images.githubusercontent.com/6218739/78884691-f2c20380-7a8d-11ea-8bab-1ba23a0bca70.png" alt="7-31"></p>
<p>（3）工业互联网平台加快构建基于知识的产业分工新体系<br>一方面，工业互联网平台为工业知识的App化、微服务化创造了条件，实现了工业知识的产品化封装、平台化汇聚、在线化开放；另一方面，工业互联网平台构建了一个工业技术和知识的交易体系，它为工业App、微服务组件、模型算法等交易对象的呈现、交易、传播和复用提供了统一的场所，促进了工业知识、技术的供给方（大型企业、科研院校、开发者）与使用方（大中小企业）等交易主体在线显现、需求清晰、交易激活。<br><img src="https://user-images.githubusercontent.com/6218739/78885032-7845b380-7a8e-11ea-8579-6993721fa875.png" alt="表7-5"><br>消费互联网革命并非简单地将线下产品迁至线上。同样，工业互联网革命也并非简单地将依附在书籍、标准、专利上的工业知识迁至平台，而是革命性地改变工业知识的生产、交易方式，<strong>将传统的由供给方定制化软件开发（作坊式）的方式及一对一的交易模式，转变成由需求方个性化定制工业App（流水式）及平台化多对多的交易方式</strong>。这一新型交易体系将会带来更多新的商业价值。<br><img src="https://user-images.githubusercontent.com/6218739/78885128-a1664400-7a8e-11ea-80ee-38e2097ae752.png" alt="7-32"><br>工业互联网通过采用类似<strong>“乐高积木”的组建模式</strong>，将大量工业知识、经验、方法、模型以微服务组件化的方式沉在工业互联网平台上。通过“平台+微服务组件+App+…”的方式，将各种业务功能组件化、模块化、微服务化，在一个基础通用平台上，对细化的业务功能进行统一编排调用，既实现了软件产品的快速开发，又充分满足了不同企业不同场景的定制化需求，实现了知识产品和功能适用化的高度统一，找到了一条通往软件开发高效、定制、产品化的发展路径。<br><img src="https://user-images.githubusercontent.com/6218739/78885360-0752cb80-7a8f-11ea-94c9-2995fe6c8a3e.png" alt="7-33"></p>
<h1 id="工业互联网平台的演进路径"><a href="#工业互联网平台的演进路径" class="headerlink" title="工业互联网平台的演进路径"></a>工业互联网平台的演进路径</h1><p>工业互联网平台与工业云有本质的区别，又有许多联系，工业互联网平台是传统工业云功能的叠加与迭代。从过去几年的工作实践及技术和产业发展趋势来看，工业云平台向工业互联网平台的演进经历了成本驱动导向、集成应用导向、能力交易导向、创新引领导向、生态构建导向五个阶段，这几个阶段可以并行，也可以跳跃。</p>
<h2 id="成本驱动导向阶段"><a href="#成本驱动导向阶段" class="headerlink" title="成本驱动导向阶段"></a>成本驱动导向阶段</h2><p>工业云发展的第一阶段是成本驱动导向阶段。这一阶段主要是<strong>研发设计类工具上云</strong>。云计算具有资源池化、弹性供给、按需付费等典型特征，它能大幅降低企业购买研发工具的成本，提高企业研发效率，降低成本是工业云平台起步发展阶段考虑的重要因素。<br>在<strong>硬件</strong>方面，云平台通过IT硬件资源租用取代直接购买或自建，可以大幅降低硬件成本。在<strong>软件</strong>方面，从购买软件授权到根据时间、人、次数来订阅云服务，也可以大幅降低成本。在<strong>部署成本与运营成本</strong>方面，工业云平台可以在任意时间、任意地点快速部署，大大缩短了信息系统建设周期，同时，工业云平台由平台运营商统一管理，可大幅减少企业运维成本。</p>
<h2 id="集成应用导向阶段"><a href="#集成应用导向阶段" class="headerlink" title="集成应用导向阶段"></a>集成应用导向阶段</h2><p>工业云发展的第二阶段是集成应用导向阶段，这一阶段主要是在研发设计类工具上云的基础上，推动<strong>核心业务系统上云，以及实现内部系统集成</strong>。<br>企业对两化融合的投入和企业从两化融合中获得的收益并不是线性关系，<strong>企业两化融合水平只有达到集成阶段之后，企业的收益才会呈现指数化增长</strong>。<br><img src="https://user-images.githubusercontent.com/6218739/78895077-c794df80-7aa0-11ea-8f38-2ce55ef4b0f2.png" alt="8-1"><br>实现集成有两种途径，其一是渐进的路径，即不改变现有的技术架构，不断地通过各类数据总线和接口实现不同业务系统之间的互联互通；其二则是工业云这一激进路径，即企业将业务系统迁移到云端，从而解决系统内部的互联互通问题。因此，集成应用导向阶段工业云平台的典型特征是通过核心业务系统上云，打通信息孤岛，促进制造资源、数据等集成共享，提升企业效益。</p>
<h2 id="能力交易导向阶段"><a href="#能力交易导向阶段" class="headerlink" title="能力交易导向阶段"></a>能力交易导向阶段</h2><p>工业云发展的第三阶段是能力交易导向阶段。这一阶段在企业研发设计类工具、核心业务系统上云之后，<strong>底层的设备和产品开始上云，工业云平台开始演进为工业互联网平台</strong>。<br>硬件设备上云+核心业务系统上云+研发工具上云，推动互联网在经历信息交流（搜狐、新浪）与产品交易（京东、阿里）之后，正在进入能力交易的新阶段，未来在互联网上不再仅仅是手机、衣服等产品的交易，还将出现研发设计能力、测试试验能力、生产制造能力、物流能力等生产能力的交易。</p>
<h2 id="创新引领导向阶段"><a href="#创新引领导向阶段" class="headerlink" title="创新引领导向阶段"></a>创新引领导向阶段</h2><p>第四阶段是创新引领导向阶段。这一阶段在企业研发设计类工具、核心业务系统、底层的设备和产品开始上云之后，制造业架构体系发生了革命性变革。创新引领主要体现在三个方面，一是“云计算+边缘计算”成为计算能力新组合；二是微服务架构成为知识经验封装的新模式；三是工业App成为新型软件形式。</p>
<h2 id="生态构建导向阶段"><a href="#生态构建导向阶段" class="headerlink" title="生态构建导向阶段"></a>生态构建导向阶段</h2><p>第五阶段是生态建设导向阶段。在这一阶段，随着海量第三方开发者与通用工业App的出现，工业互联网平台将进入一个以生态构建为导向的新阶段。当前全球领军企业都在围绕“智能机器+云平台+工业App”的功能架构，培育海量第三方开发者开发工业App，构建基于平台的制造业生态，不断巩固和强化制造业竞争优势。<br><img src="https://user-images.githubusercontent.com/6218739/78896937-f52f5800-7aa3-11ea-85af-2534fe594446.png" alt="8-2"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2020/03/28/ImagePy_19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Be interesting!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="亓欣波">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/28/ImagePy_19/" class="post-title-link" itemprop="url">ImagePy解析：19 -- Mark模式</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-28 00:00:00" itemprop="dateCreated datePublished" datetime="2020-03-28T00:00:00+08:00">2020-03-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-25 15:20:30" itemprop="dateModified" datetime="2021-03-25T15:20:30+08:00">2021-03-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/computational-material-science/" itemprop="url" rel="index"><span itemprop="name">computational material science</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/03/28/ImagePy_19/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/03/28/ImagePy_19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>%%%%%%%%%%%%%%<br>2021-1-31更新：基于最新的sciwx修改了以前的失效代码。<br>%%%%%%%%%%%%%%</p>
<p>ImagePy/sciwx有个Mark模式，即在图像上面可以再绘制其他图形，比如矩形、文本、ROI标识等，本质即是利用GDI绘图。<br>本文是对该Mark模式的解析。</p>
<h1 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h1><p>先给出一个小的demo，主要就是为了看Mark模式的输入输出，方便理解它的运行本质。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sciwx.canvas.mark <span class="keyword">import</span> drawmark</span><br><span class="line"><span class="keyword">from</span> sciapp.<span class="built_in">object</span> <span class="keyword">import</span> mark2shp</span><br><span class="line"></span><br><span class="line">mark =  &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;layer&#x27;</span>, <span class="string">&#x27;body&#x27;</span>: [&#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;rectangle&#x27;</span>, <span class="string">&#x27;body&#x27;</span>: (<span class="number">50</span>, <span class="number">50</span>, <span class="number">200</span>, <span class="number">200</span>), <span class="string">&#x27;color&#x27;</span>: (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>)&#125;, &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;circle&#x27;</span>, <span class="string">&#x27;body&#x27;</span>: (<span class="number">150</span>, <span class="number">150</span>, <span class="number">5</span>), <span class="string">&#x27;color&#x27;</span>: (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>)&#125;, &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;body&#x27;</span>: (<span class="number">75</span>, <span class="number">75</span>, <span class="string">&#x27;S:30 W:48&#x27;</span>), <span class="string">&#x27;pt&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;color&#x27;</span>: (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>)&#125;]&#125;</span><br><span class="line"></span><br><span class="line">shape = mark2shp(mark)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Example</span>(<span class="params">wx.Frame</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, parent, obj</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(parent)</span><br><span class="line">        self.obj = obj</span><br><span class="line"></span><br><span class="line">        self.Bind(wx.EVT_PAINT, self.DrawMarks)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">DrawMarks</span>(<span class="params">self, e</span>):</span></span><br><span class="line">        dc = wx.PaintDC(self)</span><br><span class="line">        drawmark(dc, f, self.obj, k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">app = wx.App()</span><br><span class="line">ex = Example(<span class="literal">None</span>, shape)</span><br><span class="line">ex.Show()</span><br><span class="line">app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>一定要使用这种PaintEvent事件来调用画图（或者调用CallLater），否则会看不到所绘制的图形，具体原因见下方链接：<br><a target="_blank" rel="noopener" href="http://zetcode.com/wxpython/gdi/">wxPython graphics</a></p>
<p>效果如图（注意这张是旧图，最新代码生成的图略有不同）：<br><img src="https://user-images.githubusercontent.com/6218739/77822795-eace9f00-7130-11ea-9033-a28077379552.png" alt="mark"></p>
<p>可以看出，整个程序的逻辑很简单，<br>（1）创建一个mark配置（具体写法后面介绍），传入mark2shp函数对其转换一下<br>（2）创建一个坐标映射函数f，这是为了坐标变换（这里不涉及坐标变换，所以直接原样返回）<br>（3）通过wx.PaintDC创建一个设备上下文dc<br>（4）将dc、f和缩放因子k传入drawmark方法，从而进行绘制</p>
<p>下面分步具体解析。</p>
<h1 id="mark配置"><a href="#mark配置" class="headerlink" title="mark配置"></a>mark配置</h1><p>mark写法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mark =  &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;layer&#x27;</span>, <span class="string">&#x27;body&#x27;</span>: [&#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;rectangle&#x27;</span>, <span class="string">&#x27;body&#x27;</span>: (<span class="number">50</span>, <span class="number">50</span>, <span class="number">200</span>, <span class="number">200</span>), <span class="string">&#x27;color&#x27;</span>: (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>)&#125;, &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;circle&#x27;</span>, <span class="string">&#x27;body&#x27;</span>: (<span class="number">150</span>, <span class="number">150</span>, <span class="number">5</span>), <span class="string">&#x27;color&#x27;</span>: (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>)&#125;, &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;body&#x27;</span>: (<span class="number">75</span>, <span class="number">75</span>, <span class="string">&#x27;S:30 W:48&#x27;</span>), <span class="string">&#x27;pt&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;color&#x27;</span>: (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>)&#125;]&#125;</span><br></pre></td></tr></table></figure>
<p>（之所以这样书写，是因为这样写是人类友好的，后面会将这一阅读良好的字典转换成sciapp内部的特有的Shape数据结构。）</p>
<p>mark的写法是这样的：<br>（1）mark是个字典，里面的重要的键值比如type、body等；<br>（2）第一层的type是layer，表明这是多个图形的结合，那么这一层的body就是所包含的图形的list<br>（3）具体到某个具体所绘制的图形，以上面的配置为例：<br>（3.1）矩形，其键值对有：type是rectangle，body是四个整型数值，即x、y、w和h，其中x和y是矩形的左上角（这里跟之前旧代码不同，原先的是矩形中心），w和h是高和宽，color是绘制的颜色。<br>（3.2）圆形：其键值对有：type是circle，body是三个整型数值，即x、y和r，即圆形中心和半径。<br>（3.3）文字：其键值对有：type是text，body是两个整型数值和一个文本，即x、y和文本内容，x、y是绘制文本的左上角，pt是指定是否在该左上角绘制一个小圆点。</p>
<h1 id="坐标映射函数和缩放因子"><a href="#坐标映射函数和缩放因子" class="headerlink" title="坐标映射函数和缩放因子"></a>坐标映射函数和缩放因子</h1><p>这个函数f存在的意义是在imagepy/sciwx的canvas中，其需要将图像坐标系中的坐标转换到面板坐标系中，所以在canvas的源码中可以看到，该函数f就是传入的转换到面板坐标的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drawmark(dc, self.to_panel_coor, self.marks[i], k=self.scale)</span><br></pre></td></tr></table></figure>
<p>同理，缩放因子也是为了适应canvas画布的缩放而需要传入的。<br>因为这里没有使用到canvas，所以f中没有做任何变换，而k也是为1。</p>
<h1 id="drawmark函数"><a href="#drawmark函数" class="headerlink" title="drawmark函数"></a>drawmark函数</h1><p>这里看一下该函数的源代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drawmark</span>(<span class="params">dc, f, body, **key</span>):</span></span><br><span class="line">	default_style = body.default</span><br><span class="line">	pen, brush, font = dc.GetPen(), dc.GetBrush(), dc.GetFont()</span><br><span class="line">	pen.SetColour(default_style[<span class="string">&#x27;color&#x27;</span>])</span><br><span class="line">	brush.SetColour(default_style[<span class="string">&#x27;fcolor&#x27;</span>])</span><br><span class="line">	brush.SetStyle((<span class="number">106</span>,<span class="number">100</span>)[default_style[<span class="string">&#x27;fill&#x27;</span>]])</span><br><span class="line">	pen.SetWidth(default_style[<span class="string">&#x27;lw&#x27;</span>])</span><br><span class="line">	dc.SetTextForeground(default_style[<span class="string">&#x27;tcolor&#x27;</span>])</span><br><span class="line">	font.SetPointSize(default_style[<span class="string">&#x27;size&#x27;</span>])</span><br><span class="line">	dc.SetPen(pen); dc.SetBrush(brush); dc.SetFont(font);</span><br><span class="line">	draw(body, dc, f, **key)</span><br></pre></td></tr></table></figure>
<p>该方法就是先获得设备上下文的画笔pen、画刷brush和字体font，然后通过SetColour设置颜色、SetStyle设置风格、SetWidth设置笔宽、SetPointSize设置字体尺寸等对上述工具进行属性设置。然后再调用全局的draw()函数。<br>通过该函数也可以看出来，它接收的body需要具有一定的格式，比如有default属性。因此，最开始的mark变量没法直接传入drawmark中，需要使用mark2shp方法来转换一下。</p>
<h1 id="mark2shp函数"><a href="#mark2shp函数" class="headerlink" title="mark2shp函数"></a>mark2shp函数</h1><p>仍然看一下该函数的源代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mark2shp</span>(<span class="params">mark</span>):</span></span><br><span class="line">    style = mark.copy()</span><br><span class="line">    style.pop(<span class="string">&#x27;body&#x27;</span>)</span><br><span class="line">    keys = &#123;<span class="string">&#x27;point&#x27;</span>:Point, <span class="string">&#x27;points&#x27;</span>:Points, <span class="string">&#x27;line&#x27;</span>:Line, <span class="string">&#x27;lines&#x27;</span>:Lines,</span><br><span class="line">            <span class="string">&#x27;polygon&#x27;</span>:Polygon, <span class="string">&#x27;polygons&#x27;</span>:Polygons, <span class="string">&#x27;circle&#x27;</span>:Circle,</span><br><span class="line">            <span class="string">&#x27;circles&#x27;</span>:Circles, <span class="string">&#x27;rectangle&#x27;</span>:Rectangle, <span class="string">&#x27;rectangles&#x27;</span>:Rectangles,</span><br><span class="line">            <span class="string">&#x27;ellipse&#x27;</span>:Ellipse, <span class="string">&#x27;ellipses&#x27;</span>:Ellipses, <span class="string">&#x27;text&#x27;</span>:Text, <span class="string">&#x27;texts&#x27;</span>:Texts&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mark[<span class="string">&#x27;type&#x27;</span>] <span class="keyword">in</span> keys: <span class="keyword">return</span> keys[mark[<span class="string">&#x27;type&#x27;</span>]](mark[<span class="string">&#x27;body&#x27;</span>], **style)</span><br><span class="line">    <span class="keyword">if</span> mark[<span class="string">&#x27;type&#x27;</span>]==<span class="string">&#x27;layer&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> Layer([mark2shp(i) <span class="keyword">for</span> i <span class="keyword">in</span> mark[<span class="string">&#x27;body&#x27;</span>]], **style)</span><br><span class="line">    <span class="keyword">if</span> mark[<span class="string">&#x27;type&#x27;</span>]==<span class="string">&#x27;layers&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> Layers(<span class="built_in">dict</span>(<span class="built_in">zip</span>(mark[<span class="string">&#x27;body&#x27;</span>].keys(),</span><br><span class="line">            [mark2shp(i) <span class="keyword">for</span> i <span class="keyword">in</span> mark[<span class="string">&#x27;body&#x27;</span>].values()])), **style)</span><br></pre></td></tr></table></figure>
<p>这里就引出了sciapp中的自定义的矢量类Shape这一数据结构。<br>Shape类是这类对象的基类，上述代码中的Point、Line、Rectangle类等都是该类的派生类。<br>Shape类会在下一篇文章中详细解释，见<a target="_blank" rel="noopener" href="https://qixinbo.info/2020/06/14/imagepy_20/">这里</a>。<br>总之，该函数就是将mark字典转换成了sciapp内置的Shape数据结构。</p>
<h1 id="全局draw-函数"><a href="#全局draw-函数" class="headerlink" title="全局draw()函数"></a>全局draw()函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">draw_dic = &#123;<span class="string">&#x27;points&#x27;</span>:plot, <span class="string">&#x27;point&#x27;</span>:plot, <span class="string">&#x27;line&#x27;</span>:plot,</span><br><span class="line"><span class="string">&#x27;polygon&#x27;</span>:plot, <span class="string">&#x27;lines&#x27;</span>:plot, <span class="string">&#x27;polygons&#x27;</span>:plot,</span><br><span class="line"><span class="string">&#x27;circle&#x27;</span>:draw_circle, <span class="string">&#x27;circles&#x27;</span>:draw_circle,</span><br><span class="line"><span class="string">&#x27;ellipse&#x27;</span>:draw_ellipse, <span class="string">&#x27;ellipses&#x27;</span>:draw_ellipse,</span><br><span class="line"><span class="string">&#x27;rectangle&#x27;</span>:draw_rectangle, <span class="string">&#x27;rectangles&#x27;</span>:draw_rectangle,</span><br><span class="line"><span class="string">&#x27;text&#x27;</span>:draw_text, <span class="string">&#x27;texts&#x27;</span>:draw_text&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span>(<span class="params">obj, dc, f, **key</span>):</span> </span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">len</span>(obj.body)==<span class="number">0</span>: <span class="keyword">return</span></span><br><span class="line">	draw_dic[obj.dtype](obj, dc, f, **key)</span><br><span class="line"></span><br><span class="line">draw_dic[<span class="string">&#x27;layer&#x27;</span>] = draw_layer</span><br></pre></td></tr></table></figure>
<p>可以看出，draw()函数中先从传入的obj中提取它的dtype这个key，从而找到obj中对应的key-value，然后这个value作为draw_dic字典中的键，找到所绘制的类型，从而定位到具体的绘制函数。</p>
<p>对于layer这个type，可以把layer认为是多个图形的集合，会调用draw_layer这个函数，该函数里也会遍历该集合里的所有图形来再调用draw绘制：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_layer</span>(<span class="params">pts, dc, f, **key</span>):</span></span><br><span class="line">         …</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> pts[<span class="string">&#x27;body&#x27;</span>]:draw(i, dc, f, **key)</span><br></pre></td></tr></table></figure>
<p>以绘制矩形为例，会调用draw_rectangle函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_rectangle</span>(<span class="params">pts, dc, f, **key</span>):</span></span><br><span class="line">	pen, brush = dc.GetPen(), dc.GetBrush()</span><br><span class="line">	width, color = pen.GetWidth(), pen.GetColour()</span><br><span class="line">	fcolor, style = brush.GetColour(), brush.GetStyle()</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> pts.color <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">		pen.SetColour(pts.color)</span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> pts.fcolor <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">		brush.SetColour(pts.fcolor)</span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> pts.lw <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">		pen.SetWidth(pts.lw)</span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> pts.fill <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">		brush.SetStyle((<span class="number">106</span>,<span class="number">100</span>)[pts.fill])</span><br><span class="line"></span><br><span class="line">	dc.SetPen(pen)</span><br><span class="line">	dc.SetBrush(brush)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> pts.dtype == <span class="string">&#x27;rectangle&#x27;</span>:</span><br><span class="line">		x, y, w, h = pts.body</span><br><span class="line">		w, h = f(x+w, y+h)</span><br><span class="line">		x, y = f(x, y)</span><br><span class="line">		dc.DrawRectangle(x.<span class="built_in">round</span>(), (y).<span class="built_in">round</span>(), </span><br><span class="line">			(w-x).<span class="built_in">round</span>(), (h-y).<span class="built_in">round</span>())</span><br><span class="line">	<span class="keyword">if</span> pts.dtype == <span class="string">&#x27;rectangles&#x27;</span>:</span><br><span class="line">		x, y, w, h = pts.body.T</span><br><span class="line">		w, h = f(x+w, y+h)</span><br><span class="line">		x, y = f(x, y)</span><br><span class="line">		lst = np.vstack((x,y,w-x,h-y)).T</span><br><span class="line">		dc.DrawRectangleList(lst.<span class="built_in">round</span>())</span><br><span class="line"></span><br><span class="line">	pen.SetWidth(width)</span><br><span class="line">	pen.SetColour(color)</span><br><span class="line">	brush.SetColour(fcolor)</span><br><span class="line">	brush.SetStyle(style)</span><br><span class="line">	dc.SetPen(pen)</span><br><span class="line">	dc.SetBrush(brush)</span><br></pre></td></tr></table></figure>

<p>可以看出，在绘制矩形时最基本的步骤就是：<br>（1）将之前的设备上下文传入；<br>（2）提取特定的Rectangle这一Shape数据结构中的body信息，即绘制矩形的坐标点；<br>（3）调用最底层的设备上下文的DrawRectangle方法将其绘制出来。</p>
<h1 id="保存mark"><a href="#保存mark" class="headerlink" title="保存mark"></a>保存mark</h1><p>上面的mark都是在最原始的wxPython的frame上进行绘制，是为了展示mark的本质，实际应用时这些mark都是在Canvas上进行绘制。<br>（以下内容已合并在ImagePy的master分支中）<br>如果想在ImagePy中保存mark，可以参考我提交的一个pull request，在<a target="_blank" rel="noopener" href="https://github.com/Image-Py/imagepy/pull/96/commits/770342625d320659d2c5c406b1a78a809be086b0">这里</a>。<br>原理就是将当前的dc中的内容都save出来。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2020/03/24/ImagePy_18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Be interesting!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="亓欣波">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/24/ImagePy_18/" class="post-title-link" itemprop="url">ImagePy解析：18 -- 参数对话框ParaDialog详解</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-24 00:00:00" itemprop="dateCreated datePublished" datetime="2020-03-24T00:00:00+08:00">2020-03-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-25 15:20:30" itemprop="dateModified" datetime="2021-03-25T15:20:30+08:00">2021-03-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/computational-material-science/" itemprop="url" rel="index"><span itemprop="name">computational material science</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/03/24/ImagePy_18/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/03/24/ImagePy_18/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本文对sciwx的独立组件——参数对话框ParaDialog进行解析。</p>
<h1 id="demo全景"><a href="#demo全景" class="headerlink" title="demo全景"></a>demo全景</h1><p>首先还是直接给出sciwx库中可运行的demo：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sciwx.widgets <span class="keyword">import</span> ParaDialog</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    para = &#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;yxdragon&#x27;</span>, <span class="string">&#x27;age&#x27;</span>:<span class="number">10</span>, <span class="string">&#x27;h&#x27;</span>:<span class="number">1.72</span>, <span class="string">&#x27;w&#x27;</span>:<span class="number">70</span>, <span class="string">&#x27;sport&#x27;</span>:<span class="literal">True</span>, <span class="string">&#x27;sys&#x27;</span>:<span class="string">&#x27;Mac&#x27;</span>, <span class="string">&#x27;lan&#x27;</span>:[<span class="string">&#x27;C/C++&#x27;</span>, <span class="string">&#x27;Python&#x27;</span>], <span class="string">&#x27;c&#x27;</span>:(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>), <span class="string">&#x27;path&#x27;</span>: <span class="string">&#x27; &#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">    view = [(<span class="string">&#x27;lab&#x27;</span>, <span class="string">&#x27;lab&#x27;</span>, <span class="string">&#x27;This is a questionnaire&#x27;</span>),</span><br><span class="line">            (<span class="built_in">str</span>, <span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;please&#x27;</span>),</span><br><span class="line">            (<span class="built_in">int</span>, <span class="string">&#x27;age&#x27;</span>, (<span class="number">0</span>,<span class="number">150</span>), <span class="number">0</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;years old&#x27;</span>),</span><br><span class="line">            (<span class="built_in">float</span>, <span class="string">&#x27;h&#x27;</span>, (<span class="number">0.3</span>, <span class="number">2.5</span>), <span class="number">2</span>, <span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;m&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;slide&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, (<span class="number">1</span>, <span class="number">150</span>), <span class="number">0</span>, <span class="string">&#x27;weight&#x27;</span>,<span class="string">&#x27;kg&#x27;</span>),</span><br><span class="line">            (<span class="built_in">bool</span>, <span class="string">&#x27;sport&#x27;</span>, <span class="string">&#x27;do you like sport&#x27;</span>),</span><br><span class="line">            (<span class="built_in">list</span>, <span class="string">&#x27;sys&#x27;</span>, [<span class="string">&#x27;Windows&#x27;</span>,<span class="string">&#x27;Mac&#x27;</span>,<span class="string">&#x27;Linux&#x27;</span>], <span class="built_in">str</span>, <span class="string">&#x27;favourite&#x27;</span>, <span class="string">&#x27;system&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;chos&#x27;</span>, <span class="string">&#x27;lan&#x27;</span>, [<span class="string">&#x27;C/C++&#x27;</span>,<span class="string">&#x27;Java&#x27;</span>,<span class="string">&#x27;Python&#x27;</span>], <span class="string">&#x27;lanuage you like(multi)&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;color&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;which&#x27;</span>, <span class="string">&#x27;you like&#x27;</span>),</span><br><span class="line">            (<span class="string">&#x27;path&#x27;</span>, <span class="string">&#x27;path&#x27;</span>, <span class="string">&#x27;Select the image&#x27;</span>, [<span class="string">&#x27;jpg&#x27;</span>, <span class="string">&#x27;jpeg&#x27;</span>, <span class="string">&#x27;png&#x27;</span>])]</span><br><span class="line"></span><br><span class="line">    app = wx.App()</span><br><span class="line">    pd = ParaDialog(<span class="literal">None</span>, <span class="string">&#x27;Test&#x27;</span>)</span><br><span class="line">    pd.init_view(view, para, preview=<span class="literal">True</span>, modal=<span class="literal">False</span>)</span><br><span class="line">    pd.pack()</span><br><span class="line">    pd.ShowModal()</span><br><span class="line">    <span class="built_in">print</span>(para)</span><br><span class="line">    app.MainLoop()</span><br></pre></td></tr></table></figure>

<p>运行结果如图：<br><img src="https://user-images.githubusercontent.com/6218739/77433470-5efc0080-6e1a-11ea-85e1-83e3d54cad10.png" alt="paradiglog"></p>
<p>可以看出，该对话框中包含了输入框（包含文本输入、数值输入）、滑动条（集成了微调器）、单选框、复选框、下拉列表等效果，完全满足日常人机交互的需求。</p>
<p>下面再分步细看具体设置。</p>
<h1 id="参数字典para"><a href="#参数字典para" class="headerlink" title="参数字典para"></a>参数字典para</h1><p>变量para是一个参数字典，用来设置该对话框所提供的想要用户进行交互调节的参数。<br>该字典中的key都是字符串，而value则视参数类型的不同，要设置好默认值，比如数值输入框的默认值是数值，颜色输入框的默认值是(255, 0, 0)这样的RGB元组等。</p>
<h1 id="视图列表view"><a href="#视图列表view" class="headerlink" title="视图列表view"></a>视图列表view</h1><p>变量view是一个列表，用来控制所形成的对话框的图形界面。<br>可以看出，view列表中又有若干元组，这些元组就是对话框中的各个组件。还有一点非常重要的是，注意到，不同组件所需的参数量不同，因此编写view时需要将这些参数搞明白。<br>编写view时，要将元组中的参数分成三类进行考虑（下文会对原因有详细说明），第一类是该元组的第一个元素，第二类是元组的第二个元素，第三类是后面所有元素。<br>第一个元素就是ImagePy/sciwx的内部组件类型，第二个元素就是para中的各个键值key（lab类型除外），第三个元素就是该组件需要的参数。因为第二个元素是para变量所定义的，所以下面只对其他元素进行说明：<br>（1）标签：内部类型’lab’，参数为title，view中写法为：(‘lab’, ‘lab’, title)<br>（2）文本输入框：内部类型str，参数为(title, unit)，即名称（或称前缀）和单位（或称后缀）， view写法为：(str, key, title, unit)<br>（3）数值输入框：内部类型int或float，即使用int或float皆可，这里只是语义上的差别，两者实际调用的是同一个组件，使用int时后面的精度设为0，使用float时精度设为大于0的整数，即保留多少位小数；参数为(rang, accury, title, unit)，即范围、精度、名称和单位，view写法为：(int, key, (lim1, lim2), accu, title, unit)<br>（4）滑动条（集成了微调器）：内部类型’slide’（注意别忘了单引号），参数为(rang, accury, title, unit=’’)，即范围、精度、名称和单位（单位有默认参数，可以不显式指定），view写法为(‘slide’, key, rang, accury, title, unit)<br>（5）单选框：内部类型bool，参数为title，view写法为(bool, key, title)<br>（6）下拉列表：内部类型list，参数为(choices, type, title, unit)，choices为字符选项，type为期望输出类型，比如str或int，view写法为(list, key, [choices], type, title, unit)<br>（7）复选框：内部类型’chos’，参数为(choices, title)，choices为字符选项，与上面的list不同的是，其支持多选，view写法为(‘chos’, key, [choices], title)<br>（8）颜色框：内部类型’color’，参数为(title, unit)，即前缀和后缀，view写法为：(‘color’, key, title, unit)<br>（9）路径选择框：内部类型’path’，参数为(title, filter)，filter是所指定的文件扩展名，view写法为：(‘path’, key, title, filter)</p>
<h1 id="初始化类"><a href="#初始化类" class="headerlink" title="初始化类"></a>初始化类</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd = ParaDialog(<span class="literal">None</span>, <span class="string">&#x27;Test&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>这一步是初始化ParaDialog类。<br>该类派生自wx.Dialog类，从该类的初始化函数可以看出，其需要传入parent和title两个参数，所以该对话框的标题就是Test。</p>
<h1 id="初始化视图"><a href="#初始化视图" class="headerlink" title="初始化视图"></a>初始化视图</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.init_view(view, para, preview=<span class="literal">True</span>, modal=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>这一步是调用了ParaDialog类的init_view函数，传入的就是上面的view和para，preview是指定是否显示preview单选框，如果勾选了该框，那么当参数发生变化时就会在后台打印参数值（因为这里的self.handle=print），。<br>该步是关键一步，在后台做了很多事情，如果想理清ParaDialog的生成机理，需要深入研究一下。<br>先看一下该函数的全貌：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_view</span>(<span class="params">self, items, para, preview=<span class="literal">False</span>, modal = <span class="literal">True</span></span>):</span></span><br><span class="line">    self.para = para</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        self.add_ctrl_(widgets[item[<span class="number">0</span>]], item[<span class="number">1</span>], item[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> preview:self.add_ctrl_(Check, <span class="string">&#x27;preview&#x27;</span>, (<span class="string">&#x27;preview&#x27;</span>,))</span><br><span class="line">    self.reset(para)</span><br><span class="line">    self.add_confirm(modal)</span><br><span class="line">    self.pack()</span><br><span class="line">    self.Bind(wx.EVT_WINDOW_DESTROY, self.OnDestroy)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;bind close&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="参数字典传递"><a href="#参数字典传递" class="headerlink" title="参数字典传递"></a>参数字典传递</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.para = para</span><br></pre></td></tr></table></figure>
<p>这一步是将外部传过来的para赋值给内部属性self.para。</p>
<h2 id="添加组件"><a href="#添加组件" class="headerlink" title="添加组件"></a>添加组件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">    self.add_ctrl_(widgets[item[<span class="number">0</span>]], item[<span class="number">1</span>], item[<span class="number">2</span>:])</span><br></pre></td></tr></table></figure>
<p>这一步是将view视图中的组件添加进来。<br>因为view是个列表，所以这里先用循环逐个提取。然后在添加时将item拆分成了三部分，将其第0个元素、第1个元素、(第2个元素及后面所有元素)分别传入不同的位置，下面具体看为什么这么做。</p>
<h3 id="view的第0个元素"><a href="#view的第0个元素" class="headerlink" title="view的第0个元素"></a>view的第0个元素</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">widgets[item[<span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<p>这个元素是传入了widgets，而widgets也是一个字典：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">widgets = &#123; <span class="string">&#x27;ctrl&#x27;</span>:<span class="literal">None</span>, <span class="string">&#x27;slide&#x27;</span>:FloatSlider, <span class="built_in">int</span>:NumCtrl, <span class="string">&#x27;path&#x27;</span>:PathCtrl,</span><br><span class="line">            <span class="built_in">float</span>:NumCtrl, <span class="string">&#x27;lab&#x27;</span>:Label, <span class="built_in">bool</span>:Check, <span class="built_in">str</span>:TextCtrl, <span class="built_in">list</span>:Choice,</span><br><span class="line">            <span class="string">&#x27;color&#x27;</span>:ColorCtrl, <span class="string">&#x27;any&#x27;</span>:AnyType, <span class="string">&#x27;chos&#x27;</span>:Choices, <span class="string">&#x27;hist&#x27;</span>:HistPanel&#125;</span><br></pre></td></tr></table></figure>
<p>可以看出，view的第0个元素是作为widgets的key，这样就能对应提取widgets中的value。而这些value就是sciwx独有的基于wxPython各种组件二次开发的组件。以NumCtrl为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NumCtrl</span>(<span class="params">wx.Panel</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;NumCtrl: diverid from wx.core.TextCtrl &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, parent, rang, accury, title, unit</span>):</span></span><br><span class="line">        wx.Panel.__init__(self, parent)</span><br><span class="line">        sizer = wx.BoxSizer( wx.HORIZONTAL )</span><br><span class="line">        self.prefix = lab_title = wx.StaticText( self, wx.ID_ANY, title,</span><br><span class="line">                                  wx.DefaultPosition, wx.DefaultSize)</span><br><span class="line"></span><br><span class="line">        lab_title.Wrap( -<span class="number">1</span> )</span><br><span class="line">        sizer.Add( lab_title, <span class="number">0</span>, wx.ALIGN_CENTER|wx.ALL, <span class="number">5</span> )</span><br><span class="line">        self.ctrl = wx.TextCtrl(self, wx.TE_RIGHT)</span><br><span class="line">        self.ctrl.Bind(wx.EVT_KEY_UP, <span class="keyword">lambda</span> x : self.para_changed(key))</span><br><span class="line">        sizer.Add( self.ctrl, <span class="number">2</span>, wx.ALL, <span class="number">5</span> )</span><br><span class="line"></span><br><span class="line">        self.postfix = lab_unit = wx.StaticText( self, wx.ID_ANY, unit,</span><br><span class="line">                                  wx.DefaultPosition, wx.DefaultSize)</span><br><span class="line"></span><br><span class="line">        lab_unit.Wrap( -<span class="number">1</span> )</span><br><span class="line">        sizer.Add( lab_unit, <span class="number">0</span>, wx.ALIGN_CENTER|wx.ALL, <span class="number">5</span> )</span><br><span class="line">        self.SetSizer(sizer)</span><br><span class="line"></span><br><span class="line">        self.<span class="built_in">min</span>, self.<span class="built_in">max</span> = rang</span><br><span class="line">        self.accury = accury</span><br><span class="line">        self.ctrl.Bind(wx.EVT_KEY_UP, self.ontext)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Bind</span>(<span class="params">self, z, f</span>):</span>self.f = f</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ontext</span>(<span class="params">self, event</span>):</span></span><br><span class="line">        self.f(self)</span><br><span class="line">        <span class="keyword">if</span> self.GetValue()==<span class="literal">None</span>:</span><br><span class="line">            self.ctrl.SetBackgroundColour((<span class="number">255</span>,<span class="number">255</span>,<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.ctrl.SetBackgroundColour((<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>))</span><br><span class="line">        self.Refresh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SetValue</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        self.ctrl.SetValue(<span class="built_in">str</span>(<span class="built_in">round</span>(n,self.accury) <span class="keyword">if</span> self.accury&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="built_in">int</span>(n)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetValue</span>(<span class="params">self</span>):</span></span><br><span class="line">        sval = self.ctrl.GetValue()</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            num = <span class="built_in">float</span>(sval) <span class="keyword">if</span> self.accury&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="built_in">int</span>(sval)</span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> num&lt;self.<span class="built_in">min</span> <span class="keyword">or</span> num&gt;self.<span class="built_in">max</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(<span class="built_in">round</span>(num, self.accury) - num) &gt; <span class="number">1E-5</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> num</span><br></pre></td></tr></table></figure>
<p>可以看出NumCtrl就是由两个wx.StaticText静态文本框和一个wx.TextCtrl输入框组合而成，表现出来就是一个前缀说明和一个后缀说明及中间的输入框。<br>同时可以可以看出NumCtrl有数值检查这一功能，当超出所设定的范围后，就会报警。</p>
<h3 id="view的第1个元素"><a href="#view的第1个元素" class="headerlink" title="view的第1个元素"></a>view的第1个元素</h3><p>view的第1个元素就是para中的各个key值</p>
<h3 id="view的第2个及后面所有元素"><a href="#view的第2个及后面所有元素" class="headerlink" title="view的第2个及后面所有元素"></a>view的第2个及后面所有元素</h3><p>因为不同组件需要的参数量不同，因为这里将第2个及后面所有元素统一打包然后传入。</p>
<h3 id="添加组件-1"><a href="#添加组件-1" class="headerlink" title="添加组件"></a>添加组件</h3><p>以上三组元素都传入了下面方法，分别作为它的Ctrl、key和p参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_ctrl_</span>(<span class="params">self, Ctrl, key, p</span>):</span></span><br><span class="line">    ctrl = Ctrl(self, *p)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> p[<span class="number">0</span>] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        self.ctrl_dic[key] = ctrl</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(ctrl, <span class="string">&#x27;Bind&#x27;</span>):</span><br><span class="line">        ctrl.Bind(<span class="literal">None</span>, self.para_changed)</span><br><span class="line">    pre = ctrl.prefix <span class="keyword">if</span> <span class="built_in">hasattr</span>(ctrl, <span class="string">&#x27;prefix&#x27;</span>) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    post = ctrl.postfix <span class="keyword">if</span> <span class="built_in">hasattr</span>(ctrl, <span class="string">&#x27;postfix&#x27;</span>) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    self.tus.append((pre, post))</span><br><span class="line">    self.lst.Add( ctrl, <span class="number">0</span>, wx.EXPAND, <span class="number">0</span> )</span><br></pre></td></tr></table></figure>
<p>这里面挺有创意的一点是通过判断组件类里是否有某个特定的属性来进行下一步操作，比如判断是否有Bind属性，若有则Bind对话框的para_changed方法，还有prefix、postfix属性等，具体参见代码。</p>
<h1 id="显示"><a href="#显示" class="headerlink" title="显示"></a>显示</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.ShowModal()</span><br></pre></td></tr></table></figure>
<p>通过调用父类wx.Dialog的ShowModal()方法来将对话框显示出来。</p>
<h1 id="隐式输出"><a href="#隐式输出" class="headerlink" title="隐式输出"></a>隐式输出</h1><p>该对话框提供了图形界面供用户来调节para变量中的参数，当调节完成后，隐含地就对para变量中的key值所对应的value进行了调节。原理就是在para_changed方法中的GetValue()，即调用各个组件的GetValue()来获取输入值。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2020/03/08/dive-into-dl/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Be interesting!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="亓欣波">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/08/dive-into-dl/" class="post-title-link" itemprop="url">《动手学深度学习》PyTorch版学习笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-08 00:00:00" itemprop="dateCreated datePublished" datetime="2020-03-08T00:00:00+08:00">2020-03-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-25 15:20:30" itemprop="dateModified" datetime="2021-03-25T15:20:30+08:00">2021-03-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">programming</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/03/08/dive-into-dl/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/03/08/dive-into-dl/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Attention预警！：<br>时刻铭记“Garbage in, garbage out!”，因此，涉及到data时，一定注意实际查看，确保计算时Input和Output的一致性和准确性。</p>
<p>原书MXNet版在<a target="_blank" rel="noopener" href="https://github.com/d2l-ai/d2l-zh">这里</a>。<br>PyTorch版在<a target="_blank" rel="noopener" href="https://github.com/ShusenTang/Dive-into-DL-PyTorch">这里</a>。</p>
<h1 id="深度学习简介"><a href="#深度学习简介" class="headerlink" title="深度学习简介"></a>深度学习简介</h1><p>目前的机器学习和深度学习应用共同的核心思想：用数据编程。<br>通俗来说，机器学习是一门讨论各式各样的适用于不同问题的函数形式，以及如何使用数据来有效地获取函数参数具体值的学科。<br>深度学习是具有多级表示的表征学习方法（表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出）。在每一级（从原始数据开始），深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合的函数足够多时，深度学习模型就可以表达非常复杂的变换。值得一提的是，作为表征学习的一种，深度学习将自动找出每一级表示数据的合适方式。<br>（1）深度学习的一个外在特点是端到端的训练。也就是说，并不是将单独调试的部分拼凑起来组成一个系统，而是将整个系统组建好之后一起训练。比如说，计算机视觉科学家之前曾一度将特征抽取与机器学习模型的构建分开处理，像是Canny边缘探测和SIFT特征提取曾占据统治性地位达10年以上，但这也就是人类能找到的最好方法了。当深度学习进入这个领域后，这些特征提取方法就被性能更强的自动优化的逐级过滤器替代了。<br>（2）除端到端的训练以外，我们也正在经历从含参数统计模型转向完全无参数的模型。当数据非常稀缺时，我们需要通过简化对现实的假设来得到实用的模型。当数据充足时，我们就可以用能更好地拟合现实的无参数模型来替代这些含参数模型。这也使我们可以得到更精确的模型，尽管需要牺牲一些可解释性。</p>
<h1 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h1><h2 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h2><p>在PyTorch中，torch.Tensor是存储和变换数据的主要工具。Tensor和NumPy的多维数组非常类似。然而，Tensor提供GPU计算和自动求梯度等更多功能，这些使Tensor更加适合深度学习。</p>
<h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>) <span class="comment"># 创建一个5x3的未初始化的Tensor</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>) <span class="comment"># 创建一个5x3的随机初始化的Tensor</span></span><br><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long) <span class="comment"># 创建一个5x3的long型全0的Tensor</span></span><br><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>]) <span class="comment"># 接根据数据创建</span></span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.float64) <span class="comment"># 可以通过现有的Tensor来创建，此方法会默认重用输入Tensor的一些属性，例如torch.dtype和torch.device</span></span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>) <span class="comment"># 除非自定义数据类型</span></span><br><span class="line"><span class="built_in">print</span>(x.shape) <span class="comment"># 可以通过shape或者size()来获取Tensor的形状</span></span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># 注意：返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。</span></span><br></pre></td></tr></table></figure>
<p>还有很多函数可以创建Tensor，如eye、arange、linspace，这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu)。</p>
<h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><p>（1）算术操作，以加法为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x + y) <span class="comment"># 加法形式一</span></span><br><span class="line"><span class="built_in">print</span>(torch.add(x, y)) <span class="comment"># 加法形式二</span></span><br><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result) <span class="comment"># 可以指定输出</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line">y.add_(x) <span class="comment"># 加法形式三 inplace，PyTorch操作inplace版本都有后缀_</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>

<p>（2）索引<br>可以使用类似NumPy的索引操作来访问Tensor的一部分，需要注意的是：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = x[<span class="number">0</span>, :]</span><br><span class="line">y += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>, :])</span><br></pre></td></tr></table></figure>
<p>PyTorch还提供了一些高级的选择函数，如index_select、masked_select等。<br>（3）改变形状<br>用view()来改变Tensor的形状：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = x.view(<span class="number">15</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>, <span class="number">5</span>) <span class="comment"># -1所指的维度可以根据其他维度的值推出来</span></span><br><span class="line"><span class="built_in">print</span>(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure>
<p>注意view()返回的新Tensor与源Tensor虽然可能有不同的size，但是是共享data的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)<br>另外注意：虽然view返回的Tensor与源Tensor是共享data的，但是依然是一个新的Tensor（因为Tensor除了包含data外还有一些其他属性），二者id（内存地址）并不一致。<br>所以如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个reshape()可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用clone创造一个副本然后再使用view。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_cp = x.clone().view(<span class="number">15</span>)</span><br><span class="line">x -= <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x_cp)</span><br></pre></td></tr></table></figure>
<p>使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor。<br>另外一个常用的函数就是item(), 它可以将一个标量Tensor转换成一个Python number：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.item())</span><br></pre></td></tr></table></figure>
<p>PyTorch中的Tensor支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，具体可参考官方API。</p>
<h3 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h3><p>当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">3</span>).view(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">y = torch.arange(<span class="number">1</span>, <span class="number">4</span>).view(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br></pre></td></tr></table></figure>
<p>这部分的广播机制可以参照numpy的广播来理解。</p>
<h3 id="运算的内存开销"><a href="#运算的内存开销" class="headerlink" title="运算的内存开销"></a>运算的内存开销</h3><p>前面说了，索引操作是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。可以使用Python自带的id函数来验证：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = <span class="built_in">id</span>(y)</span><br><span class="line">y = x + y</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(y) == id_before)</span><br></pre></td></tr></table></figure>
<p>如果想指定结果到原来的y的内存，可以使用前面介绍的索引来进行替换操作，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = <span class="built_in">id</span>(y)</span><br><span class="line">y[:] = x + y</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(y) == id_before)</span><br></pre></td></tr></table></figure>
<p>还可以使用运算符全名函数中的out参数或者自加运算符+=(也即add_())达到上述效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = <span class="built_in">id</span>(y)</span><br><span class="line"><span class="comment"># torch.add(x, y, out = y) </span></span><br><span class="line"><span class="comment"># y += x</span></span><br><span class="line">y.add_(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(y) == id_before)</span><br></pre></td></tr></table></figure>

<h3 id="Tensor和Numpy相互转换"><a href="#Tensor和Numpy相互转换" class="headerlink" title="Tensor和Numpy相互转换"></a>Tensor和Numpy相互转换</h3><p>我们很容易用numpy()和from_numpy()将Tensor和NumPy中的数组相互转换。但是需要注意的一点是： 这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！<br>还有一个常用的将NumPy中的array转换成Tensor的方法就是torch.tensor(), 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的Tensor和原来的数据不再共享内存。<br>（1）Tensor转Numpy</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">b = a.numpy()</span><br><span class="line"><span class="built_in">print</span>(a, b)</span><br><span class="line">a += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(a, b)</span><br><span class="line">b += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(a, b)</span><br></pre></td></tr></table></figure>

<p>（2）Numpy转Tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line"><span class="built_in">print</span>(a, b)</span><br><span class="line">a += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(a, b)</span><br><span class="line">b += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(a, b)</span><br></pre></td></tr></table></figure>
<p>所有在CPU上的Tensor（除了CharTensor）都支持与NumPy数组相互转换。</p>
<h3 id="Tensor-on-GPU"><a href="#Tensor-on-GPU" class="headerlink" title="Tensor on GPU"></a>Tensor on GPU</h3><p>用方法to()可以将Tensor在CPU和GPU（需要硬件支持）之间相互移动。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    y = torch.ones_like(x, device=device) <span class="comment"># 直接创建一个在GPU上的Tensor</span></span><br><span class="line">    x = x.to(device)  <span class="comment"># 等价于.to(&#x27;cuda&#x27;)</span></span><br><span class="line">    z = x + y</span><br><span class="line">    <span class="built_in">print</span>(z)</span><br><span class="line">    <span class="built_in">print</span>(z.to(<span class="string">&#x27;cpu&#x27;</span>), torch.double)</span><br></pre></td></tr></table></figure>

<h2 id="自动求梯度"><a href="#自动求梯度" class="headerlink" title="自动求梯度"></a>自动求梯度</h2><p>PyTorch提供的autograd包能够根据输入和前向传播过程自动构建计算图，并执行反向传播。</p>
<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>Tensor是autograd包的核心类，如果将其属性.requires_grad设置为True，它将开始追踪(track)在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用.backward()来完成所有梯度计算。此Tensor的梯度将累积到.grad属性中。<br>注意在y.backward()时，如果y是标量，则不需要为backward()传入任何参数；否则，需要传入一个与y同形的Tensor。<br>如果不想要被继续追踪，可以调用.detach()将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用with torch.no_grad()将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（requires_grad=True）的梯度。</p>
<p>Function是另外一个很重要的类。Tensor和Function互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个Tensor都有一个.grad_fn属性，该属性即创建该Tensor的Function, 就是说该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与这些运算相关的对象，否则是None。</p>
<h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># 创建一个Tensor并设置requires_grad=True</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.grad_fn) <span class="comment"># None, x是直接创建的，所以它没有grad_fn</span></span><br><span class="line">y = x + <span class="number">2</span> </span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.grad_fn) <span class="comment"># y是通过一个加法操作创建的，所以它有一个为&lt;AddBackward&gt;的grad_fn</span></span><br><span class="line"><span class="built_in">print</span>(x.is_leaf, y.is_leaf) <span class="comment"># 像x这种直接创建的称为叶子节点，叶子节点对应的grad_fn是None</span></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(z, out) <span class="comment"># z的grad_fn是MulBackward，out的grad_fn是MeanBackward</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>) <span class="comment"># 缺失情况下默认 requires_grad = False</span></span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>) <span class="comment"># 通过.requires_grad_()来用in-place的方式改变requires_grad属性</span></span><br><span class="line"><span class="built_in">print</span>(a.requires_grad)</span><br><span class="line">b = (a * a).<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(b.grad_fn) <span class="comment"># b的grad_fn是SumBackward</span></span><br></pre></td></tr></table></figure>

<h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">out.backward() <span class="comment"># 因为out是一个标量，所以调用backward()时不需要指定求导变量</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">out2 = x.<span class="built_in">sum</span>()</span><br><span class="line">out2.backward() <span class="comment"># grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">out3 = x.<span class="built_in">sum</span>()</span><br><span class="line">x.grad.data.zero_() <span class="comment"># 所以一般在反向传播之前需把梯度清零</span></span><br><span class="line">out3.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>
<p>首先，有一个向量x，它经过某个运算得到了向量y，那么y对x的导数就是一个Jacobian矩阵，记为J。<br>再者，始终记着，且抛开诸如神经网络等所有的知识点，从计算上来说，torch.autograd就是一个纯粹的计算引擎，它是计算向量与Jacobian矩阵的乘积的一个运算库（Jacobian矩阵是在底层计算的），即vector-Jabobian product计算引擎。这个vector可以是任意向量，记为v，注意是任意的。<br>最后，向量y经过运算又得到了标量l。此时，如果上面的向量v恰好是l对y的导数，那么根据链式法则，autograd所计算的vector-Jacobian product恰好就是标量l对向量x的导数。所以，autograd的效果就是只要知道y=f(x)和l=g(y)或者l对y的导数v，就可以得到l对x的导数。</p>
<p>那么，具体来说，autograd在反向传播计算梯度时，out.backward()其实是需要一个grad_tensors，就是上面那个向量v，则有两种情形：<br>（1）如果out是标量，该参数就可以为空，此时out就是l。因为通常是最后的损失函数调用backward()，而损失函数又通常是标量，所以此时backward()不需要参数。<br>（2）如果out不是标量，比如上面的y，y.backward(v)，那么就需要指定这个v，而这个v是与y同型的，这是为了让y乘以v是一个标量，这样就保证了始终是标量对张量求导。再说一遍，这个v可以是任意的，但如果恰巧是未知的标量l对y的导数，这样往前计算的梯度就有了意义，比如x中的梯度就是遥远的l对x的梯度。<br>那么，总的计算梯度的方式就是这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x -&gt; y -&gt; z -&gt; m -&gt; n -&gt; l</span><br><span class="line">设l就是一个标量，那么：</span><br><span class="line">l.backward()</span><br><span class="line">就会先计算dl/dn, 然后：</span><br><span class="line">n.backward(dl/dn)</span><br><span class="line">这样，仍然n*dl/dn是一个标量，就是l，那么继续往前传，先得到dl/dm，再传入：</span><br><span class="line">m.backward(dl/dm)</span><br><span class="line">所以，每一次backward()时总是一个标量，而且是l，所以，传到最后，就可以得到：</span><br><span class="line">dl/dx</span><br></pre></td></tr></table></figure>

<p>如果我们想要修改tensor的数值，但是又不希望被autograd记录（即不会影响反向传播），那么我们可以对tensor.data进行操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x.data) <span class="comment"># 还是一个Tensor</span></span><br><span class="line"><span class="built_in">print</span>(x.data.requires_grad) <span class="comment"># False，此时x已经独立于计算图以外</span></span><br><span class="line"></span><br><span class="line">y = <span class="number">2</span> * x</span><br><span class="line">x.data *= <span class="number">100</span> <span class="comment"># 只改变了值，不会记录在计算图中，所以不会影响梯度传播</span></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># x的值会发生变化</span></span><br><span class="line"><span class="built_in">print</span>(x.grad) <span class="comment"># 但梯度不变</span></span><br></pre></td></tr></table></figure>

<h1 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归输出是一个连续值，因此适用于回归问题。<br>与回归问题不同，分类问题中模型的最终输出是一个离散值。softmax回归则适用于分类问题。</p>
<h2 id="线性回归的从零开始实现"><a href="#线性回归的从零开始实现" class="headerlink" title="线性回归的从零开始实现"></a>线性回归的从零开始实现</h2><p>尽管强大的深度学习框架可以减少大量重复性工作，但若过于依赖它提供的便利，会导致我们很难深入理解深度学习是如何工作的。因此，本节将介绍如何只利用Tensor和autograd来实现一个线性回归的训练。<br>首先，导入本节中实验所需的包或模块，其中的matplotlib包可用于作图，且设置成嵌入显示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br></pre></td></tr></table></figure>

<h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造一个简单的人工训练数据集，它可以使我们能够直观比较学到的参数和真实的模型参数的区别</span></span><br><span class="line">num_inputs = <span class="number">2</span> <span class="comment"># 输入特征数为2</span></span><br><span class="line">num_examples = <span class="number">100</span> <span class="comment"># 训练集样本数为100</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>] <span class="comment"># 真实的权重</span></span><br><span class="line">true_b = <span class="number">4.2</span> <span class="comment"># 真实的偏差</span></span><br><span class="line"></span><br><span class="line">features = torch.randn(num_examples, num_inputs, dtype=torch.float32) <span class="comment"># 根据训练集个数生成随机的批量样本</span></span><br><span class="line">labels = true_w[<span class="number">0</span>]*features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>]*features[:, <span class="number">1</span>] + true_b <span class="comment"># 真实的标签</span></span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float32) <span class="comment"># 在真实标签上加上服从正态分布的噪声项</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(features[<span class="number">0</span>], labels[<span class="number">0</span>]) <span class="comment"># 打印第一个样本的输入特征和标签</span></span><br><span class="line"></span><br><span class="line">plt.scatter(features[:, <span class="number">1</span>].numpy(), labels.numpy(), <span class="number">1</span>) <span class="comment"># 将第二个特征与标签进行作图，看一下它们的线性关系</span></span><br></pre></td></tr></table></figure>

<h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><p>在训练模型的时候，我们需要遍历数据集并不断读取小批量数据样本。这里我们定义一个函数：它每次返回batch_size（批量大小）个随机样本的特征和标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span></span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples)) <span class="comment"># 生成读取索引</span></span><br><span class="line">    random.shuffle(indices) <span class="comment"># 打乱索引顺序</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = torch.LongTensor(indices[i:<span class="built_in">min</span>(i+batch_size, num_examples)]) <span class="comment"># 将索引包装成一个Tensor，同时注意最后一次可能不足一个batch，要取实际的大小</span></span><br><span class="line">        <span class="keyword">yield</span> features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j) <span class="comment"># yield关键字将data_iter()函数做成了一个迭代器</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, y)</span><br><span class="line">    <span class="keyword">break</span> <span class="comment"># 读取第一个小批量数据样本并打印。每个批量的特征形状为(10, 2)，分别对应批量大小和输入个数；标签形状为批量大小。</span></span><br></pre></td></tr></table></figure>
<p>对这里面用的yield用法，可以参看这篇博文辅助理解：<a target="_blank" rel="noopener" href="https://blog.csdn.net/mieleizhi0522/article/details/82142856">python中yield的用法详解——最简单，最清晰的解释</a></p>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, <span class="number">1</span>)), dtype=torch.float32) <span class="comment"># 将权重初始化为均值为0、标准差为0.01的正态随机数</span></span><br><span class="line">b = torch.zeros(<span class="number">1</span>, dtype=torch.float32) <span class="comment"># 将偏差初始化为0</span></span><br><span class="line">w.requires_grad_(requires_grad=<span class="literal">True</span>) <span class="comment"># 模型训练过程中需要对这些参数求梯度来迭代取值，所以需要将requires_grad置为True</span></span><br><span class="line">b.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linreg</span>(<span class="params">X, w, b</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.mm(X, w) + b <span class="comment"># 线性回归的矢量计算表达式</span></span><br></pre></td></tr></table></figure>
<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="comment"># 注意这里返回的是向量，把y变成了y_hat的样子，注意这个地方与PyTorch中MSELoss不同</span></span><br><span class="line">       <span class="comment"># MSELoss返回的是标量，即将这里的向量再做一个操作，默认是取平均值，也可以取和</span></span><br><span class="line">    <span class="keyword">return</span>(y_hat - y.view(y_hat.size())) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="定义优化算法"><a href="#定义优化算法" class="headerlink" title="定义优化算法"></a>定义优化算法</h3><p>以下的sgd函数实现了小批量随机梯度下降算法，它通过不断迭代模型参数来优化损失函数。这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和（这是因为上面的损失函数是带小批量的，后面的loss.sum()会将这一批样本上的损失都加和，如果是像PyTorch的MSELoss取平均值的话，这里就不需要除以批量大小），将它除以批量大小来得到平均值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">params, lr, batch_size</span>):</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size <span class="comment"># 注意这里更改param用的是param.data</span></span><br></pre></td></tr></table></figure>
<p>下图的计算公式一目了然：<br><img src="https://user-images.githubusercontent.com/6218739/75660181-4d558c00-5ca6-11ea-95a8-a343ea73ad35.png" alt="image"></p>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>在训练中，我们将多次迭代模型参数。在每次迭代中，我们根据当前读取的小批量数据样本（特征X和标签y），通过调用反向函数backward计算小批量随机梯度，并调用优化算法sgd迭代模型参数。由于我们之前设批量大小batch_size为10，每个小批量的损失l的形状为(10, 1)。由于变量l并不是一个标量，所以我们可以调用.sum()将其求和得到一个标量，再运行l.backward()得到该变量有关模型参数的梯度。注意在每次更新完参数后不要忘了将参数的梯度清零。<br>在一个迭代周期（epoch）中，我们将完整遍历一遍data_iter函数，并对训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。这里的迭代周期个数num_epochs和学习率lr都是超参数，分别设3和0.03。在实践中，大多超参数都需要通过反复试错来不断调节。虽然迭代周期数设得越大模型可能越有效，但是训练时间可能过长。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span> <span class="comment"># 学习率</span></span><br><span class="line">num_epochs = <span class="number">30</span> <span class="comment"># 迭代次数</span></span><br><span class="line">net = linreg <span class="comment"># 线性回归模型</span></span><br><span class="line">loss = squared_loss <span class="comment"># 平方损失</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y).<span class="built_in">sum</span>() <span class="comment"># l是有关小批量X和y的损失</span></span><br><span class="line">        l.backward() <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size) <span class="comment"># 使用小批量随机梯度下降迭代模型参数</span></span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">    train_l = loss(net(features, w, b), labels) <span class="comment"># 每次迭代后都将模型测试一下</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch + <span class="number">1</span>, train_l.mean().item()))</span><br></pre></td></tr></table></figure>
<p>打印一下最终的参数，它们应该很接近于真实的参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(true_w, <span class="string">&#x27;\n&#x27;</span>, w)</span><br><span class="line"><span class="built_in">print</span>(true_b, <span class="string">&#x27;\n&#x27;</span>, b)</span><br></pre></td></tr></table></figure>
<h2 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h2><p>在本节中，将介绍如何使用PyTorch更方便地实现线性回归的训练。</p>
<h3 id="生成数据集-1"><a href="#生成数据集-1" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>这一步跟上面的相同，不再赘述。</p>
<h3 id="读取数据-1"><a href="#读取数据-1" class="headerlink" title="读取数据"></a>读取数据</h3><p>PyTorch提供了data包来读取数据。由于data常用作变量名，将导入的data模块用Data代替。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line">batch_size = <span class="number">10</span> <span class="comment"># 一个小批量设为10个</span></span><br><span class="line">dataset = Data.TensorDataset(features, labels) <span class="comment"># 将训练数据的特征和标签组合</span></span><br><span class="line">data_iter = Data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这里data_iter的使用跟上一节中的一样。让我们读取并打印第一个小批量数据样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">    <span class="built_in">print</span>(X, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><p>首先，导入torch.nn模块。实际上，“nn”是neural networks（神经网络）的缩写。顾名思义，该模块定义了大量神经网络的层。之前我们已经用过了autograd，而nn就是利用autograd来定义模型。nn的核心数据结构是Module，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络/层。一个nn.Module实例应该包含一些层以及返回输出的前向传播（forward）方法。下面先来看看如何用nn.Module实现一个线性回归模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(n_feature, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span> <span class="comment"># 定义前向传播</span></span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = LinearNet(num_inputs)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>
<p>事实上我们还可以用nn.Sequential来更加方便地搭建网络，Sequential是一个有序的容器，网络层将按照在传入Sequential的顺序依次被添加到计算图中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写法一</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 还可以再添加层</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 写法二</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add_module(<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 写法三</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(OrderedDict([</span><br><span class="line">                                 (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">                                 <span class="comment"># 还可以再添加层</span></span><br><span class="line">]))</span><br><span class="line"><span class="built_in">print</span>(net) <span class="comment"># 注意这个地方的输出与前面自定义net的不同</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>]) <span class="comment"># 这里的net是Sequential实例，所以需要加索引</span></span><br></pre></td></tr></table></figure>
<p>可以通过net.parameters()来查看模型所有的可学习参数，此函数将返回一个生成器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param) <span class="comment"># 这里面的参数的个数和尺寸是根据网络层的结构及输入输出自动确定的。</span></span><br></pre></td></tr></table></figure>
<p>注意：torch.nn仅支持输入一个batch的样本不支持单个样本输入，如果只有单个样本，可使用input.unsqueeze(0)来添加一维。</p>
<h3 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>在使用net前，我们需要初始化模型参数，如线性回归模型中的权重和偏差。PyTorch在init模块中提供了多种参数初始化方法。这里的init是initializer的缩写形式。我们通过init.normal_将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布。偏差会初始化为零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="comment"># net[0]这样根据下标访问子模块的写法只有当net是个ModuleList或者Sequential实例时才可以</span></span><br><span class="line"><span class="comment"># 如果net是像第一种那样自定义的，需要将索引去掉</span></span><br><span class="line"></span><br><span class="line">init.normal_(net[<span class="number">0</span>].weight, mean=<span class="number">0</span>, std=<span class="number">0.1</span>)</span><br><span class="line">init.constant_(net[<span class="number">0</span>].bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="定义损失函数-1"><a href="#定义损失函数-1" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>PyTorch在nn模块中提供了各种损失函数，这些损失函数可看作是一种特殊的层，PyTorch也将这些损失函数实现为nn.Module的子类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss() <span class="comment"># 使用均方误差损失作为损失函数</span></span><br></pre></td></tr></table></figure>

<h3 id="定义优化算法-1"><a href="#定义优化算法-1" class="headerlink" title="定义优化算法"></a>定义优化算法</h3><p>同样，我们也无须自己实现小批量随机梯度下降算法。torch.optim模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>) <span class="comment"># 设置学习率为0.03</span></span><br><span class="line"><span class="built_in">print</span>(optimizer)</span><br></pre></td></tr></table></figure>
<p>还可以为不同子网络设置不同的学习率，这在finetune时经常用到。例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里不能直接运行，因为subnet1都是假的</span></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">                       &#123;<span class="string">&#x27;params&#x27;</span>: net.subnet1.parameters()&#125;, <span class="comment"># 如果对某个参数不指定学习率，就使用最外层的默认学习率</span></span><br><span class="line">                       &#123;<span class="string">&#x27;params&#x27;</span>: net.subnet2.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>&#125;</span><br><span class="line">], lr=<span class="number">0.03</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>有时候我们不想让学习率固定成一个常数，那如何调整学习率呢？主要有两种做法。一种是修改optimizer.param_groups中对应的学习率，另一种是更简单也是较为推荐的做法——新建优化器，由于optimizer十分轻量级，构建开销很小，故而可以构建新的optimizer。但是后者对于使用动量的优化器（如Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    param_group[<span class="string">&#x27;lr&#x27;</span>] *= <span class="number">0.1</span> <span class="comment"># 学习率调整为之前的0.1倍</span></span><br></pre></td></tr></table></figure>
<h3 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h3><p>通过调用optim实例的step函数来迭代模型参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_epochs+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        output = net(X)</span><br><span class="line">        l = loss(output, y.view(-<span class="number">1</span>, <span class="number">1</span>)) <span class="comment"># output的size是[10, 1]，而y的size是[10]，所以y要改变一下形状</span></span><br><span class="line">        <span class="comment"># 同时，这里的l是算的这一批次上的样本的损失的平均值</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 梯度清零， 等价于net.zero_grad()</span></span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch, l.item()))</span><br></pre></td></tr></table></figure>
<p>将学习到的权重和偏差与真实值进行一下对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(true_w, net[<span class="number">0</span>].weight)</span><br><span class="line"><span class="built_in">print</span>(true_b, net[<span class="number">0</span>].bias)</span><br></pre></td></tr></table></figure>
<h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>前几节介绍的线性回归模型适用于输出为连续值的情景。在另一类情景中，模型输出可以是一个像图像类别这样的离散值。对于这样的离散值预测问题，我们可以使用诸如softmax回归在内的分类模型。和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。</p>
<p>虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到1、2和3这样的离散值之一，但这种连续值到离散值的转化通常会影响到分类质量。因此我们一般使用更加适合离散值输出的模型来解决分类问题。<br>（1）softmax运算的必要性<br>既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值当作预测类别是i的置信度，并将值最大的输出所对应的类作为预测输出。然而，直接使用输出层的输出有两个问题。一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。<br>softmax运算符（softmax operator）解决了以上两个问题。它通过softmax操作将输出值变换成值为正且和为1的概率分布，这样某一概率大了，也等价于它的输出是大的，这样就不改变预测类别输出。<br>（2）交叉熵损失函数<br>softmax运算将输出变换成一个合法的类别预测分布。另一方面，真实标签也可以用类别分布表达，比如构造一个向量，使其第i个元素为1，其余为0，就代表样本i类别的离散数值。这样，训练目标就可以设为预测概率分布尽可能接近于真实的标签概率分布。<br>因此，就是寻找适合衡量两个概率分布差异的测量函数，其中，交叉熵（cross entropy）是一个常用的衡量方法，它只关心对正确类别的预测概率，只要其值足够大，就可以确保分类结果正确，因为如果不是正确类别，这个交叉熵就是0。<br>（3）模型预测及评价<br>在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常把预测概率最大的类别作为输出类别。如果它与真实类别（标签）一致，说明这次预测是正确的。下面将使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。</p>
<h2 id="图像分类数据集Fashion-MNIST"><a href="#图像分类数据集Fashion-MNIST" class="headerlink" title="图像分类数据集Fashion-MNIST"></a>图像分类数据集Fashion-MNIST</h2><p>本节将使用torchvision包来加载Fashion-MNIST数据集，<br>它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。torchvision主要由以下几部分构成：<br>torchvision.datasets: 一些加载数据的函数及常用的数据集接口；<br>torchvision.models: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；<br>torchvision.transforms: 常用的图片变换，例如裁剪、旋转等；<br>torchvision.utils: 其他的一些有用的方法。</p>
<h3 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过train参数指定获取训练集或测试集，download指定是否下载</span></span><br><span class="line"><span class="comment"># transforms.ToTensor()使所有数据转换为Tensor，如果不进行转换则返回的是PIL图片</span></span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;.&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;.&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br></pre></td></tr></table></figure>
<p>transforms.ToTensor()将尺寸为 (H x W x C) 且数据位于[0, 255]的PIL图片或者数据类型为np.uint8的NumPy数组转换为尺寸为(C x H x W)且数据类型为torch.float32且位于[0.0, 1.0]的Tensor。<br>这个地方有个坑：如果输入的数组是0到255之间，但数据类型不是np.uint8，那么ToTensor()只会更改通道顺序，而不会除以255变换到0到1之间，所以，如果用像素值(0-255整数)表示图片数据，那么一律将其类型设置成uint8，避免不必要的bug。一定要确保input和output都是心里有数的。</p>
<p>见如下几篇相关帖子：<br><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/bugs-with-torchvision-transforms-topilimage/26109">Bugs with torchvision.transforms.ToPILImage()?</a><br><a target="_blank" rel="noopener" href="https://tangshusen.me/2018/12/05/kaggle-doodle-reco/">2.2.4 图像数据的一个坑</a></p>
<p>查看数据集中的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> img_as_ubyte</span><br><span class="line"><span class="comment"># mnist_train和mnist_test都是torch.utils.data.Dataset的子类</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(mnist_train))</span><br><span class="line">feature, label = mnist_train[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(feature.shape, label) <span class="comment"># shape是通道*高*宽</span></span><br><span class="line">io.imsave(<span class="string">&quot;1.png&quot;</span>, img_as_ubyte(feature.view((<span class="number">28</span>, <span class="number">28</span>)).numpy()))</span><br></pre></td></tr></table></figure>
<p>feature中的图像已经是[0.0, 1.0]范围，所以需要转化成[0, 255]范围才能正常显示。<br>参考见<a target="_blank" rel="noopener" href="https://www.cnblogs.com/denny402/p/5122328.html">这篇文章</a>。</p>
<p>将数值标签与文本标签对应起来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fashion_mnist_labels</span>(<span class="params">labels</span>):</span></span><br><span class="line">    text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>, <span class="string">&#x27;sneaker&#x27;</span>, <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br></pre></td></tr></table></figure>
<h3 id="读取小批量"><a href="#读取小批量" class="headerlink" title="读取小批量"></a>读取小批量</h3><p>在实践中，数据读取经常是训练的性能瓶颈，特别当模型较简单或者计算硬件性能较高时。PyTorch的DataLoader中一个很方便的功能是允许使用多进程来加速数据读取。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">num_workers = <span class="number">4</span></span><br><span class="line"><span class="comment"># mnist_train是torch.utils.data.Dataset的子类，所以我们可以将其传入torch.utils.data.DataLoader来创建一个读取小批量数据样本的DataLoader实例</span></span><br><span class="line"><span class="comment"># 通过参数num_workers来设置4个进程读取数据</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(mnist_train, batch_size = batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(mnist_test, batch_size = batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br></pre></td></tr></table></figure>
<p>查看一下，读取一遍训练数据需要的时间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start = time.time()</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;%.2f sec&#x27;</span> % (time.time()-start))</span><br></pre></td></tr></table></figure>
<h2 id="softmax回归的从零开始实现"><a href="#softmax回归的从零开始实现" class="headerlink" title="softmax回归的从零开始实现"></a>softmax回归的从零开始实现</h2><h3 id="获取和读取数据"><a href="#获取和读取数据" class="headerlink" title="获取和读取数据"></a>获取和读取数据</h3><p>这一步就是按照上一节中下载和小批量读取Fashion-MNIST数据集的方式。</p>
<h3 id="初始化模型参数-2"><a href="#初始化模型参数-2" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>跟线性回归中的例子一样，我们将使用向量表示每个样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">num_inputs = <span class="number">784</span> <span class="comment"># 这里用向量来表示图像，28*28=784，相当于784个输入特征</span></span><br><span class="line">num_outputs = <span class="number">10</span> <span class="comment"># 输出为10个类别</span></span><br><span class="line">W = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, num_outputs)), dtype=torch.<span class="built_in">float</span>) <span class="comment"># 权重为784*10，初始为正态分布</span></span><br><span class="line">b = torch.tensor(np.zeros(num_outputs), dtype=torch.<span class="built_in">float</span>) <span class="comment"># 偏差为10， 初始为0</span></span><br><span class="line">W.requires_grad_(requires_grad=<span class="literal">True</span>) <span class="comment"># 设置需要计算梯度</span></span><br><span class="line">b.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="实现softmax运算"><a href="#实现softmax运算" class="headerlink" title="实现softmax运算"></a>实现softmax运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先演示一下怎样对多维Tensor按维度进行操作</span></span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(X.<span class="built_in">sum</span>(dim = <span class="number">0</span>, keepdim=<span class="literal">True</span>)) <span class="comment"># tensor([[5, 7, 9]])，对同一列的元素求和，并保持原来的维度</span></span><br><span class="line"><span class="built_in">print</span>(X.<span class="built_in">sum</span>(dim = <span class="number">1</span>, keepdim=<span class="literal">True</span>)) <span class="comment"># tensor([[6], [15]])，对同一行的元素求和，并保持原来的维度</span></span><br><span class="line"><span class="comment"># 定义softmax运算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">X</span>):</span> <span class="comment"># 矩阵X的行数是样本数，列数是输出个数</span></span><br><span class="line">    X_exp = X.exp() <span class="comment"># 先对每个元素做指数运算</span></span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(dim = <span class="number">1</span>, keepdim = <span class="literal">True</span>) <span class="comment"># 然后对exp矩阵同一行的元素求和</span></span><br><span class="line">    <span class="keyword">return</span> X_exp / partition <span class="comment"># 最后令exp矩阵的每行各元素与该行元素之和相除，最终使得每行元素的和为1且非负，所以每行都是合法的概率分布</span></span><br><span class="line">    <span class="comment"># 这个除法也应用了广播机制</span></span><br></pre></td></tr></table></figure>

<h3 id="定义模型-2"><a href="#定义模型-2" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="comment"># 注意，这里需要使用view函数将图像转换为向量</span></span><br><span class="line">    <span class="keyword">return</span> softmax(torch.mm(X.view((-<span class="number">1</span>, num_inputs)), W) + b)</span><br></pre></td></tr></table></figure>
<h3 id="定义损失函数-2"><a href="#定义损失函数-2" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先演示一下gather函数的用法</span></span><br><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]]) <span class="comment"># 这是虚构的两个样本在3个类别下的预测概率</span></span><br><span class="line">y = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>]) <span class="comment"># 这是虚构的两个样本的标签类别</span></span><br><span class="line">y_hat.gather(<span class="number">1</span>, y.view(-<span class="number">1</span>, <span class="number">1</span>)) <span class="comment"># tensor([[0.1000], [0.5000]]) 这样就得到了两个样本的标签的预测概率</span></span><br></pre></td></tr></table></figure>
<p>关于gather函数，其实就是索引，具体解析可以参看<a target="_blank" rel="noopener" href="https://www.cnblogs.com/HongjianChen/p/9451526.html">这篇博文</a>。<br>那么交叉熵损失函数就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat.gather(<span class="number">1</span>, y.view(-<span class="number">1</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure>

<h3 id="计算分类准确率"><a href="#计算分类准确率" class="headerlink" title="计算分类准确率"></a>计算分类准确率</h3><p>给定一个类别的预测概率分布y_hat，我们把预测概率最大的类别作为输出类别。如果它与真实类别y一致，说明这次预测是正确的。分类准确率即正确预测数量与总预测数量之比。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="comment"># argmax返回矩阵y_hat每行中最大元素的索引，且返回结果与变量y形状相同</span></span><br><span class="line">    <span class="comment"># 相等条件判断式(y_hat.argmax(dim=1) == y)是一个类型为ByteTensor的Tensor</span></span><br><span class="line">    <span class="comment"># 用float()将其转换为值为0（相等为假）或1（相等为真）的浮点型Tensor</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">float</span>().mean().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(accuracy(y_hat, y))</span><br><span class="line"><span class="comment"># 继续使用在演示gather函数时定义的变量y_hat和y，并将它们分别作为预测概率分布和标签</span></span><br><span class="line"><span class="comment"># 可以看到，第一个样本预测类别为2（该行最大元素0.6在本行的索引为2），与真实标签0不一致</span></span><br><span class="line"><span class="comment"># 第二个样本预测类别为2（该行最大元素0.5在本行的索引为2），与真实标签2一致</span></span><br><span class="line"><span class="comment"># 因此，这两个样本上的分类准确率为0.5。</span></span><br></pre></td></tr></table></figure>

<p>评价模型net在数据集data_iter上的准确率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">data_iter, net</span>):</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        acc_sum += (net(X).argmax(dim=<span class="number">1</span>)==y).<span class="built_in">float</span>().<span class="built_in">sum</span>().item() <span class="comment"># 注意这个地方先求和</span></span><br><span class="line">        n +=  y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> acc_sum /n <span class="comment"># 再求平均</span></span><br></pre></td></tr></table></figure>

<h3 id="训练模型-2"><a href="#训练模型-2" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">params, lr, batch_size</span>):</span> <span class="comment"># 定义优化算法</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, batch_size, params=<span class="literal">None</span>, lr=<span class="literal">None</span>, optimizer=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y).<span class="built_in">sum</span>() <span class="comment"># 这个地方是求和，所以后面的SGD算法中除以batch size</span></span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># 这里看是否传入了优化器，默认是不传入</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">            <span class="keyword">elif</span> params <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># 如果不传入优化器，就显式地对参数梯度置为0</span></span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                optimizer.step()</span><br><span class="line"></span><br><span class="line">            train_l_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#x27;</span> </span><br><span class="line">              % (epoch+<span class="number">1</span>, train_l_sum/n, train_acc_sum/n, test_acc))</span><br><span class="line"></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)</span><br></pre></td></tr></table></figure>

<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, y = <span class="built_in">iter</span>(test_iter).<span class="built_in">next</span>()</span><br><span class="line"><span class="built_in">print</span>(net(X).argmax(dim=<span class="number">1</span>)[:<span class="number">10</span>], y[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<h2 id="softmax回归的简洁实现"><a href="#softmax回归的简洁实现" class="headerlink" title="softmax回归的简洁实现"></a>softmax回归的简洁实现</h2><h3 id="获取和读取数据-1"><a href="#获取和读取数据-1" class="headerlink" title="获取和读取数据"></a>获取和读取数据</h3><p>与上一节相同</p>
<h3 id="定义和初始化模型"><a href="#定义和初始化模型" class="headerlink" title="定义和初始化模型"></a>定义和初始化模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_inputs, num_outputs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(num_inputs, num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span> <span class="comment"># x的形状是(batch, 1, 28, 28)，所以要view一下</span></span><br><span class="line">        y = self.linear(x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = LinearNet(num_inputs, num_outputs)</span><br></pre></td></tr></table></figure>
<p>还可以将形状转换这一块单独提出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FlattenLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    OrderedDict([</span><br><span class="line">                 (<span class="string">&#x27;flatten&#x27;</span>, FlattenLayer()),</span><br><span class="line">                 (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, num_outputs))</span><br><span class="line">    ])</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>然后，我们使用均值为0、标准差为0.01的正态分布随机初始化模型的权重参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net.linear.bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h3 id="softmax和交叉熵损失函数"><a href="#softmax和交叉熵损失函数" class="headerlink" title="softmax和交叉熵损失函数"></a>softmax和交叉熵损失函数</h3><p>在上一节的练习中，分开定义softmax运算和交叉熵损失函数可能会造成数值不稳定。因此，PyTorch提供了一个包括softmax运算和交叉熵损失计算的函数。它的数值稳定性更好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<h3 id="定义优化算法-2"><a href="#定义优化算法-2" class="headerlink" title="定义优化算法"></a>定义优化算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="训练模型-3"><a href="#训练模型-3" class="headerlink" title="训练模型"></a>训练模型</h3><p>使用上一节定义的训练函数，只是传入不同的参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>

<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><p>多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。隐藏层位于输入层和输出层之间。<br>由于输入层不涉及计算，因此，多层感知机的层数等于隐藏层的层数加上输出层。<br>全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。<br>（1）激活函数<br>对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。<br>常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。<br>（2）多层感知机<br>多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。<br>多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。</p>
<h2 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h2><p>过拟合现象，即模型的训练误差远小于它在测试集上的误差。虽然增大训练数据集可能会减轻过拟合，但是获取额外的训练数据往往代价高昂。<br>应对过拟合问题的常用方法：权重衰减（weight decay）和丢弃法（Dropout，下一节介绍）。<br>权重衰减等价于L2范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。<br>L2范数正则化在模型原损失函数基础上添加L2范数惩罚项，从而得到训练所需要最小化的函数。L2范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。<br>L2范数正则化令权重先自乘小于1的数，再减去不含惩罚项的梯度。因此，L2范数正则化又叫权重衰减。<br>权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，我们有时也在惩罚项中添加偏差元素的平方和。<br>可以直接在构造优化器实例时通过weight_decay参数来指定权重衰减超参数。默认下，PyTorch会对权重和偏差同时衰减。我们可以分别对权重和偏差构造优化器实例，从而只对权重衰减。</p>
<h2 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h2><p>当对某隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。<br>丢弃概率是丢弃法的超参数。<br>丢弃法不改变其输入的期望值。<br>在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法。<br>在PyTorch中，我们只需要在全连接层后添加Dropout层并指定丢弃概率。在训练模型时，Dropout层将以指定的丢弃概率随机丢弃上一层的输出元素；在测试模型时（即model.eval()后），Dropout层并不发挥作用。</p>
<h2 id="正向传播、反向传播和计算图"><a href="#正向传播、反向传播和计算图" class="headerlink" title="正向传播、反向传播和计算图"></a>正向传播、反向传播和计算图</h2><p>正向传播是指对神经网络沿着从输入层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）。<br>我们通常绘制计算图来可视化运算符和变量在计算中的依赖关系，其中方框代表变量，圆圈代表运算符，箭头表示从输入到输出之间的依赖关系。<br>反向传播指的是计算神经网络参数梯度的方法。总的来说，反向传播依据微积分中的链式法则，沿着从输出层到输入层的顺序，依次计算并存储目标函数有关神经网络各层的中间变量以及参数的梯度。</p>
<p>在训练深度学习模型时，正向传播和反向传播之间相互依赖：<br>一方面，正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的；<br>另一方面，反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。</p>
<p>因此，在模型参数初始化完成后，我们交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。既然我们在反向传播中使用了正向传播中计算得到的中间变量来避免重复计算，那么这个复用也导致正向传播结束后不能立即释放中间变量内存。这也是训练要比预测占用更多内存的一个重要原因。另外需要指出的是，这些中间变量的个数大体上与网络层数线性相关，每个变量的大小跟批量大小和输入个数也是线性相关的，它们是导致较深的神经网络使用较大批量训练时更容易超内存的主要原因。</p>
<h2 id="数值稳定性和模型初始化"><a href="#数值稳定性和模型初始化" class="headerlink" title="数值稳定性和模型初始化"></a>数值稳定性和模型初始化</h2><p>深度模型有关数值稳定性的典型问题是衰减（vanishing）和爆炸（explosion）。<br>在神经网络中，通常需要随机初始化模型参数。<br>随机初始化模型参数的方法有很多。之前的实现中，我们使用torch.nn.init.normal_()使模型net的权重参数采用正态分布的随机初始化方式。不过，PyTorch中nn.Module的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考源代码），因此一般不用我们考虑。</p>
<p>还有一种比较常用的随机初始化方法叫作Xavier随机初始化，它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。</p>
<h1 id="深度学习计算"><a href="#深度学习计算" class="headerlink" title="深度学习计算"></a>深度学习计算</h1><h2 id="模型构造"><a href="#模型构造" class="headerlink" title="模型构造"></a>模型构造</h2><p>有多种模型构造的方法：</p>
<h3 id="继承Module类来构造模型"><a href="#继承Module类来构造模型" class="headerlink" title="继承Module类来构造模型"></a>继承Module类来构造模型</h3><p>Module类是nn模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型。</p>
<h3 id="Module的子类"><a href="#Module的子类" class="headerlink" title="Module的子类"></a>Module的子类</h3><p>Module类是一个通用的部件。事实上，PyTorch还实现了继承自Module的可以方便构建模型的类: 如Sequential、ModuleList和ModuleDict等等。不过虽然Sequential等类可以使模型构造更加简单，但直接继承Module类可以极大地拓展模型构造的灵活性。<br>（1）Sequential类：当模型的前向计算为简单串联各个层的计算时，Sequential类可以通过更加简单的方式定义模型。这正是Sequential类的目的：它可以接收一个子模块的有序字典（OrderedDict）或者一系列子模块作为参数来逐一添加Module的实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。<br>（2）ModuleList类：ModuleList接收一个子模块的列表作为输入，然后也可以类似List那样进行append和extend操作。<br>既然Sequential和ModuleList都可以进行列表化构造网络，那二者区别是什么呢。ModuleList仅仅是一个储存各种模块的列表，这些模块之间没有联系也没有顺序（所以不用保证相邻层的输入输出维度匹配），而且没有实现forward功能需要自己实现；而Sequential内的模块需要按照顺序排列，要保证相邻层的输入输出大小相匹配，内部forward功能已经实现。<br>ModuleList的出现只是让网络定义前向传播时更加灵活；另外，ModuleList不同于一般的Python的list，加入到ModuleList里面的所有模块的参数会被自动添加到整个网络中。<br>（3）ModuleDict类：ModuleDict接收一个子模块的字典作为输入, 然后也可以类似字典那样进行添加访问操作；和ModuleList一样，ModuleDict实例仅仅是存放了一些模块的字典，并没有定义forward函数需要自己定义。同样，ModuleDict也与Python的Dict有所不同，ModuleDict里的所有模块的参数会被自动添加到整个网络中。</p>
<h2 id="模型参数的访问、初始化和共享"><a href="#模型参数的访问、初始化和共享" class="headerlink" title="模型参数的访问、初始化和共享"></a>模型参数的访问、初始化和共享</h2><h3 id="访问模型参数"><a href="#访问模型参数" class="headerlink" title="访问模型参数"></a>访问模型参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">4</span>, <span class="number">3</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">) <span class="comment"># Pytorch 已进行默认初始化</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">4</span>) <span class="comment"># 构造数据集的输入</span></span><br><span class="line">Y = net(X).<span class="built_in">sum</span>() <span class="comment"># 根据默认初始化的参数计算输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以通过Module类的parameters()或者named_parameters方法来访问所有参数（以迭代器的形式返回）</span></span><br><span class="line"><span class="comment"># 后者除了返回参数Tensor外还会返回其名字</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net.named_parameters()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name, param.size()) <span class="comment"># 返回的名字自动加上了层数的索引作为前缀</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于使用Sequential类构造的神经网络，可以通过方括号[]来访问网络的任一层。</span></span><br><span class="line"><span class="comment"># 索引0表示隐藏层为Sequential实例最先添加的层。</span></span><br><span class="line"><span class="comment"># 返回的param的类型为torch.nn.parameter.Parameter，其实这是Tensor的子类</span></span><br><span class="line"><span class="comment"># 和Tensor不同的是如果一个Tensor是Parameter，那么它会自动被添加到模型的参数列表里</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name, param.size(), <span class="built_in">type</span>(param)) <span class="comment"># 这里是单层的参数，所以名字中没有层数索引的前缀</span></span><br><span class="line"></span><br><span class="line">weight_0 = <span class="built_in">list</span>(net[<span class="number">0</span>].parameters())[<span class="number">0</span>] <span class="comment"># 要使用list将这个迭代器转化一下</span></span><br><span class="line"><span class="comment"># 因为Parameter是Tensor，即Tensor拥有的属性它都有，比如可以根据data来访问参数数值，用grad来访问参数梯度。</span></span><br><span class="line"><span class="built_in">print</span>(weight_0.data)</span><br><span class="line"><span class="built_in">print</span>(weight_0.grad) <span class="comment"># 反向传播前梯度为None</span></span><br><span class="line">Y.backward()</span><br><span class="line"><span class="built_in">print</span>(weight_0.grad)</span><br></pre></td></tr></table></figure>

<h3 id="初始化模型参数-3"><a href="#初始化模型参数-3" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>PyTorch中nn.Module的模块参数采取了默认的较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考源代码）。但我们经常需要使用其他方法来初始化权重。PyTorch的init模块里提供了多种预设的初始化方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;weight&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>) <span class="comment"># 用正态分布初始化权重</span></span><br><span class="line">        <span class="built_in">print</span>(name, param.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;bias&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        init.constant_(param, val=<span class="number">0</span>) <span class="comment"># 将偏差置0</span></span><br><span class="line">        <span class="built_in">print</span>(name, param.data)</span><br></pre></td></tr></table></figure>

<h3 id="自定义初始化方法"><a href="#自定义初始化方法" class="headerlink" title="自定义初始化方法"></a>自定义初始化方法</h3><p>有时候我们需要的初始化方法并没有在init模块中提供。这时，可以实现一个初始化方法，从而能够像使用其他初始化方法那样使用它。在这之前我们先来看看PyTorch是怎么实现这些初始化方法的，例如torch.nn.init.normal_：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_</span>(<span class="params">tensor, mean=<span class="number">0</span>, std=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">return</span> tensor.normal_(mean, std)</span><br></pre></td></tr></table></figure>
<p>可以看到这就是一个inplace改变Tensor值的函数，而且这个过程是不记录梯度的。 类似的我们来实现一个自定义的初始化方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weight_</span>(<span class="params">tensor</span>):</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        tensor.uniform_(-<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        tensor *= (tensor.<span class="built_in">abs</span>() &gt;= <span class="number">5</span>).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;weight&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        init_weight_(param)</span><br><span class="line">        <span class="built_in">print</span>(name, param.data)</span><br></pre></td></tr></table></figure>
<p>还可以通过改变这些参数的data来改写模型参数值同时不会影响梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;bias&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        param.data += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(name, param.data)</span><br></pre></td></tr></table></figure>

<h3 id="共享模型参数"><a href="#共享模型参数" class="headerlink" title="共享模型参数"></a>共享模型参数</h3><p>在有些情况下，我们希望在多个层之间共享模型参数。共享模型参数的方式有: Module类的forward函数里多次调用同一个层。此外，如果我们传入Sequential的模块是同一个Module实例的话参数也是共享的。<br>不过注意，因为模型参数里包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的。</p>
<h2 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h2><p>虽然PyTorch提供了大量常用的层，但有时候我们依然希望自定义层。本节将介绍如何使用Module来自定义层，从而可以被重复调用。</p>
<h3 id="不含模型参数的自定义层"><a href="#不含模型参数的自定义层" class="headerlink" title="不含模型参数的自定义层"></a>不含模型参数的自定义层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CenteredLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CenteredLayer, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将计算放在了forward函数里，所以，该自定义层没有模型参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x - x.mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以实例化这个层，然后做前向计算</span></span><br><span class="line">layer = CenteredLayer()</span><br><span class="line">layer(torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.<span class="built_in">float</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以用它来构造更复杂的模型</span></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">8</span>, <span class="number">128</span>), CenteredLayer())</span><br><span class="line">y = net(torch.rand(<span class="number">4</span>, <span class="number">8</span>))</span><br><span class="line">y.mean().item()</span><br></pre></td></tr></table></figure>

<h3 id="含模型参数的自定义层"><a href="#含模型参数的自定义层" class="headerlink" title="含模型参数的自定义层"></a>含模型参数的自定义层</h3><p>我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。<br>之前介绍了Parameter类其实是Tensor的子类，如果一个Tensor是Parameter，那么它会自动被添加到模型的参数列表里。所以在自定义含模型参数的层时，我们应该将参数定义成Parameter，除了像之前那样直接定义成Parameter类外，还可以使用ParameterList和ParameterDict分别定义参数的列表和字典。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ParameterList接收一个Parameter实例的列表作为输入然后得到一个参数列表，使用的时候可以用索引来访问某个参数</span></span><br><span class="line"><span class="comment"># 另外也可以使用append和extend在列表后面新增参数。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br><span class="line">        self.params.append(nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>)))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.params)):</span><br><span class="line">            x = torch.mm(x, self.params[i])</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = MyDense()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="comment"># 而ParameterDict接收一个Parameter实例的字典作为输入然后得到一个参数字典，然后可以按照字典的规则使用了。</span></span><br><span class="line"><span class="comment"># 例如使用update()新增参数，使用keys()返回所有键值，使用items()返回所有键值对等等</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDictDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.params = nn.ParameterDict(&#123;</span><br><span class="line">            <span class="string">&#x27;linear1&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)),</span><br><span class="line">            <span class="string">&#x27;linear2&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        self.params.update(&#123;<span class="string">&#x27;linear3&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">2</span>))&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, choice=<span class="string">&#x27;linear1&#x27;</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.mm(x, self.params[choice])</span><br><span class="line"></span><br><span class="line">net = MyDictDense()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这样就可以根据传入的键值来进行不同的前向传播</span></span><br><span class="line">x = torch.ones(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(net(x, <span class="string">&#x27;linear1&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(net(x, <span class="string">&#x27;linear2&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(net(x, <span class="string">&#x27;linear3&#x27;</span>))</span><br><span class="line"><span class="comment"># 可以使用自定义层构造模型。它和PyTorch的其他层在使用上很类似</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    MyDictDense(),</span><br><span class="line">    MyDense()</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="built_in">print</span>(net(x))</span><br></pre></td></tr></table></figure>

<h2 id="读取和存储"><a href="#读取和存储" class="headerlink" title="读取和存储"></a>读取和存储</h2><p>在实际中，我们有时需要把训练好的模型部署到很多不同的设备。在这种情况下，我们可以把内存中训练好的模型参数存储在硬盘上供后续读取使用。</p>
<h3 id="读写Tensor"><a href="#读写Tensor" class="headerlink" title="读写Tensor"></a>读写Tensor</h3><p>我们可以直接使用save函数和load函数分别存储和读取Tensor。save使用Python的pickle实用程序将对象进行序列化，然后将序列化的对象保存到disk，使用save可以保存各种对象,包括模型、张量和字典等。而load使用pickle unpickle工具将pickle的对象文件反序列化为内存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">3</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x.pt&#x27;</span>) <span class="comment"># 保存到文件</span></span><br><span class="line">x2 = torch.load(<span class="string">&#x27;x.pt&#x27;</span>) <span class="comment"># 读取文件数据到内存</span></span><br><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y], <span class="string">&#x27;xy.pt&#x27;</span>) <span class="comment"># 存储一个Tensor列表</span></span><br><span class="line"></span><br><span class="line">xy_list = torch.load(<span class="string">&#x27;xy.pt&#x27;</span>) <span class="comment"># 读取该Tensor列表</span></span><br><span class="line">torch.save(&#123;<span class="string">&#x27;x&#x27;</span>:x, <span class="string">&#x27;y&#x27;</span>:y&#125;, <span class="string">&#x27;xy_dict.pt&#x27;</span>) <span class="comment"># 存储一个从字符串映射到Tensor的字典</span></span><br><span class="line"></span><br><span class="line">xy_dict = torch.load(<span class="string">&#x27;xy_dict.pt&#x27;</span>)</span><br><span class="line">xy_dict</span><br></pre></td></tr></table></figure>

<h3 id="读写模型"><a href="#读写模型" class="headerlink" title="读写模型"></a>读写模型</h3><p>在PyTorch中，Module的可学习参数，即权重和偏差，模块模型包含在参数中(通过model.parameters()访问)。state_dict是一个从参数名称隐射到参数Tesnor的字典对象。<br>注意，只有具有可学习参数的层(卷积层、线性层等)才有state_dict中的条目。优化器(optim)也有一个state_dict，其中包含关于优化器状态以及所使用的超参数的信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.output = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        a = self.relu(self.hidden(X))</span><br><span class="line">        <span class="keyword">return</span> self.output(a)</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">net.state_dict() <span class="comment"># relu层没有可学习的参数</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">optimizer.state_dict()</span><br></pre></td></tr></table></figure>

<p>PyTorch中保存和加载训练模型有两种常见的方法:<br>（1） 仅保存和加载模型参数(state_dict)，这是推荐方式，形式为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH) <span class="comment"># 保存，推荐的文件后缀名是pt或pth</span></span><br><span class="line">model = TheModelClass(*args, **kwargs) <span class="comment"># 加载</span></span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>

<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">Y = net(X)</span><br><span class="line">PATH = <span class="string">&#x27;./net.pt&#x27;</span></span><br><span class="line">torch.save(net.state_dict(), PATH) <span class="comment"># 存储模型参数</span></span><br><span class="line"></span><br><span class="line">net2 = MLP() <span class="comment"># net2和net一样都是MLP()类，所以模型参数相同</span></span><br><span class="line">net2.load_state_dict(torch.load(PATH))</span><br><span class="line">Y2 = net2(X)</span><br><span class="line">Y2 == Y <span class="comment"># 两者计算结果相同</span></span><br></pre></td></tr></table></figure>
<p>（2）保存和加载整个模型，形式为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model, PATH) <span class="comment"># 保存</span></span><br><span class="line">model = torch.load(PATH) <span class="comment"># 加载</span></span><br></pre></td></tr></table></figure>

<h2 id="GPU计算"><a href="#GPU计算" class="headerlink" title="GPU计算"></a>GPU计算</h2><p>对复杂的神经网络和大规模的数据来说，使用CPU来计算可能不够高效。在本节中，将介绍如何使用单块NVIDIA GPU来计算。需要确保已经安装好了PyTorch GPU版本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi <span class="comment"># 查看显卡信息</span></span><br></pre></td></tr></table></figure>

<h3 id="计算设备"><a href="#计算设备" class="headerlink" title="计算设备"></a>计算设备</h3><p>PyTorch可以指定用来存储和计算的设备，如使用内存的CPU或者使用显存的GPU。默认情况下，PyTorch会将数据创建在内存，然后利用CPU来计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.cuda.is_available() <span class="comment"># 查看GPU是否可用</span></span><br><span class="line">torch.cuda.device_count() <span class="comment"># 查看GPU数目</span></span><br><span class="line">torch.cuda.current_device() <span class="comment"># 查看当前GPU索引号，从0开始</span></span><br><span class="line">torch.cuda.get_device_name(<span class="number">0</span>) <span class="comment"># 根据索引号查看GPU名称</span></span><br></pre></td></tr></table></figure>

<h3 id="Tensor的GPU计算"><a href="#Tensor的GPU计算" class="headerlink" title="Tensor的GPU计算"></a>Tensor的GPU计算</h3><p>默认情况下，Tensor会被存在内存上。因此，之前我们每次打印Tensor的时候看不到GPU相关标识。<br>使用.cuda()可以将CPU上的Tensor转换（复制）到GPU上。如果有多块GPU，我们用.cuda(i)来表示第 iii 块GPU及相应的显存（iii从0开始）且cuda(0)和cuda()等价。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x = x.cuda(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.device) <span class="comment"># 可以通过Tensor的device属性来查看该Tensor所在的设备</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], device=device) <span class="comment"># 可以在创建时就指定设备</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]).to(device) <span class="comment"># 第二种方法</span></span><br><span class="line">y = x**<span class="number">2</span> <span class="comment"># 如果对在GPU上的数据进行运算，那么结果还是存放在GPU上</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>
<p>需要注意的是，存储在不同位置中的数据是不可以直接进行计算的。即存放在CPU上的数据不可以直接与存放在GPU上的数据进行运算，位于不同GPU上的数据也是不能直接进行计算的。</p>
<h3 id="模型的GPU计算"><a href="#模型的GPU计算" class="headerlink" title="模型的GPU计算"></a>模型的GPU计算</h3><p>同Tensor类似，PyTorch模型也可以通过.cuda转换到GPU上。我们可以通过检查模型的参数的device属性来查看存放模型的设备。<br>同时，需要保证模型输入的Tensor和模型都在同一设备上，否则会报错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Linear(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">list</span>(net.parameters())[<span class="number">0</span>].device</span><br><span class="line">net.cuda() <span class="comment"># 将模型转换到CPU上</span></span><br><span class="line"><span class="built_in">list</span>(net.parameters())[<span class="number">0</span>].device</span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>).cuda()</span><br><span class="line">net(x)</span><br></pre></td></tr></table></figure>

<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>卷积神经网络中涉及到输入和输出图像的形状的转换，这里将后面模型构造时用到的一段通用测试代码摘出来，供以后参考：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = vgg(conv_arch, fc_features, fc_hidden_units)</span><br><span class="line"><span class="comment"># 构造一个高和宽均为224的单通道数据样本来观察每一层的输出形状</span></span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="comment"># named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)</span></span><br><span class="line"><span class="keyword">for</span> name, blk <span class="keyword">in</span> net.named_children(): </span><br><span class="line">    X = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(name, <span class="string">&#x27;output shape: &#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>

<h2 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h2><p>卷积神经网络（convolutional neural network）是含有卷积层（convolutional layer）的神经网络。<br>虽然卷积层得名于卷积（convolution）运算，但我们通常在卷积层中使用更加直观的互相关（cross-correlation）运算。<br>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。即，可以通过数据来学习卷积核。<br>实际上，卷积运算与互相关运算类似。为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。<br>那么，你也许会好奇卷积层为何能使用互相关运算替代卷积运算。其实，在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。<br>为了与大多数深度学习文献一致，如无特别说明，本书中提到的卷积运算均指互相关运算。<br>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做X的感受野（receptive field）。<br>我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。<br>比如，输入层是一个3乘3的图像，记为X，经过一个2乘2的卷积核，得到的输出层是一个2乘2的图像，记为Y。那么Y中每个元素的感受野是X中的2乘2的范围大小，即这个元素仅与X中的这四个元素相关。此时考虑一个更深的卷积网络：将Y与另一个形状为2乘2的卷积核做互相关运算，输出单个元素z，那么，z在Y上的感受野包括Y的全部四个元素，则在输入X上的感受野包括其中全部的9个元素（X的这9个元素是由Y的四个元素所感受的）</p>
<h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>卷积层的输出形状由输入形状和卷积核窗口形状决定。卷积层的两个超参数，即填充和步幅，它们可以对给定形状的输入和卷积核改变输出形状。<br>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）。<br>卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。</p>
<h2 id="多输入通道和多输出通道"><a href="#多输入通道和多输出通道" class="headerlink" title="多输入通道和多输出通道"></a>多输入通道和多输出通道</h2><p>（1）多输入通道<br>当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。<br>即，卷积核的通道数由输入数据的通道数所决定。<br>（2）多输出通道<br>当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。<br>如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为c乘以h乘以w的卷积核，将它们在输出通道上进行连结。<br>即，输出数据的通道数由卷积核的个数所决定。<br>（3）1乘1卷积层<br>1乘1卷积层通常用来调整网络层之间的通道数（可以类比于全连接层的隐藏神经元个数），并控制模型复杂度。<br>因为使用了最小窗口，1乘1卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，1乘1卷积的主要计算发生在通道维上。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。<br>假设将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1乘1卷积层的作用与全连接层等价。但它又相比于全连接层有一个优点：它仍然保留了输入图像的空间信息，即不是一个很长的向量，从而使空间信息能够自然传递到后面的层中去。</p>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化（pooling）层的提出是为了缓解卷积层对位置的过度敏感性。<br>不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。<br>同卷积层一样，池化层也可以在输入的高和宽两侧的填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。<br>默认情况下，PyTorch里的池化层的步幅和池化窗口形状相同。<br>当然，我们也可以指定非正方形的池化窗口，并分别指定高和宽上的填充和步幅。<br>在处理多通道输入数据时，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加。这意味着池化层的输出通道数与输入通道数相等。</p>
<h2 id="卷积神经网络（LeNet）"><a href="#卷积神经网络（LeNet）" class="headerlink" title="卷积神经网络（LeNet）"></a>卷积神经网络（LeNet）</h2><p>在之前对Fashion-MNIST数据集分类时，使用的方法是对图像中的像素全部展开得到一个很长的向量，然后输入进全连接层中。<br>然而，这种分类方法有一定的局限性。<br>（1）图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。<br>（2）对于大尺寸的输入图像，使用全连接层容易造成模型过大。这带来过复杂的模型和过高的存储开销。<br>卷积层尝试解决这两个问题。一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</p>
<h3 id="定义模型-3"><a href="#定义模型-3" class="headerlink" title="定义模型"></a>定义模型</h3><p>LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="comment"># 卷积操作的维度变化公式为:</span></span><br><span class="line"><span class="comment"># Height_out = (Height_in - Height_kernal + 2*padding) / stride +1 </span></span><br><span class="line"><span class="comment"># LeNet当时的输入图片是单通道的32*32像素的灰度图</span></span><br><span class="line"><span class="comment"># 但现在的Fashion-MNIST是28*28</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># (输入通道，输出通道，卷积核尺寸)，所以输出尺寸为(28-5)+1=24，通道为6</span></span><br><span class="line">            nn.Sigmoid(), <span class="comment"># Sigmoid激活</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># (卷积核尺寸，步幅)，所以输出尺寸为(24-2)/2+1=12，通道仍然为6</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>), <span class="comment"># 所以输出尺寸为(12-5)+1=8，通道为16</span></span><br><span class="line">            nn.Sigmoid(), <span class="comment"># Sigmoid激活</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>) <span class="comment"># 所以输出尺寸为(8-2)/2+1=4，通道仍然为16</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>), <span class="comment"># 输入为16*4*4，输出为120个神经元</span></span><br><span class="line">            nn.Sigmoid(), <span class="comment"># Sigmoid激活</span></span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>), <span class="comment"># 又一个全连接层</span></span><br><span class="line">            nn.Sigmoid(), <span class="comment"># Sigmoid激活</span></span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>) <span class="comment"># 输出层，类别为10</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, img</span>):</span></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>)) <span class="comment"># view一下形状，第一维为batch_size，剩下的就是图像转换成的向量</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">net = LeNet()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<h3 id="获取数据和训练模型"><a href="#获取数据和训练模型" class="headerlink" title="获取数据和训练模型"></a>获取数据和训练模型</h3><p>（1）还是使用之前的数据下载和加载方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;.&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;.&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">num_workers = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">train_iter = torch.utils.data.DataLoader(mnist_train, batch_size = batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(mnist_test, batch_size = batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br></pre></td></tr></table></figure>
<p>（2）修改分类准确度计算代码，使其支持GPU计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">data_iter, net, device=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        <span class="comment"># 如果没指定device，则使用net的device</span></span><br><span class="line">        device = <span class="built_in">list</span>(net.parameters())[<span class="number">0</span>].device</span><br><span class="line"></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">                net.<span class="built_in">eval</span>() <span class="comment"># 评估模式，这会关闭dropout</span></span><br><span class="line">                acc_sum += (net(X.to(device)).argmax(dim=<span class="number">1</span>)==y.to(device)).<span class="built_in">float</span>().<span class="built_in">sum</span>().cpu().item()</span><br><span class="line">                net.train() <span class="comment"># 改回训练模式</span></span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br></pre></td></tr></table></figure>
<p>（3）修改训练过程，使其支持GPU计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch5</span>(<span class="params">net, train_iter, test_iter, batch_size, optimizer, device, num_epochs</span>):</span></span><br><span class="line">    net = net.to(device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    loss = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n, start = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            train_l_sum += l.cpu().item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().cpu().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec&#x27;</span> </span><br><span class="line">              % (epoch+<span class="number">1</span>, train_l_sum/n, train_acc_sum/n, test_acc, time.time()-start))</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>

<h2 id="深度卷积神经网络（AlexNet）"><a href="#深度卷积神经网络（AlexNet）" class="headerlink" title="深度卷积神经网络（AlexNet）"></a>深度卷积神经网络（AlexNet）</h2><p>我们在上一节看到，神经网络可以直接基于图像的原始像素进行分类。这种称为端到端（end-to-end）的方法节省了很多中间步骤。然而，在很长一段时间里更流行的是研究者通过勤劳与智慧所设计并生成的手工特征。这类图像分类研究的主要流程是：<br>获取图像数据集；<br>使用已有的特征提取函数生成图像的特征；<br>使用机器学习模型对图像的特征分类。<br>当时认为的机器学习部分仅限最后这一步。如果那时候跟机器学习研究者交谈，他们会认为机器学习既重要又优美。优雅的定理证明了许多分类器的性质。机器学习领域生机勃勃、严谨而且极其有用。然而，如果跟计算机视觉研究者交谈，则是另外一幅景象。他们会告诉你图像识别里“不可告人”的现实是：计算机视觉流程中真正重要的是数据和特征。也就是说，使用较干净的数据集和较有效的特征甚至比机器学习模型的选择对图像分类结果的影响更大。</p>
<h3 id="学习特征表示"><a href="#学习特征表示" class="headerlink" title="学习特征表示"></a>学习特征表示</h3><p>既然特征如此重要，它该如何表示呢？<br>我们已经提到，在相当长的时间里，特征都是基于各式各样手工设计的函数从数据中提取的。事实上，不少研究者通过提出新的特征提取函数不断改进图像分类结果。这一度为计算机视觉的发展做出了重要贡献。</p>
<p>然而，另一些研究者则持异议。他们认为特征本身也应该由学习得来。他们还相信，为了表征足够复杂的输入，特征本身应该分级表示。持这一想法的研究者相信，多层神经网络可能可以学得数据的多级表征，并逐级表示越来越抽象的概念或模式。以图像分类为例，以物体边缘检测为例。在多层神经网络中，图像的第一级的表示可以是在特定的位置和⻆度是否出现边缘；而第二级的表示说不定能够将这些边缘组合出有趣的模式，如花纹；在第三级的表示中，也许上一级的花纹能进一步汇合成对应物体特定部位的模式。这样逐级表示下去，最终，模型能够较容易根据最后一级的表示完成分类任务。需要强调的是，输入的逐级表示由多层模型中的参数决定，而这些参数都是学出来的。</p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>2012年，AlexNet横空出世。AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。<br>AlexNet跟LeNet结构类似，但使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。<br>两者具体对比如下：<br>第一，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。<br>AlexNet第一层中的卷积窗口形状是11×11。因为ImageNet中绝大多数图像的高和宽均比MNIST图像的高和宽大10倍以上，ImageNet图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到5×5，之后全采用3×3。此外，第一、第二和第五个卷积层之后都使用了窗口形状为3×3、步幅为2的最大池化层。而且，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数数十倍。<br>紧接着最后一个卷积层的是两个输出个数为4096的全连接层。这两个巨大的全连接层带来将近1 GB的模型参数。由于早期显存的限制，最早的AlexNet使用双数据流的设计使一个GPU只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。<br>第二，AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。<br>第三，AlexNet通过丢弃法来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。<br>第四，AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</p>
<p>虽然看上去AlexNet的实现比LeNet的实现也就多了几行代码而已，但这个观念上的转变和真正优秀实验结果的产生令学术界付出了很多年。<br>（1）定义模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># AlexNet所使用的数据集是ImageNet</span></span><br><span class="line"><span class="comment"># 这里使用的输入图像是单通道的尺寸为224*224</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            <span class="comment"># 使用较大的11 x 11窗口来捕获物体。同时使用步幅4来较大幅度减小输出高和宽</span></span><br><span class="line">            <span class="comment"># 这里使用的输出通道数比LeNet中的也要大很多</span></span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># (输入通道，输出通道，卷积核尺寸，步幅，填充)，所以输出尺寸为(224-11)/4+1=54 这里不能整除，所以向下取整</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># 所以输出尺寸为(54-3)/2+1=26，通道数仍为96，池化层是默认向下取整，可以改变ceil_mode参数来改成向上取整</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，这是因为(x-5+2*2)/1+1=x</span></span><br><span class="line">            <span class="comment"># 且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>), <span class="comment"># 输出为(26-5+2*2)/1+1=26</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># (26-3)/2+1 = 12</span></span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># 输出为(12-3+2*1)/1+1=12</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># 12</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># 12</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>) <span class="comment"># (12-3)/2+1=5</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, img</span>):</span></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">net = AlexNet()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<p>（2）读取数据<br>虽然论文中AlexNet使用ImageNet数据集，但因为ImageNet数据集训练时间较长，我们仍用前面的Fashion-MNIST数据集来演示AlexNet。读取数据的时候我们额外做了一步将图像高和宽扩大到AlexNet使用的图像高和宽224。这个可以通过torchvision.transforms.Resize实例来实现。也就是说，我们在ToTensor实例前使用Resize实例，然后使用Compose实例来将这两个变换串联以方便调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将读取数据的步骤封装成一个函数，方便调用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span>, root=<span class="string">&#x27;.&#x27;</span></span>):</span></span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.append(torchvision.transforms.Resize(size=resize))</span><br><span class="line"></span><br><span class="line">    trans.append(torchvision.transforms.ToTensor())</span><br><span class="line">    transform = torchvision.transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;.&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;.&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size = batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size = batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br></pre></td></tr></table></figure>

<p>（3）训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分类准确度测量函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">data_iter, net, device=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        <span class="comment"># 如果没指定device，则使用net的device</span></span><br><span class="line">        device = <span class="built_in">list</span>(net.parameters())[<span class="number">0</span>].device</span><br><span class="line"></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">                net.<span class="built_in">eval</span>() <span class="comment"># 评估模式，这会关闭dropout</span></span><br><span class="line">                acc_sum += (net(X.to(device)).argmax(dim=<span class="number">1</span>)==y.to(device)).<span class="built_in">float</span>().<span class="built_in">sum</span>().cpu().item()</span><br><span class="line">                net.train() <span class="comment"># 改回训练模式</span></span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将训练过程封装起来，方便调用</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch5</span>(<span class="params">net, train_iter, test_iter, batch_size, optimizer, device, num_epochs</span>):</span></span><br><span class="line">    net = net.to(device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n, start = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            train_l_sum += l.cpu().item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().cpu().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec&#x27;</span> </span><br><span class="line">              % (epoch+<span class="number">1</span>, train_l_sum/n, train_acc_sum/n, test_acc, time.time()-start))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br><span class="line"><span class="comment"># 相对于LeNet，由于图片尺寸变大了而且模型变大了，所以需要更大的显存，也需要更长的训练时间了。</span></span><br></pre></td></tr></table></figure>

<h2 id="使用重复元素的网络（VGG）"><a href="#使用重复元素的网络（VGG）" class="headerlink" title="使用重复元素的网络（VGG）"></a>使用重复元素的网络（VGG）</h2><p>AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整。虽然AlexNet指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则以指导后来的研究者如何设计新的网络。</p>
<p>接下来会介绍几种不同的深度网络设计思路。</p>
<p>本节介绍VGG，它的名字来源于论文作者所在的实验室Visual Geometry Group。VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。</p>
<h3 id="VGG块"><a href="#VGG块" class="headerlink" title="VGG块"></a>VGG块</h3><p>VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为3乘3的卷积层后接上一个步幅为2、窗口形状为2乘2的最大池化层。卷积层保持输入的高和宽不变（因为(h-3+2x1）+1=h），而池化层则对其减半（因为(h-2)/2+1=h/2）。<br>对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。例如，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">    blk.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br></pre></td></tr></table></figure>

<h3 id="VGG网络"><a href="#VGG网络" class="headerlink" title="VGG网络"></a>VGG网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这个自定义层是将(n, c, h, w)拉伸成(n, c*h*w)，即将图像转成向量</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FlattenLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 整个vgg网络前面是若干个卷积块，后面是3个全连接层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg</span>(<span class="params">conv_arch, fc_features, fc_hidden_units=<span class="number">4096</span></span>):</span></span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="comment"># 整个网络的卷积层部分</span></span><br><span class="line">    <span class="comment"># conv_arch参数包含了上面的vgg块的三个参数：块的数目、输入通道、输出通道</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, (num_convs, in_channels, out_channels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(conv_arch):</span><br><span class="line">        net.add_module(<span class="string">&#x27;vgg_block_&#x27;</span> + <span class="built_in">str</span>(i+<span class="number">1</span>), vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整个网络的全连接层部分</span></span><br><span class="line">    net.add_module(<span class="string">&quot;fc&quot;</span>, nn.Sequential(</span><br><span class="line">        FlattenLayer(),</span><br><span class="line">        <span class="comment"># fc_features就是前面经过卷积操作后图像的尺寸c*h*w, fc_hidden_units是隐藏层的神经元个数</span></span><br><span class="line">        nn.Linear(fc_features, fc_hidden_units),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(fc_hidden_units, fc_hidden_units),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(fc_hidden_units, <span class="number">10</span>)</span><br><span class="line">    ))</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"><span class="comment"># 有5个卷积块，前2块是单卷积层，后3块是双卷积层，即连续做两次卷积</span></span><br><span class="line"><span class="comment"># 经过5个vgg_block，宽和高会减半5次，变成224/(2^5)=224/32=7</span></span><br><span class="line"><span class="comment"># 同时，通道数也在翻倍，起始是1个通道，之后不断翻倍，直到512个通道</span></span><br><span class="line"><span class="comment"># 因为这个网络使用了8个卷积层和3个全连接层，所以经常被称为VGG-11。 </span></span><br><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">64</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">128</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">256</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 经过上面的卷积操作后，输出通道为512，图像尺寸为7，所以输入到下面的全连接层的向量就是512*7*7</span></span><br><span class="line">fc_features = <span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span></span><br><span class="line"><span class="comment"># 全连接层的隐藏层的神经元个数，这个可以任意设定</span></span><br><span class="line">fc_hidden_units = <span class="number">4096</span></span><br><span class="line"><span class="comment"># 构造网络</span></span><br><span class="line">net = vgg(conv_arch, fc_features, fc_hidden_units)</span><br></pre></td></tr></table></figure>
<p>模型加载和训练过程与上一节的AlexNet相同。</p>
<h2 id="网络中的网络（NiN）"><a href="#网络中的网络（NiN）" class="headerlink" title="网络中的网络（NiN）"></a>网络中的网络（NiN）</h2><p>前几节介绍的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。<br>网络中的网络（NiN）则提出了另外一个思路，即重复使用由卷积层和代替全连接层的1乘1卷积层构成的NiN块来构建深层网络。<br>NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。<br>NiN的以上设计思想影响了后面一系列卷积神经网络的设计。</p>
<p>加粗！！：NiN因为使用了全局平均池化层，从而使得每个通道的宽和高都为1，这样就使得输出与输入图片的尺寸无关，极大地提高了模型的灵活性，而之前的LeNet、AlexNet和VGG必须要给定特定尺寸的图片才能运行，否则会与全连接层的输入不匹配。</p>
<h3 id="NiN块"><a href="#NiN块" class="headerlink" title="NiN块"></a>NiN块</h3><p>卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。而1乘1卷积层可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用1乘1卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NiN块是NiN中的基础块。</span></span><br><span class="line"><span class="comment"># 它由一个卷积层加两个充当全连接层的1×11×1卷积层串联而成。</span></span><br><span class="line"><span class="comment"># 其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, stride, padding</span>):</span></span><br><span class="line">    blk = nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU()</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>

<h3 id="NiN网络"><a href="#NiN网络" class="headerlink" title="NiN网络"></a>NiN网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalAvgPool2d</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现</span></span><br><span class="line">    <span class="comment"># 这样就可以将输入的图像平均池化成一个1*1大小的元素</span></span><br><span class="line">    <span class="comment"># 再配合上下面的FlattenLayer，就起到了最后全连接输出的效果</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.avg_pool2d(x, kernel_size = x.size()[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个自定义层是将(n, c, h, w)拉伸成(n, c*h*w)，即将图像转成向量</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FlattenLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># NiN是在AlexNet问世不久后提出的。它们的卷积层设定有类似之处。</span></span><br><span class="line">    <span class="comment"># NiN使用卷积窗口形状分别为11×1111×11、5×55×5和3×33×3的卷积层，相应的输出通道数也与AlexNet中的一致。</span></span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">0</span>), <span class="comment"># 输出为(224-11)/4+1=54</span></span><br><span class="line">    <span class="comment"># 每个NiN块后接一个步幅为2、窗口形状为3×33×3的最大池化层</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), <span class="comment"># 输出为(54-3)/2+1=26 </span></span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>), <span class="comment"># 输出为(26-5+2*2)/1+1=26</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), <span class="comment"># 输出为(26-3)/2+1=12</span></span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>), <span class="comment"># (12-3+2*1)/1+1 = 12</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), <span class="comment"># (12-3)/2+1=5</span></span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>), <span class="comment"># (5-3+2*1)/1+1=5</span></span><br><span class="line">    GlobalAvgPool2d(), <span class="comment"># (5-5)/1+1=1</span></span><br><span class="line">    FlattenLayer()</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<h2 id="含并行连结的网络（GoogLeNet）"><a href="#含并行连结的网络（GoogLeNet）" class="headerlink" title="含并行连结的网络（GoogLeNet）"></a>含并行连结的网络（GoogLeNet）</h2><p>在2014年的ImageNet图像识别挑战赛中，一个名叫GoogLeNet的网络结构大放异彩。它虽然在名字上向LeNet致敬，但在网络结构上已经很难看到LeNet的影子。GoogLeNet吸收了NiN中网络串联网络的思想，并在此基础上做了很大改进。</p>
<h3 id="Inception块"><a href="#Inception块" class="headerlink" title="Inception块"></a>Inception块</h3><p>GoogLeNet中的基础卷积块叫作Inception块，得名于同名电影《盗梦空间》（Inception）。其结构如图所示：<br><img src="https://user-images.githubusercontent.com/6218739/75971838-4e3c2700-5f0d-11ea-8d34-cbe8e187f07b.png" alt="image"></p>
<p>可以看出，Inception块里有4条并行的线路。前3条线路使用窗口大小分别是1乘1、3乘3和5乘5的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做1乘1卷积来减少输入通道数，以降低模型复杂度。第四条线路则使用3乘3最大池化层，后接1乘1卷积层来改变通道数。4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。<br>其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inception块中可以自定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># c1 - c4为每条线路里的层的输出通道数</span></span><br><span class="line">    <span class="comment"># c2和c3的内部因为都有两个层，所以输出通道也要有两个，来分别设定</span></span><br><span class="line">    <span class="comment"># c4的最大池化层不需要通道设定，所以c4的通道也只有一个即可</span></span><br><span class="line">    <span class="comment"># 假设输入图像的尺寸为h，经过下面的计算可得，输出图像的尺寸仍为h</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_c, c1, c2, c3, c4</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 线路1，单1 x 1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=<span class="number">1</span>) <span class="comment"># (h-1)+1=h</span></span><br><span class="line">        <span class="comment"># 线路2，1 x 1卷积层后接3 x 3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_c, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>) <span class="comment"># (h-1)+1=h</span></span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) <span class="comment"># (h-3+2*1)+1=h</span></span><br><span class="line">        <span class="comment"># 线路3，1 x 1卷积层后接5 x 5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_c, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>) <span class="comment"># (h-1)+1=h</span></span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>) <span class="comment"># (h-5+2*2)+1=h</span></span><br><span class="line">        <span class="comment"># 线路4，3 x 3最大池化层后接1 x 1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>) <span class="comment"># (h-3+2*1)/1+1=h</span></span><br><span class="line">        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=<span class="number">1</span>) <span class="comment"># (h-1)+1=h</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x)))) <span class="comment"># 注意这里输入的也是x</span></span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x)))) <span class="comment"># 注意这里输入的也是x</span></span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>) <span class="comment"># 在通道维上进行连结</span></span><br></pre></td></tr></table></figure>

<h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GoogLeNet跟VGG一样，在主体卷积部分中使用5个模块（block），每个模块之间使用步幅为2的3×3最大池化层来减小输出高宽。</span></span><br><span class="line"><span class="comment"># 图片尺寸就以文中给出的96为例</span></span><br><span class="line"><span class="comment"># 第一模块使用一个64通道的7×77×7卷积层，输入通道为1，输出通道为64</span></span><br><span class="line">b1 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>), <span class="comment"># (96-7+2*3)/2+1=48</span></span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>) <span class="comment"># (48-3+2*1)/2+1=24</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 第二模块使用2个卷积层和1个池化层，输入通道为64，输出通道为192</span></span><br><span class="line">b2 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>), <span class="comment"># (24-1)+1=24</span></span><br><span class="line">    nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), <span class="comment">#(24-3+2*1)+1=24</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>) <span class="comment"># (24-3+2*1)/2+1=12</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 第三模块串联2个完整的Inception块</span></span><br><span class="line">b3 = nn.Sequential(</span><br><span class="line">    <span class="comment"># 第1个Inception块的输入通道为192，输出通道为64+128+32+32=256</span></span><br><span class="line">    Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>), <span class="comment"># 12</span></span><br><span class="line">    <span class="comment"># 第1个Inception块的输入通道为256，输出通道为128+192+96+64=480</span></span><br><span class="line">    Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>), <span class="comment"># 12</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>) <span class="comment"># (12-3+2*1)/2+1=6</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 第四模块串联5个Inception块</span></span><br><span class="line">b4 = nn.Sequential(</span><br><span class="line">    <span class="comment"># 第1个Inception块的输入通道为480， 输出通道为192+208+48+64=512</span></span><br><span class="line">    Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>), <span class="comment"># 6</span></span><br><span class="line">    <span class="comment"># 输入512， 输出160+224+64+64=512</span></span><br><span class="line">    Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>), <span class="comment"># 6</span></span><br><span class="line">    <span class="comment"># 输入512， 输出128+256+64+64=512</span></span><br><span class="line">    Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>), <span class="comment"># 6</span></span><br><span class="line">    <span class="comment"># 输入512， 输出112+288+64+64=528</span></span><br><span class="line">    Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>), <span class="comment"># 6</span></span><br><span class="line">    <span class="comment"># 输入528， 输出256+320+128+128=832</span></span><br><span class="line">    Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>), <span class="comment"># 6</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>) <span class="comment"># (6-3+2*1)/2+1=3 </span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 第五模块串联2个Inception块</span></span><br><span class="line">b5 = nn.Sequential(</span><br><span class="line">    <span class="comment"># 输入832， 输出256+320+128+128=832</span></span><br><span class="line">    Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>), <span class="comment"># 3</span></span><br><span class="line">    <span class="comment"># 输入832， 输出384+384+128+128=1024</span></span><br><span class="line">    Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>), <span class="comment"># 3</span></span><br><span class="line">    <span class="comment"># 这一步非常重要，使用全局平均池化层来将每个通道的高和宽变成1，这样就与输入图像的尺寸无关</span></span><br><span class="line">    GlobalAvgPool2d() <span class="comment"># 1</span></span><br><span class="line">)</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    b1, b2, b3, b4, b5,</span><br><span class="line">    FlattenLayer(), <span class="comment"># (N, 1024, 1, 1)转化成向量(N, 1024)</span></span><br><span class="line">    nn.Linear(<span class="number">1024</span>, <span class="number">10</span>) <span class="comment"># 全连接层输出类别，这里的1024是之前的通道数，与图像尺寸无关</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>加载数据时，这里的尺寸改成了96，也可以继续用之前的224，GoogLeNet因为使用了全局平均池化，所以对输入图片尺寸不敏感：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br></pre></td></tr></table></figure>
<h2 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h2><p>在之前的例子里，我们对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。<br>通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。<br>批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。<br>批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路。<br>对全连接层和卷积层做批量归一化的方法稍有不同。<br>（1）对全连接层做批量归一化：将批量归一化层置于全连接层中的仿射变换和激活函数之间，批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数$\gamma$和偏移（shift）参数$\beta$。<br>（2）对卷积层做批量归一化：对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。<br>（3）预测时的批量归一化：使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。</p>
<p>Pytorch中nn模块定义的BatchNorm1d和BatchNorm2d类使用起来非常简单，二者分别用于全连接层和卷积层，都需要指定输入的num_features参数值，对于全连接层来说该值应为输出个数，对于卷积层来说则为输出通道数。</p>
<h2 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h2><p>让我们先思考一个问题：对神经网络模型添加新的层，充分训练后的模型是否只可能更有效地降低训练误差？理论上，原模型解的空间只是新模型解的空间的子空间。也就是说，如果我们能将新添加的层训练成恒等映射f(x)=x，新模型和原模型将同样有效。由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。然而在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。针对这一问题，何恺明等人提出了残差网络（ResNet）。它在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。<br>残差块通过跨层的数据通道从而能够训练出有效的深度神经网络。</p>
<h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><p>在残差块中，输入可通过跨层的数据线路更快地向前传播。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 残差块可以设定输出通道数、是否使用额外的1×11×1卷积层来修改通道数以及卷积层的步幅，即可以改变输出大小。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, use_1x1conv=<span class="literal">False</span>, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># ResNet沿用了VGG全3×33×3卷积层的设计。</span></span><br><span class="line">        <span class="comment"># 残差块里首先有2个有相同输出通道数的3×33×3卷积层。</span></span><br><span class="line">        <span class="comment"># 每个卷积层后接一个批量归一化层和ReLU激活函数。</span></span><br><span class="line">        <span class="comment"># 然后将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。</span></span><br><span class="line">        <span class="comment"># 这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=stride)</span><br><span class="line">        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channels)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=stride)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y + X)</span><br></pre></td></tr></table></figure>
<h3 id="ResNet模型"><a href="#ResNet模型" class="headerlink" title="ResNet模型"></a>ResNet模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ResNet的前两层跟GoogLeNet中的一样：在输出通道数为64、步幅为2的7×7卷积层后接步幅为2的3×3的最大池化层。</span></span><br><span class="line"><span class="comment"># 不同之处在于ResNet每个卷积层后增加的批量归一化层。</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"><span class="comment"># GoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_block</span>(<span class="params">in_channels, out_channels, num_residuals, first_block=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> first_block:</span><br><span class="line">        <span class="comment"># 第一个模块的通道数同输入通道数一致</span></span><br><span class="line">        <span class="keyword">assert</span> in_channels == out_channels</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            <span class="comment"># 后面的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</span></span><br><span class="line">            blk.append(Residual(in_channels, out_channels, use_1x1conv=<span class="literal">True</span>, stride=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 在第一个模块中，由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。</span></span><br><span class="line">            blk.append(Residual(out_channels, out_channels))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"><span class="comment"># 为ResNet加入所有残差块。这里每个模块使用两个残差块。</span></span><br><span class="line">net.add_module(<span class="string">&#x27;resnet_block1&#x27;</span>, resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">net.add_module(<span class="string">&#x27;resnet_block2&#x27;</span>, resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">&#x27;resnet_block3&#x27;</span>, resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">&#x27;resnet_block4&#x27;</span>, resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 最后，与GoogLeNet一样，加入全局平均池化层后接上全连接层输出。</span></span><br><span class="line">net.add_module(<span class="string">&#x27;global_avg_pool&#x27;</span>, GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, 512, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">&#x27;fc&#x27;</span>, nn.Sequential(FlattenLayer(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>)))</span><br><span class="line"><span class="comment"># 这里每个模块里有4个卷积层（不计算1×11×1卷积层），加上最开始的卷积层和最后的全连接层，共计18层。这个模型通常也被称为ResNet-18。</span></span><br><span class="line"><span class="comment"># 通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152。</span></span><br><span class="line"><span class="comment"># 虽然ResNet的主体架构跟GoogLeNet的类似，但ResNet结构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。</span></span><br></pre></td></tr></table></figure>

<p>拿个测试数据跑一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> name, layer <span class="keyword">in</span> net.named_children():</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(name, <span class="string">&#x27;output shape = &#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>

<p>最后，加载数据和训练模型都跟之前的一样。</p>
<h2 id="稠密连接网络（DenseNet）"><a href="#稠密连接网络（DenseNet）" class="headerlink" title="稠密连接网络（DenseNet）"></a>稠密连接网络（DenseNet）</h2><p>ResNet中的跨层连接设计引申出了数个后续工作，比如这里的稠密连接网络（DenseNet）。<br>假设将部分前后相邻的运算抽象为模块A和模块B。与ResNet的主要区别在于，DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是在通道维上连结。这样模块A的输出可以直接传入模块B后面的层。在这个设计里，模块A直接跟模块B后面的所有层连接在了一起。这也是它被称为“稠密连接”的原因。<br>DenseNet的主要构建模块是稠密块（dense block）和过渡层（transition layer）。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。</p>
<h3 id="稠密块"><a href="#稠密块" class="headerlink" title="稠密块"></a>稠密块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DenseNet使用了ResNet改良版的“批量归一化、激活和卷积”结构，首先在conv_block函数里实现这个结构。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span>(<span class="params">in_channels, out_channels</span>):</span></span><br><span class="line">    blk = nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(in_channels),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"><span class="comment"># 稠密块由多个conv_block组成，每块使用相同的输出通道数。</span></span><br><span class="line"><span class="comment"># 卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）</span></span><br><span class="line"><span class="comment"># 比如，定义一个有2个输出通道数为10的卷积块。使用通道数为3的输入时，我们会得到通道数为3+2×10=23的输出，增长率就是10</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_convs, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        net = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">            in_c = in_channels + i* out_channels</span><br><span class="line">            net.append(conv_block(in_c, out_channels))</span><br><span class="line"></span><br><span class="line">        self.net = nn.ModuleList(net)</span><br><span class="line">        self.out_channels = in_channels + num_convs * out_channels <span class="comment"># 计算输出通道数</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>) <span class="comment"># 在通道维上将输入和输出连结</span></span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<h3 id="过渡层"><a href="#过渡层" class="headerlink" title="过渡层"></a>过渡层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。</span></span><br><span class="line"><span class="comment"># 过渡层用来控制模型复杂度。它通过1×11×1卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span>(<span class="params">in_channels, out_channels</span>):</span></span><br><span class="line">    blk = nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(in_channels),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>

<h3 id="DenseNet模型"><a href="#DenseNet模型" class="headerlink" title="DenseNet模型"></a>DenseNet模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DenseNet首先使用同ResNet一样的单卷积层和最大池化层。</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 类似于ResNet接下来使用的4个残差块，DenseNet使用的是4个稠密块。</span></span><br><span class="line"><span class="comment"># 同ResNet一样，可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与上一节的ResNet-18保持一致。</span></span><br><span class="line"><span class="comment"># 稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。</span></span><br><span class="line">num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span> <span class="comment"># num_channels为当前的通道数</span></span><br><span class="line">num_convs_in_dense_blocks = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>] <span class="comment"># length为4，表明有4个稠密块，每个元素都是4，表明每个稠密块有4个卷积层</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, num_convs <span class="keyword">in</span> <span class="built_in">enumerate</span>(num_convs_in_dense_blocks):</span><br><span class="line">    DB = DenseBlock(num_convs, num_channels, growth_rate)</span><br><span class="line">    net.add_module(<span class="string">&#x27;DenseBlock_%d&#x27;</span> % i, DB)</span><br><span class="line">    <span class="comment"># 上一个稠密块的输出通道，这里用的是类.属性的用法</span></span><br><span class="line">    num_channels = DB.out_channels</span><br><span class="line">    <span class="comment"># 在稠密块之间加入通道数减半的过渡层</span></span><br><span class="line">    <span class="keyword">if</span> i != <span class="built_in">len</span>(num_convs_in_dense_blocks) - <span class="number">1</span>:</span><br><span class="line">        net.add_module(<span class="string">&#x27;transition_block_%d&#x27;</span> % i, transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">        num_channels = num_channels // <span class="number">2</span></span><br><span class="line">net.add_module(<span class="string">&#x27;BN&#x27;</span>, nn.BatchNorm2d(num_channels))</span><br><span class="line">net.add_module(<span class="string">&#x27;relu&#x27;</span>, nn.ReLU())</span><br><span class="line"><span class="comment"># 与ResNet一样，最后加入全局平均池化层后接上全连接层输出。</span></span><br><span class="line">net.add_module(<span class="string">&#x27;global_avg_pool&#x27;</span>, GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, num_channels, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">&#x27;fc&#x27;</span>, nn.Sequential(FlattenLayer(), nn.Linear(num_channels, <span class="number">10</span>)))</span><br></pre></td></tr></table></figure>
<p>最后，加载数据和训练模型都跟之前的一样。</p>
<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>本章先略过</p>
<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="优化与深度学习"><a href="#优化与深度学习" class="headerlink" title="优化与深度学习"></a>优化与深度学习</h2><p>虽然优化为深度学习提供了最小化损失函数的方法，但本质上，优化与深度学习的目标是有区别的。<br>由于优化算法的目标函数通常是一个基于训练数据集的损失函数，优化的目标在于降低训练误差。 而深度学习的目标在于降低泛化误差。为了降低泛化误差，除了使用优化算法降低训练误差以外，还需要注意应对过拟合。<br>在一个深度学习问题中，我们通常会预先定义一个损失函数。有了损失函数以后，我们就可以使用优化算法试图将其最小化。在优化中，这样的损失函数通常被称作优化问题的目标函数（objective function）。依据惯例，优化算法通常只考虑最小化目标函数。其实，任何最大化问题都可以很容易地转化为最小化问题，只需令目标函数的相反数为新的目标函数即可。<br>优化在深度学习中有很多挑战，比如局部最小值和鞍点。<br>（1）深度学习模型的目标函数可能有若干局部最优值。当一个优化问题的数值解在局部最优解附近时，由于目标函数有关解的梯度接近或变成零，最终迭代求得的数值解可能只令目标函数局部最小化而非全局最小化。<br>（2）梯度接近或变成零可能是由于当前解在局部最优解附近造成的。事实上，另一种可能性是当前解在鞍点（saddle point）附近。比如在鞍点位置，目标函数在x轴方向上是局部最小值，但在y轴方向上是局部最大值。<br>假设一个函数的输入为k维向量，输出为标量，那么它的海森矩阵（Hessian matrix）有k个特征值。该函数在梯度为0的位置上可能是局部最小值、局部最大值或者鞍点。<br>当函数的海森矩阵在梯度为零的位置上的特征值全为正时，该函数得到局部最小值。<br>当函数的海森矩阵在梯度为零的位置上的特征值全为负时，该函数得到局部最大值。<br>当函数的海森矩阵在梯度为零的位置上的特征值有正有负时，该函数得到鞍点。<br>随机矩阵理论告诉我们，对于一个大的高斯随机矩阵来说，任一特征值是正或者是负的概率都是0.5。那么，以上第一种情况的概率为 0.5的k次方。由于深度学习模型参数通常都是高维的（k很大），目标函数的鞍点通常比局部最小值更常见。</p>
<h2 id="梯度下降和随机梯度下降"><a href="#梯度下降和随机梯度下降" class="headerlink" title="梯度下降和随机梯度下降"></a>梯度下降和随机梯度下降</h2><p>下图中的公式清晰地解释了为什么梯度下降能降低目标函数的数值：<br><img src="https://user-images.githubusercontent.com/6218739/76134337-5bb1f800-6058-11ea-908d-a03ebaa1a44f.png" alt="image"><br>在深度学习里，目标函数通常是训练数据集中有关各个样本的损失函数的平均。当训练数据样本数很大时，梯度下降每次迭代的计算开销很高。<br>随机梯度下降（stochastic gradient descent，SGD）减少了每次迭代的计算开销。</p>
<h2 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h2><p>在每一次迭代中，梯度下降使用整个训练数据集来计算梯度，因此它有时也被称为批量梯度下降（batch gradient descent）。而随机梯度下降在每次迭代中只随机采样一个样本来计算梯度。我们还可以在每轮迭代中随机均匀采样多个样本来组成一个小批量，然后使用这个小批量来计算梯度。<br>在实际中，（小批量）随机梯度下降的学习率可以在迭代过程中自我衰减。<br>当批量大小为1时，小批量随机梯度下降算法即为随机梯度下降；当批量大小等于训练数据样本数时，该算法即为梯度下降。当批量较小时，每次迭代中使用的样本少，这会导致并行处理和内存使用效率变低。这使得在计算同样数目样本的情况下比使用更大批量时所花时间更多。当批量较大时，每个小批量梯度里可能含有更多的冗余信息。为了得到较好的解，批量较大时比批量较小时需要计算的样本数目可能更多，例如增大迭代周期数。</p>
<h2 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h2><p>目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。举个例子：同一位置上，假设目标函数在竖直方向比在水平方向的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。<br>动量法的提出是为了解决梯度下降的上述问题。<br>（1）动量法使用了指数加权移动平均的思想。它将过去时间步的梯度做了加权平均，且权重按时间步指数衰减。<br>（2）动量法使得相邻时间步的自变量更新在方向上更加一致。<br>在PyTorch中，只需要通过参数momentum来指定动量超参数即可使用动量法。</p>
<h2 id="AdaGrad算法"><a href="#AdaGrad算法" class="headerlink" title="AdaGrad算法"></a>AdaGrad算法</h2><p>在之前介绍过的优化算法中，目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。<br>动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。而AdaGrad算法根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。<br>AdaGrad算法在迭代过程中不断调整学习率，并让目标函数自变量中每个元素都分别拥有自己的学习率。<br>使用AdaGrad算法时，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。<br>通过名称为Adagrad的优化器方法，我们便可使用PyTorch提供的AdaGrad算法来训练模型。</p>
<h2 id="RMSProp算法"><a href="#RMSProp算法" class="headerlink" title="RMSProp算法"></a>RMSProp算法</h2><p>RMSProp算法和AdaGrad算法的不同在于，RMSProp算法使用了小批量随机梯度按元素平方的指数加权移动平均来调整学习率。<br>通过名称为RMSprop的优化器方法，我们便可使用PyTorch提供的RMSProp算法来训练模型。注意，超参数$\gamma$通过alpha指定。</p>
<h2 id="AdaDelta算法"><a href="#AdaDelta算法" class="headerlink" title="AdaDelta算法"></a>AdaDelta算法</h2><p>除了RMSProp算法以外，另一个常用优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有用解的问题做了改进。有意思的是，AdaDelta算法没有学习率这一超参数。AdaDelta算法没有学习率超参数，它通过使用有关自变量更新量平方的指数加权移动平均的项来替代RMSProp算法中的学习率。<br>通过名称为Adadelta的优化器方法，我们便可使用PyTorch提供的AdaDelta算法。它的超参数可以通过rho来指定。</p>
<h2 id="Adam算法"><a href="#Adam算法" class="headerlink" title="Adam算法"></a>Adam算法</h2><p>Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。所以Adam算法可以看做是RMSProp算法与动量法的结合。<br>Adam算法使用了偏差修正。<br>通过名称为“Adam”的优化器实例，我们便可使用PyTorch提供的Adam算法。</p>
<h1 id="计算性能"><a href="#计算性能" class="headerlink" title="计算性能"></a>计算性能</h1><h2 id="命令式编程和符号式编程"><a href="#命令式编程和符号式编程" class="headerlink" title="命令式编程和符号式编程"></a>命令式编程和符号式编程</h2><p>（1）命令式编程更方便。当我们在Python里使用命令式编程时，大部分代码编写起来都很直观。同时，命令式编程更容易调试。这是因为我们可以很方便地获取并打印所有的中间变量值，或者使用Python的调试工具。<br>（2）符号式编程更高效并更容易移植。一方面，在编译的时候系统容易做更多优化（因为在编译时系统能够完整地获取整个程序）；另一方面，符号式编程可以将程序变成一个与Python无关的格式，从而可以使程序在非Python环境下运行，以避开Python解释器的性能问题。<br>截止目前（2020年3月），PyTorch仅采用了命令式编程方式。</p>
<h2 id="异步计算"><a href="#异步计算" class="headerlink" title="异步计算"></a>异步计算</h2><p>以下一段是唐树森同学对PyTorch官网上的翻译。<br>默认情况下，PyTorch中的 GPU 操作是异步的。当调用一个使用 GPU 的函数时，这些操作会在特定的设备上排队但不一定会在稍后立即执行。这就使我们可以并行更多的计算，包括 CPU 或其他 GPU 上的操作。 一般情况下，异步计算的效果对调用者是不可见的，因为（1）每个设备按照它们排队的顺序执行操作，（2）在 CPU 和 GPU 之间或两个 GPU 之间复制数据时，PyTorch会自动执行必要的同步操作。因此，计算将按每个操作同步执行的方式进行。 可以通过设置环境变量CUDA_LAUNCH_BLOCKING = 1来强制进行同步计算。当 GPU 产生error时，这可能非常有用。（异步执行时，只有在实际执行操作之后才会报告此类错误，因此堆栈跟踪不会显示请求的位置。）</p>
<h2 id="自动并行计算"><a href="#自动并行计算" class="headerlink" title="自动并行计算"></a>自动并行计算</h2><p>PyTorch能有效地实现在不同设备上（比如两块GPU）自动并行计算。</p>
<h2 id="多GPU计算"><a href="#多GPU计算" class="headerlink" title="多GPU计算"></a>多GPU计算</h2><p>因为目前手头只有一块GPU，所以本节没法实战，故略过。<br>需要注意单主机多GPU计算与分布式计算的区别。</p>
<h1 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h1><h2 id="图像增广"><a href="#图像增广" class="headerlink" title="图像增广"></a>图像增广</h2><p>这个地方有几点提前注意：<br>（1）下面介绍的都是torchvision自带的函数，关于图像增广可以借助更专业的第三方库（比如可以将标注一块增广），比如：<br><a target="_blank" rel="noopener" href="https://github.com/aleju/imgaug">Image augmentation for machine learning experiments</a><br>（2）为了在预测时得到确定的结果，我们通常只将图像增广应用在训练样本上，而不在预测时使用含随机操作的图像增广。此外，在实际PyTorch应用时，注意使用ToTensor将小批量图像转成PyTorch需要的格式，即形状为(批量大小, 通道数, 高, 宽)、值域在0到1之间且类型为32位浮点数。</p>
<p>图像增广（image augmentation）技术通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。图像增广的另一种解释是，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。例如，我们可以对图像进行不同方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性。我们也可以调整亮度、色彩等因素来降低模型对色彩的敏感度。可以说，在当年AlexNet的成功中，图像增广技术功不可没。</p>
<p>这部分涉及PIL、skimage、OpenCV、PyTorch Tensor，这四个有类似的地方，也有很多小区别，比如：<br>（1）PIL、skimage、OpenCV的图像通道都是h乘w乘c，即高乘宽乘通道，而PyTorch的ToTensor自己会转化为c乘h乘w，同时转为float后除以255（这里一定注意，如果ToTensor接收的是numpy的array，一定保证它是uint8格式，否则可能仅是通道顺序变化，而数值没有除以255）<br>（2）PIL的数据类型是Image对象，skimage和OpenCV都是numpy。</p>
<p>torchvision.transforms模块有大量现成的转换方法，不过需要注意的是有的方法输入的是PIL图像，如Resize；有的方法输入的是tensor，如Normalize；而还有的是用于二者转换，如ToTensor将PIL图像转换成tensor。一定要注意这点，使用时看清文档。</p>
<p>具体可以参考如下文章：<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/52344534">OpenCV，PIL，Skimage你pick谁</a><br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/dd08418c306f">opencv-PIL-matplotlib-Skimage-Pytorch图片读取区别与联系</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27382990">pytorch图像基本操作</a></p>
<h3 id="常用的图像增广方法"><a href="#常用的图像增广方法" class="headerlink" title="常用的图像增广方法"></a>常用的图像增广方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里使用PIL读取图像</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;drive/My Drive/cat1.jpg&#x27;</span>)</span><br><span class="line"><span class="comment"># 翻转</span></span><br><span class="line"><span class="comment"># 左右翻转图像通常不改变物体的类别。它是最早也是最广泛使用的一种图像增广方法。</span></span><br><span class="line">horizon_flip = transforms.RandomHorizontalFlip()</span><br><span class="line"><span class="comment"># 上下翻转不如左右翻转通用。但是至少对于样例图像，上下翻转不会造成识别障碍。</span></span><br><span class="line">vertical_flip = transforms.RandomVerticalFlip()</span><br><span class="line"><span class="comment"># 裁剪</span></span><br><span class="line"><span class="comment"># 可以通过对图像随机裁剪来让物体以不同的比例出现在图像的不同位置</span></span><br><span class="line"><span class="comment"># 这同样能够降低模型对目标位置的敏感性。</span></span><br><span class="line"><span class="comment"># 每次随机裁剪出一块面积为原面积10%∼100%的区域，且该区域的宽和高之比随机取自0.5∼2，</span></span><br><span class="line"><span class="comment"># 然后再将该区域的宽和高分别缩放到200像素。</span></span><br><span class="line">shape_aug = transforms.RandomResizedCrop(<span class="number">200</span>, scale=(<span class="number">0.1</span>, <span class="number">1</span>), ratio=(<span class="number">0.5</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 变化颜色</span></span><br><span class="line"><span class="comment"># 可以从4个方面改变图像的颜色：亮度（brightness）、对比度（contrast）、饱和度（saturation）和色调（hue）。</span></span><br><span class="line"><span class="comment"># 将图像的亮度随机变化为原图亮度的50%（1−0.5）∼150%（1+0.5）</span></span><br><span class="line">brightness_change = transforms.ColorJitter(brightness=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># 随机变化图像的色调</span></span><br><span class="line">hue_change = transforms.ColorJitter(hue=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># 随机变化图像的对比度</span></span><br><span class="line">contrast_change = transforms.ColorJitter(contrast=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># 综合设置颜色变化</span></span><br><span class="line">color_aug = transforms.ColorJitter(brightness=<span class="number">0.5</span>, contrast=<span class="number">0.5</span>, saturation=<span class="number">0.5</span>, hue=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># 叠加多个图像增广方法</span></span><br><span class="line"><span class="comment"># 实际应用中我们会将多个图像增广方法叠加使用。</span></span><br><span class="line"><span class="comment"># 我们可以通过Compose实例将上面定义的多个图像增广方法叠加起来，再应用到每张图像之上。</span></span><br><span class="line">compose = transforms.Compose([</span><br><span class="line">                              transforms.RandomHorizontalFlip(),</span><br><span class="line">                              color_aug,</span><br><span class="line">                              shape_aug</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 大部分图像增广方法都有一定的随机性。</span></span><br><span class="line"><span class="comment"># 为了方便观察图像增广的效果，需要多次运行转换函数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img_trans = horizon_flip(img)</span><br><span class="line">    img_trans = vertical_flip(img)</span><br><span class="line">    img_trans = shape_aug(img)</span><br><span class="line">    img_trans = brightness_change(img)</span><br><span class="line">    img_trans = hue_change(img)</span><br><span class="line">    img_trans = contrast_change(img)</span><br><span class="line">    img_trans = color_aug(img)</span><br><span class="line">    img_trans = compose(img)</span><br><span class="line">    <span class="comment"># 存储时使用skimage，借此看看skimage与PIL的转换</span></span><br><span class="line">    io.imsave(np.<span class="built_in">str</span>(i)+<span class="string">&#x27;.jpg&#x27;</span>, np.array(img_trans))</span><br></pre></td></tr></table></figure>

<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><p>迁移学习是解决小数据集的一个有效方法。<br>本节介绍迁移学习中的一种常用技术：微调（fine tuning）。微调由以下4步构成。<br>在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。<br>创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。<br>为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。<br>在目标数据集上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。<br>当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。</p>
<p>注: 在使用预训练模型时，一定要和预训练时作同样的预处理。 如果你使用的是torchvision的models，那就要求: All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. 如果你使用的是pretrained-models.pytorch仓库，请务必阅读其README，其中说明了如何预处理。</p>
<p>接下来我们来实践一个具体的例子：热狗识别。我们将基于一个小数据集对在ImageNet数据集上训练好的ResNet模型进行微调。该小数据集含有数千张包含热狗和不包含热狗的图像。我们将使用微调得到的模型来识别一张图像中是否包含热狗。<br>导入必要的包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> ImageFolder</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="获取数据集-1"><a href="#获取数据集-1" class="headerlink" title="获取数据集"></a>获取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载数据集</span></span><br><span class="line">!wget https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/hotdog.<span class="built_in">zip</span></span><br><span class="line"><span class="comment"># 解压，得到两个文件夹hotdog/train和hotdog/test。</span></span><br><span class="line"><span class="comment"># 这两个文件夹下面均有hotdog和not-hotdog两个类别文件夹，每个类别文件夹里面是图像文件。</span></span><br><span class="line">!unzip hotdog.<span class="built_in">zip</span></span><br><span class="line"><span class="comment"># 查看一下数据</span></span><br><span class="line"><span class="comment"># 关于ImageFolder的用法可以参考下面的链接：</span></span><br><span class="line"><span class="comment"># https://discuss.pytorch.org/t/questions-about-imagefolder/774/3</span></span><br><span class="line"><span class="comment"># https://blog.csdn.net/TH_NUM/article/details/80877435</span></span><br><span class="line"><span class="built_in">print</span>(train_imgs.classes) <span class="comment"># ImageFolder假设所有的文件按文件夹保存好，每个文件夹下面存贮同一类别的图片，文件夹的名字为分类的名字。</span></span><br><span class="line"><span class="built_in">print</span>(train_imgs.class_to_idx) <span class="comment"># 字符串类别所对应的数值类别</span></span><br><span class="line">train_imgs[<span class="number">0</span>][<span class="number">0</span>] <span class="comment"># 前面的是正类图像，即热狗</span></span><br><span class="line">train_imgs[-<span class="number">1</span>][<span class="number">0</span>] <span class="comment"># 后面的是负类图像，即非热狗</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定RGB三个通道的均值和方差来将图像通道归一化 (每个数值减去该通道所有数值的平均值，再除以该通道所有数值的标准差作为输出)</span></span><br><span class="line"><span class="comment"># 这个地方一定与预训练模型所做的处理保持一致！！</span></span><br><span class="line">normalize = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"><span class="comment"># 训练集所做的预处理</span></span><br><span class="line">train_augs = transforms.Compose([</span><br><span class="line">                                 <span class="comment"># 先从图像中裁剪出随机大小和随机高宽比的一块随机区域，然后将该区域缩放为高和宽均为224像素的输入</span></span><br><span class="line">                                 transforms.RandomResizedCrop(size=<span class="number">224</span>),</span><br><span class="line">                                 <span class="comment"># 左右翻转</span></span><br><span class="line">                                 transforms.RandomHorizontalFlip(),</span><br><span class="line">                                 <span class="comment"># 转为PyTorch所需的形状为(批量大小, 通道数, 高, 宽)、值域在0到1之间且类型为32位浮点数的数据</span></span><br><span class="line">                                 transforms.ToTensor(),</span><br><span class="line">                                 <span class="comment"># 归一化</span></span><br><span class="line">                                 normalize</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 测试集所做的预处理</span></span><br><span class="line">test_augs = transforms.Compose([</span><br><span class="line">                                <span class="comment"># 将图像的高和宽均缩放为256像素</span></span><br><span class="line">                                transforms.Resize(size=<span class="number">256</span>),</span><br><span class="line">                                <span class="comment"># 然后从中裁剪出高和宽均为224像素的中心区域作为输入</span></span><br><span class="line">                                transforms.CenterCrop(size=<span class="number">224</span>),</span><br><span class="line">                                transforms.ToTensor(),</span><br><span class="line">                                normalize</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<h3 id="定义和初始化模型-1"><a href="#定义和初始化模型-1" class="headerlink" title="定义和初始化模型"></a>定义和初始化模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用在ImageNet数据集上预训练的ResNet-18作为源模型。</span></span><br><span class="line"><span class="comment"># 这里指定pretrained=True来自动下载并加载预训练的模型参数。在第一次使用时需要联网下载模型参数。</span></span><br><span class="line"><span class="comment"># 不管你是使用的torchvision的models还是pretrained-models.pytorch仓库，默认都会将预训练好的模型参数下载到你的home目录下.torch文件夹。</span></span><br><span class="line"><span class="comment"># 你可以通过修改环境变量$TORCH_MODEL_ZOO来更改下载目录。</span></span><br><span class="line"><span class="comment"># 另一个比较常用的方法是，在其源码中找到下载地址直接浏览器输入地址下载，下载好后将其放到环境变量$TORCH_MODEL_ZOO所指文件夹即可，这样比较快。</span></span><br><span class="line">pretrained_net = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面打印源模型的成员变量fc。</span></span><br><span class="line"><span class="comment"># 作为一个全连接层，它将ResNet最终的全局平均池化层输出变换成ImageNet数据集上1000类的输出。</span></span><br><span class="line"><span class="comment"># 如果你使用的是其他模型，那可能没有成员变量fc（比如models中的VGG预训练模型），</span></span><br><span class="line"><span class="comment"># 所以正确做法是查看对应模型源码中其定义部分，这样既不会出错也能加深我们对模型的理解。</span></span><br><span class="line"><span class="built_in">print</span>(pretrained_net.fc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里应该将最后的fc成修改我们需要的输出类别数:</span></span><br><span class="line">pretrained_net.fc = nn.Linear(<span class="number">512</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(pretrained_net.fc)</span><br><span class="line"><span class="comment"># 此时fc层中的参数已经被初始化了，但是其他层依然保存着预训练得到的参数。</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(pretrained_net.fc.parameters()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于是在很大的ImageNet数据集上预训练的，所以非fc层的参数已经足够好，因此一般只需使用较小的学习率来微调这些参数</span></span><br><span class="line"><span class="comment"># 而fc中的随机初始化参数一般需要更大的学习率从头训练。</span></span><br><span class="line"><span class="comment"># PyTorch可以方便的对模型的不同部分设置不同的学习参数</span></span><br><span class="line"><span class="comment"># 将fc层的参数的id取出</span></span><br><span class="line">output_params = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, pretrained_net.fc.parameters()))</span><br><span class="line"><span class="comment"># 从整个net的所有参数中剔除fc的参数，保留非fc中的参数放入feature_params中</span></span><br><span class="line">feature_params = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: <span class="built_in">id</span>(p) <span class="keyword">not</span> <span class="keyword">in</span> output_params, pretrained_net.parameters())</span><br><span class="line">lr = <span class="number">0.01</span> <span class="comment"># 默认的学习率设为0.01</span></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">                       &#123;<span class="string">&#x27;params&#x27;</span>: feature_params&#125;, <span class="comment"># 非fc层的参数使用默认的学习率，即外层的学习率</span></span><br><span class="line">                       &#123;<span class="string">&#x27;params&#x27;</span>: pretrained_net.fc.parameters(), <span class="string">&#x27;lr&#x27;</span>: lr*<span class="number">10</span>&#125; <span class="comment"># fc层参数的学习率设为已训练过的部分的10倍</span></span><br><span class="line">                       ],</span><br><span class="line">                      lr=lr,</span><br><span class="line">                      weight_decay=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>

<h3 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先定义一个统一的训练过程函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">data_iter, net, device=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        <span class="comment"># 如果没指定device，则使用net的device</span></span><br><span class="line">        device = <span class="built_in">list</span>(net.parameters())[<span class="number">0</span>].device</span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">                net.<span class="built_in">eval</span>() <span class="comment"># 评估模式，这会关闭dropout</span></span><br><span class="line">                acc_sum += (net(X.to(device)).argmax(dim=<span class="number">1</span>)==y.to(device)).<span class="built_in">float</span>().<span class="built_in">sum</span>().cpu().item()</span><br><span class="line">                net.train() <span class="comment"># 改回训练模式</span></span><br><span class="line"></span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">train_iter, test_iter, net, loss, optimizer, device, num_epochs</span>):</span></span><br><span class="line">    net = net.to(device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;training on &quot;</span>, device)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n, start = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            train_l_sum += l.cpu().item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().cpu().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec&#x27;</span> </span><br><span class="line">              % (epoch+<span class="number">1</span>, train_l_sum/n, train_acc_sum/n, test_acc, time.time()-start))</span><br><span class="line"><span class="comment"># 再定义一个使用微调的训练函数train_fine_tuning以便多次调用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_fine_tuning</span>(<span class="params">net, optimizer, batch_size=<span class="number">128</span>, num_epochs=<span class="number">5</span></span>):</span></span><br><span class="line">    train_iter = DataLoader(ImageFolder(<span class="string">&#x27;hotdog/train&#x27;</span>, transform=train_augs), batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_iter = DataLoader(ImageFolder(<span class="string">&#x27;hotdog/test&#x27;</span>, transform=test_augs), batch_size)</span><br><span class="line">    loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">    train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据前面的设置，我们将以10倍的学习率从头训练目标模型的输出层参数</span></span><br><span class="line">train_fine_tuning(pretrained_net, optimizer)</span><br></pre></td></tr></table></figure>

<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><p>很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置。在计算机视觉里，我们将这类任务称为目标检测（object detection）或物体检测。<br>在目标检测里，我们通常使用边界框（bounding box）来描述目标位置。边界框是一个矩形框，可以由矩形左上角的x和y轴坐标与右下角的x和y轴坐标确定。<br>目标检测相关知识暂时略过，包括下面的锚框、目标检测数据集、SSD、区域卷积神经网络R-CNN系列（R-CNN、Fast R-CNN、Faster R-CNN、Mask R-CNN）。<br>值得一提的是，Mask R-CNN在Faster R-CNN的基础上做了修改。Mask R-CNN将兴趣区域池化层替换成了兴趣区域对齐层，即通过双线性插值（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测。兴趣区域对齐层的输出包含了所有兴趣区域的形状相同的特征图。它们既用来预测兴趣区域的类别和边界框，又通过额外的全卷积网络预测目标的像素级位置。</p>
<h2 id="语义分割和数据集"><a href="#语义分割和数据集" class="headerlink" title="语义分割和数据集"></a>语义分割和数据集</h2><p>在目标检测问题中，我们一直使用方形边界框来标注和预测图像中的目标。而语义分割（semantic segmentation）问题，它关注如何将图像分割成属于不同语义类别的区域。值得一提的是，这些语义区域的标注和预测都是像素级的。与目标检测相比，语义分割标注的像素级的边框显然更加精细。</p>
<h3 id="图像分割和实例分割"><a href="#图像分割和实例分割" class="headerlink" title="图像分割和实例分割"></a>图像分割和实例分割</h3><p>计算机视觉领域还有2个与语义分割相似的重要问题，即图像分割（image segmentation）和实例分割（instance segmentation）。在这里将它们与语义分割简单区分一下。<br>（1）图像分割将图像分割成若干组成区域。这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。比如，图像分割后不知道分割出来的东西是什么，并不知道哪个是狗，哪个是猫。<br>（2）实例分割又叫同时检测并分割（simultaneous detection and segmentation）。它研究如何识别图像中各个目标实例的像素级区域。与语义分割有所不同，实例分割不仅需要区分语义，还要区分不同的目标实例。如果图像中有两只狗，实例分割需要区分像素属于这两只狗中的哪一只。</p>
<h3 id="Pascal-VOC2012语义分割数据集"><a href="#Pascal-VOC2012语义分割数据集" class="headerlink" title="Pascal VOC2012语义分割数据集"></a>Pascal VOC2012语义分割数据集</h3><p>语义分割的一个重要数据集叫作Pascal VOC2012。<br>（1）下载并读取数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载数据集</span></span><br><span class="line">!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-<span class="number">2012.</span>tar</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_voc_images</span>(<span class="params">root=<span class="string">&quot;drive/My Drive/VOCdevkit/VOC2012&quot;</span>, is_train=<span class="literal">True</span>, max_num=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># ImageSets/Segmentation路径包含了指定训练和测试样本的文本文件</span></span><br><span class="line">    txt_fname = <span class="string">&#x27;%s/ImageSets/Segmentation/%s&#x27;</span> % (root, <span class="string">&#x27;train.txt&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;val.txt&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(txt_fname, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        images = f.read().split()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> max_num <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        images = images[:<span class="built_in">min</span>(max_num, <span class="built_in">len</span>(images))]</span><br><span class="line">    features, labels = [<span class="literal">None</span>]*<span class="built_in">len</span>(images), [<span class="literal">None</span>]*<span class="built_in">len</span>(images)</span><br><span class="line">    <span class="keyword">for</span> i, fname <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        <span class="comment"># JPEGImages和SegmentationClass路径下分别包含了样本的输入图像和标签</span></span><br><span class="line">        features[i] = Image.<span class="built_in">open</span>(<span class="string">&#x27;%s/JPEGImages/%s.jpg&#x27;</span> % (root, fname)).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">        <span class="comment"># 这里的标签也是图像格式，其尺寸和它所标注的输入图像的尺寸相同。标签中颜色相同的像素属于同一个语义类别。</span></span><br><span class="line">        labels[i] = Image.<span class="built_in">open</span>(<span class="string">&#x27;%s/SegmentationClass/%s.png&#x27;</span> % (root, fname)).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line">voc_dir = <span class="string">&#x27;drive/My Drive/VOCdevkit/VOC2012&#x27;</span></span><br><span class="line">train_features, train_labels = read_voc_images(voc_dir, max_num=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在标签图像中，白色和黑色分别代表边框和背景，而其他不同的颜色则对应不同的类别。</span></span><br><span class="line"><span class="comment"># 接下来，我们列出标签中每个RGB颜色的值及其标注的类别。</span></span><br><span class="line">VOC_COLORMAP = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">192</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">192</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">128</span>]]</span><br><span class="line"></span><br><span class="line">VOC_CLASSES = [<span class="string">&#x27;background&#x27;</span>, <span class="string">&#x27;aeroplane&#x27;</span>, <span class="string">&#x27;bicycle&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;boat&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;bottle&#x27;</span>, <span class="string">&#x27;bus&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;chair&#x27;</span>, <span class="string">&#x27;cow&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;diningtable&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;motorbike&#x27;</span>, <span class="string">&#x27;person&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;potted plant&#x27;</span>, <span class="string">&#x27;sheep&#x27;</span>, <span class="string">&#x27;sofa&#x27;</span>, <span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;tv/monitor&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个256*256*256长度的tensor</span></span><br><span class="line">colormap2label = torch.zeros(<span class="number">256</span>**<span class="number">3</span>, dtype=torch.uint8)</span><br><span class="line"><span class="keyword">for</span> i, colormap <span class="keyword">in</span> <span class="built_in">enumerate</span>(VOC_COLORMAP):</span><br><span class="line">    <span class="comment"># 将颜色索引与类别索引一一对应起来</span></span><br><span class="line">    <span class="comment"># 注意colormap2label是一个一维向量，所以不同通道的颜色值传入后要乘以256</span></span><br><span class="line">    <span class="comment"># 具体的数值大小无意义，只要是不同类别能分辨开即可，同时与下面的</span></span><br><span class="line">    <span class="comment"># (colormap[:, :, 0]*256+colormap[:, :, 1])*256+colormap[:,:,2]要对应起来</span></span><br><span class="line">    colormap2label[(colormap[<span class="number">0</span>]*<span class="number">256</span>+colormap[<span class="number">1</span>])*<span class="number">256</span>+colormap[<span class="number">2</span>]] = i</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">voc_label_indices</span>(<span class="params">colormap, colormap2label</span>):</span></span><br><span class="line">    <span class="comment"># 将PIL Image转成numpy，然后数据类型改为int32位，colormap仍然是h*w*c的样式</span></span><br><span class="line">    colormap = np.array(colormap.convert(<span class="string">&#x27;RGB&#x27;</span>)).astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将不同的通道乘以256转化成索引值</span></span><br><span class="line">    <span class="comment"># 这样，idx的shape就是h*w</span></span><br><span class="line">    idx = ((colormap[:, :, <span class="number">0</span>]*<span class="number">256</span>+colormap[:, :, <span class="number">1</span>])*<span class="number">256</span>+colormap[:,:,<span class="number">2</span>])</span><br><span class="line">    <span class="comment"># 找到该索引矩阵所对应的标签类别</span></span><br><span class="line">    <span class="comment"># 因为idx是索引矩阵，其大小与图像大小相同，那么返回的也正是每个像素所对应的类别，即与图像同样大小的类别矩阵</span></span><br><span class="line">    <span class="keyword">return</span> colormap2label[idx]</span><br></pre></td></tr></table></figure>
<p>（2）预处理数据<br>在之前的章节中，我们通过缩放图像使其符合模型的输入形状。然而在语义分割里，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射难以做到精确，尤其在不同语义的分割区域。为了避免这个问题，我们将图像裁剪成固定尺寸而不是缩放。具体来说，我们使用图像增广里的随机裁剪，并对输入图像和标签裁剪相同区域。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">voc_rand_crop</span>(<span class="params">feature, label, height, width</span>):</span></span><br><span class="line">    i, j, h, w = torchvision.transforms.RandomCrop.get_params(feature, output_size=(height, width))</span><br><span class="line">    feature = torchvision.transforms.functional.crop(feature, i, j, h, w)</span><br><span class="line">    label = torchvision.transforms.functional.crop(label, i, j, h, w)</span><br><span class="line">    <span class="keyword">return</span> feature, label</span><br><span class="line"></span><br><span class="line"><span class="comment"># 比如随机裁剪200*300大小的区域</span></span><br><span class="line">img = voc_rand_crop(train_features[<span class="number">0</span>], train_labels[<span class="number">0</span>], <span class="number">200</span>, <span class="number">300</span>)</span><br></pre></td></tr></table></figure>
<p>（3）自定义语义分割数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VOCSegDataset</span>(<span class="params">torch.utils.data.Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, is_train, crop_size, voc_dir, colormap2label, max_num=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 对输入图像的RGB三个通道的值分别做标准化</span></span><br><span class="line">        self.rgb_mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">        self.rgb_std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">        self.tsf = torchvision.transforms.Compose([</span><br><span class="line">                                                   torchvision.transforms.ToTensor(),</span><br><span class="line">                                                   torchvision.transforms.Normalize(</span><br><span class="line">                                                       mean=self.rgb_mean,</span><br><span class="line">                                                       std=self.rgb_std)</span><br><span class="line">        ])</span><br><span class="line">        self.crop_size = crop_size</span><br><span class="line">        features, labels = read_voc_images(</span><br><span class="line">            root = voc_dir,</span><br><span class="line">            is_train = is_train,</span><br><span class="line">            max_num = max_num</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本需要通过自定义的filter函数所移除。</span></span><br><span class="line">        self.features = self.<span class="built_in">filter</span>(features)</span><br><span class="line">        self.labels = self.<span class="built_in">filter</span>(labels)</span><br><span class="line">        self.colormap2label = colormap2label</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(self.features)) + <span class="string">&#x27; valid samples&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这个地方需要特别注意，PIL.size返回的是(width, height)，即宽在前，高在后，而我们输入的参数是高在前</span></span><br><span class="line">    <span class="comment"># https://liam.page/2015/04/22/pil-tutorial-basic-usage/</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">filter</span>(<span class="params">self, imgs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [img <span class="keyword">for</span> img <span class="keyword">in</span> imgs <span class="keyword">if</span> (</span><br><span class="line">            img.size[<span class="number">1</span>] &gt;= self.crop_size[<span class="number">0</span>] <span class="keyword">and</span> </span><br><span class="line">            img.size[<span class="number">0</span>] &gt;= self.crop_size[<span class="number">1</span>]</span><br><span class="line">        )]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过实现__getitem__函数，我们可以任意访问数据集中索引为idx的输入图像及其每个像素的类别索引</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        feature, label = voc_rand_crop(self.features[idx], self.labels[idx], *self.crop_size)</span><br><span class="line">        <span class="keyword">return</span> (self.tsf(feature), <span class="comment"># feature是float32的tensor</span></span><br><span class="line">                voc_label_indices(label, self.colormap2label) <span class="comment"># label是uint8的tensor</span></span><br><span class="line">                )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.features)</span><br><span class="line"></span><br><span class="line">crop_size = (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">max_num = <span class="number">100</span></span><br><span class="line">voc_train = VOCSegDataset(<span class="literal">True</span>, crop_size, voc_dir, colormap2label, max_num)</span><br><span class="line">voc_test = VOCSegDataset(<span class="literal">False</span>, crop_size, voc_dir, colormap2label, max_num)</span><br><span class="line"><span class="comment"># 设批量大小为64，分别定义训练集和测试集的迭代器。</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">num_workers = <span class="number">4</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                              drop_last=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line"></span><br><span class="line">test_iter = torch.utils.data.DataLoader(voc_test, batch_size, drop_last=<span class="literal">True</span>,</span><br><span class="line">                             num_workers=num_workers)</span><br></pre></td></tr></table></figure>

<h2 id="全卷积网络"><a href="#全卷积网络" class="headerlink" title="全卷积网络"></a>全卷积网络</h2><p>全卷积网络（fully convolutional network，FCN）采用卷积神经网络实现了从图像像素到像素类别的变换。与之前介绍的卷积神经网络有所不同，全卷积网络通过转置卷积（transposed convolution）层将中间层特征图的高和宽变换回输入图像的尺寸，从而令预测结果与输入图像在空间维（高和宽）上一一对应：给定空间维上的位置，通道维的输出即该位置对应像素的类别预测。<br>全卷积网络先使用卷积神经网络抽取图像特征，然后通过 1乘1 卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸，从而输出每个像素的类别。<br>在全卷积网络中，可以将转置卷积层初始化为双线性插值的上采样。</p>
<h1 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h1><p>暂时略过。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2020/02/29/kaggle-nuclei/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Be interesting!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="亓欣波">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/29/kaggle-nuclei/" class="post-title-link" itemprop="url">Kaggle细胞赛：基于PyTorch/UNet算法的细胞核识别</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-29 00:00:00" itemprop="dateCreated datePublished" datetime="2020-02-29T00:00:00+08:00">2020-02-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-25 15:20:30" itemprop="dateModified" datetime="2021-03-25T15:20:30+08:00">2021-03-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">programming</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/02/29/kaggle-nuclei/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/02/29/kaggle-nuclei/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>从2015年开始，Kaggle每年都举办一次Data Science Bowl，旨在召集众多力量开发算法，来解决当前某一特定领域的迫切问题。2018年的数据碗的任务是识别细胞的细胞核nuclei，从而使得更加方便地进行药物测试，使得新药的上市时间缩短。</p>
<p>Yun Chen分享了他的使用PyTorch/UNet算法的notebook，见<a target="_blank" rel="noopener" href="https://www.kaggle.com/cloudfall/pytorch-tutorials-on-dsb2018">这里</a>，本文是对该notebook代码的详细解析和再现，并适当做了一些修改。</p>
<p>再分享一篇挺好的背景文章：<br><a target="_blank" rel="noopener" href="https://www.zybuluo.com/Team/note/1205894">基于深度学习的图像语义分割算法综述</a></p>
<h1 id="数据集分析"><a href="#数据集分析" class="headerlink" title="数据集分析"></a>数据集分析</h1><h2 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions download -c data-science-bowl-<span class="number">2018</span></span><br></pre></td></tr></table></figure>
<h2 id="解压并迁移数据"><a href="#解压并迁移数据" class="headerlink" title="解压并迁移数据"></a>解压并迁移数据</h2><p>解压数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">!unzip stage1_sample_submission.csv.<span class="built_in">zip</span></span><br><span class="line">!unzip stage1_solution.csv.<span class="built_in">zip</span></span><br><span class="line">!unzip stage1_train_labels.csv.<span class="built_in">zip</span></span><br><span class="line">!unzip stage2_sample_submission_final.csv.<span class="built_in">zip</span></span><br><span class="line"> </span><br><span class="line">!mkdir stage1_test</span><br><span class="line">!unzip stage1_test.<span class="built_in">zip</span> -d stage1_test</span><br><span class="line"></span><br><span class="line">!mkdir stage1_train</span><br><span class="line">!unzip stage1_train.<span class="built_in">zip</span> -d stage1_train</span><br><span class="line"></span><br><span class="line">!mkdir stage2_test_final</span><br><span class="line">!unzip stage2_test_final.<span class="built_in">zip</span> -d stage2_test_final</span><br></pre></td></tr></table></figure>

<p>迁移数据到Google Drive：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">!mkdir nuclei</span><br><span class="line">!mv stage1_test nuclei</span><br><span class="line">!mv stage1_train nuclei</span><br><span class="line">!mv stage2_test_final/ nuclei</span><br><span class="line">!mv *.csv nuclei</span><br><span class="line">!mv nuclei /content/drive/My\ Drive</span><br></pre></td></tr></table></figure>
<p>这样就做到了持久化，防止notebook重启时数据丢失。</p>
<h2 id="数据文件描述"><a href="#数据文件描述" class="headerlink" title="数据文件描述"></a>数据文件描述</h2><p>（1）/stage1_train/*：该文件夹是训练集，包含训练用的图像及其掩膜图像<br>（2）/stage1_test/*：该文件夹是测试集，仅包含图像<br>（3）/stage2_test/*：这是第二阶段的测试集，仅包含图像<br>（4）stage1_sample_submission.csv：在第一阶段需要提交的文件格式<br>（5）stage2_sample_submission.csv：在第二阶段需要提交的文件格式<br>（6）stage1_train_labels.csv：该文件是训练集中的掩膜图像的游程编码RLE</p>
<h1 id="加载必要的Python包"><a href="#加载必要的Python包" class="headerlink" title="加载必要的Python包"></a>加载必要的Python包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line">TRAIN_PATH = <span class="string">&#x27;./train.pth&#x27;</span></span><br><span class="line">TEST_PATH = <span class="string">&#x27;./test.tph&#x27;</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<h1 id="数据集加载"><a href="#数据集加载" class="headerlink" title="数据集加载"></a>数据集加载</h1><p>数据集加载是至关重要的一步，也是非常繁琐的一步。因为这一步无法标准化，必须针对特定的数据集进行解析。</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>下面是对该竞赛的数据集的处理方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span>(<span class="params">file_path, has_mask=<span class="literal">True</span></span>):</span></span><br><span class="line">  file_path = Path(file_path)</span><br><span class="line">  files = <span class="built_in">sorted</span>(<span class="built_in">list</span>(file_path.iterdir()))</span><br><span class="line">  datas = []</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> tqdm(files):</span><br><span class="line">    item = &#123;&#125;</span><br><span class="line">    imgs = []</span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> (file/<span class="string">&#x27;images&#x27;</span>).iterdir():</span><br><span class="line">      img = io.imread(image)</span><br><span class="line">      imgs.append(img)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(imgs) == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> img.shape[<span class="number">2</span>] &gt; <span class="number">3</span>:</span><br><span class="line">      <span class="keyword">assert</span> (img[:, :, <span class="number">3</span>]!=<span class="number">255</span>).<span class="built_in">sum</span>() == <span class="number">0</span></span><br><span class="line">    img = img[:, :, :<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> has_mask:</span><br><span class="line">      mask_files = <span class="built_in">list</span>((file/<span class="string">&#x27;masks&#x27;</span>).iterdir())</span><br><span class="line">      masks = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> i, mask <span class="keyword">in</span> <span class="built_in">enumerate</span>(mask_files):</span><br><span class="line">        mask = io.imread(mask)</span><br><span class="line">        <span class="keyword">assert</span> (mask[(mask!=<span class="number">0</span>)] == <span class="number">255</span>).<span class="built_in">all</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> masks <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          H, W = mask.shape</span><br><span class="line">          masks = np.zeros((<span class="built_in">len</span>(mask_files), H, W))</span><br><span class="line">        masks[i] = mask</span><br><span class="line"></span><br><span class="line">      total_mask = masks.<span class="built_in">sum</span>(<span class="number">0</span>)</span><br><span class="line">           <span class="keyword">assert</span> (total_mask[(total_mask!=<span class="number">0</span>)] == <span class="number">255</span>).<span class="built_in">all</span>()</span><br><span class="line"></span><br><span class="line">      item[<span class="string">&#x27;mask&#x27;</span>] = torch.from_numpy(total_mask)</span><br><span class="line"></span><br><span class="line">    item[<span class="string">&#x27;name&#x27;</span>] = <span class="built_in">str</span>(file).split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">    item[<span class="string">&#x27;img&#x27;</span>] = torch.from_numpy(img)</span><br><span class="line">    datas.append(item)</span><br><span class="line">    <span class="keyword">return</span> datas</span><br><span class="line"> </span><br><span class="line">test = process(<span class="string">&quot;/content/drive/My Drive/nuclei/stage1_test&quot;</span>, <span class="literal">False</span>)</span><br><span class="line">torch.save(test, TEST_PATH)</span><br><span class="line">train = process(<span class="string">&quot;/content/drive/My Drive/nuclei/stage1_train&quot;</span>)</span><br><span class="line">torch.save(train, TRAIN_PATH)</span><br></pre></td></tr></table></figure>

<p>具体来看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file_path = Path(file_path)</span><br><span class="line">files = <span class="built_in">sorted</span>(<span class="built_in">list</span>(file_path.iterdir()))</span><br></pre></td></tr></table></figure>
<p>这里用了Python3的pathlib库来读入文件夹路径，之前大家常用的是os.path，但现在普遍推荐使用pathlib库来替代os.path，因为其采用面向对象的方式，且用法更简单，参加资料如下：<br><a target="_blank" rel="noopener" href="https://xin053.github.io/2016/07/03/pathlib%E8%B7%AF%E5%BE%84%E5%BA%93%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/">pathlib路径库使用详解</a></p>
<p>然后再用list转换一下，是为了下面使用tqdm库来可视化进度条。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> image <span class="keyword">in</span> (file/<span class="string">&#x27;images&#x27;</span>).iterdir():</span><br><span class="line">  img = io.imread(image)</span><br><span class="line">  imgs.append(img)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">len</span>(imgs) == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> img.shape[<span class="number">2</span>] &gt; <span class="number">3</span>:</span><br><span class="line">  <span class="keyword">assert</span> (img[:, :, <span class="number">3</span>]!=<span class="number">255</span>).<span class="built_in">sum</span>() == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">img = img[:, :, :<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<p>这一部分是读取具体的图像，但这里注意两点：<br>（1）首先对每一子文件夹下的图像数量进行判断，确保只有一张图像；<br>（2）这个数据集中的图像有个特点，它是4通道的，最后一个通道的值都是255，所以这里会有shape的判断，并且使用了assert来确保最后一个通道值都是255。<br>最后取原图像的前三个通道存入新图像中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> has_mask:</span><br><span class="line">  mask_files = <span class="built_in">list</span>((file/<span class="string">&#x27;masks&#x27;</span>).iterdir())</span><br><span class="line">  masks = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i, mask <span class="keyword">in</span> <span class="built_in">enumerate</span>(mask_files):</span><br><span class="line">    mask = io.imread(mask)</span><br><span class="line">    <span class="keyword">assert</span> (mask[(mask!=<span class="number">0</span>)] == <span class="number">255</span>).<span class="built_in">all</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> masks <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      H, W = mask.shape</span><br><span class="line">      masks = np.zeros((<span class="built_in">len</span>(mask_files), H, W))</span><br><span class="line"></span><br><span class="line">    masks[i] = mask</span><br></pre></td></tr></table></figure>
<p>这一步是逐个读取掩膜文件，其中的assert语句是保证mask确实是0和255二值的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tmp_mask = masks.<span class="built_in">sum</span>(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> (tmp_mask[(tmp_mask!=<span class="number">0</span>)] == <span class="number">255</span>).<span class="built_in">all</span>()</span><br></pre></td></tr></table></figure>
<p>这一步是将masks中的同一位置上的元素进行加和，然后通过assert语句保证加和后不为0的元素都是255，这一步是保证每个像素上都最多只有一个掩膜值，即两个掩膜没有重叠。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_mask = masks.<span class="built_in">sum</span>(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>因为masks变量实际有多个通道，即多个掩膜，这一步是将每个通道上的掩膜值根据序号重新赋值，然后组合在一起，使得所有掩膜都在一张图像上。(这一步与原notebook不同，原notebook是不同的掩膜有不一样的值)</p>
<p>比如这张细胞核图像：<br><img src="https://user-images.githubusercontent.com/6218739/75554371-2b28f780-5a75-11ea-9357-928dde25f866.png" alt="1f84ac0d-1df9-42c9-b4ee-ca08102cd715"><br>它的掩膜就是：<br><img src="https://user-images.githubusercontent.com/6218739/75554405-3d0a9a80-5a75-11ea-9663-e85c11cf5df3.png" alt="012a8162-4eaa-489c-8f35-e196651a8071"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  item[<span class="string">&#x27;mask&#x27;</span>] = torch.from_numpy(total_mask)</span><br><span class="line"></span><br><span class="line">item[<span class="string">&#x27;name&#x27;</span>] = <span class="built_in">str</span>(file).split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">item[<span class="string">&#x27;img&#x27;</span>] = torch.from_numpy(img)</span><br><span class="line">datas.append(item)</span><br></pre></td></tr></table></figure>
<p>然后将图像img、文件名name和掩膜mask（如果有的话）以字典的形式存入datas这个列表中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test = process(<span class="string">&quot;/content/drive/My Drive/nuclei/stage1_test&quot;</span>, <span class="literal">False</span>)</span><br><span class="line">torch.save(test, TEST_PATH)</span><br><span class="line">train = process(<span class="string">&quot;/content/drive/My Drive/nuclei/stage1_train&quot;</span>)</span><br><span class="line">torch.save(train, TRAIN_PATH)</span><br></pre></td></tr></table></figure>
<p>最后，将这个列表用PyTorch存储模型的方式持久化存储下来。<br>这一步需要的时间很长，所以最好是将这个存储数据的列表持久化后，将其挪动到Google Drive中，防止下次重启丢失。<br>但实际操作下来，发现过程不是那么美好，首先是Google Colab给分配的RAM有点小，而训练集中的数据特别多，尤其是mask分开存储的方式，使得数据量巨多，后期直接把内存撑爆了，而且执行速度非常慢，于是将代码稍微改了以下，每隔5步就torch save一下，然后将datas清零，这样就能保证及时释放内存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> k % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">  torch.save(datas, name + np.<span class="built_in">str</span>(k)+<span class="string">&quot;.pt&quot;</span>)</span><br><span class="line">  datas = []</span><br></pre></td></tr></table></figure>
<p>然后再把所有save的数据load以后串接起来就行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_catenate</span>(<span class="params">file_path</span>):</span></span><br><span class="line">    path = Path(file_path)</span><br><span class="line">    data = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(path.iterdir()):</span><br><span class="line">        <span class="comment"># print(i)</span></span><br><span class="line">        temp = torch.load(i)</span><br><span class="line">        data += temp</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>

<h2 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h2><p>首先定义数据集格式，自定义的数据集格式需要继承torch.utils.data.Dataset，然后重载以下两个方法：<br>（1）__len__：这样len(dataset)就可以返回整个数据集的大小，<br>（2）__getitem__：这样就可以使用dataset[i]来对数据进行索引。<br>针对这里的具体训练数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainDataset</span>(<span class="params">data.Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data, source_transform, target_transform</span>):</span></span><br><span class="line">        self.datas = data</span><br><span class="line">        self.s_transform = source_transform</span><br><span class="line">        self.t_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        data = self.datas[index]</span><br><span class="line">        img = data[<span class="string">&#x27;img&#x27;</span>].numpy()</span><br><span class="line">        mask = data[<span class="string">&#x27;mask&#x27;</span>][:, :, <span class="literal">None</span>].byte().numpy()</span><br><span class="line">        img = self.s_transform(img)</span><br><span class="line">        mask = self.t_transform(mask)</span><br><span class="line">        <span class="keyword">return</span> img, mask</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.datas)</span><br></pre></td></tr></table></figure>
<p>可以看出，对img和mask分别又做了一些变换。<br>对于这些变换操作来说，最佳实践是不要写函数，而是写可调用的类，这样参数就不必每次都要传递。因此，只需实现__call__方法和__init__方法（如有必要）。然后如下调用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tsfm = Transform(params)</span><br><span class="line">transformed_sample = tsfm(sample)</span><br></pre></td></tr></table></figure>
<p>这里因为所有的变换在torchvision的transforms中是自带的，所以不需要自定义变换，只调用即可，而且使用了Compose将这些变换组合起来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL</span><br><span class="line">s_trans = transforms.Compose([</span><br><span class="line">                              transforms.ToPILImage(),</span><br><span class="line">                              transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),</span><br><span class="line">                              transforms.ToTensor(),</span><br><span class="line">                              transforms.Normalize(mean=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], std=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">t_trans = transforms.Compose([</span><br><span class="line">                              transforms.ToPILImage(),</span><br><span class="line">                              transforms.Resize((<span class="number">128</span>, <span class="number">128</span>), interpolation=PIL.Image.NEAREST),</span><br><span class="line">                              transforms.ToTensor()</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>然后将变换规则传入数据集中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = TrainDataset(train, s_trans, t_trans)</span><br></pre></td></tr></table></figure>

<p>具体使用该数据集时，可以使用for循环来逐个读取数据，但这样势必会丧失一些功能：<br>（1）批量处理数据；<br>（2）打乱数据顺序；<br>（3）并行加载数据。<br>因此，PyTorch提供了torch.utils.data.DataLoader类作为迭代器，提供上述功能。<br>将数据集放入DataLoader中，并指定参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_dataloader = data.DataLoader(train_dataset, num_workers=<span class="number">2</span>, batch_size=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<h1 id="UNet网络结构"><a href="#UNet网络结构" class="headerlink" title="UNet网络结构"></a>UNet网络结构</h1><p>崔家华的一篇博文对UNet的网络结构和代码实现讲得挺好，见<a target="_blank" rel="noopener" href="https://cuijiahua.com/blog/2019/12/dl-15.html">这里</a>。<br>本部分是对崔同学的博文的摘抄学习，代码部分中的参数是结合上面的notebook中的参数进行了修改。</p>
<h2 id="网络结构原理"><a href="#网络结构原理" class="headerlink" title="网络结构原理"></a>网络结构原理</h2><blockquote>
<p>UNet最早发表在2015的MICCAI会议上，5年多的时间，论文引用量已经达到了接近12000次。<br>UNet成为了大多做医疗影像语义分割任务的baseline，同时也启发了大量研究者对于U型网络结构的研究，发表了一批基于UNet网络结构的改进方法的论文。<br>UNet网络结构，最主要的两个特点是：U型网络结构和Skip Connection跳层连接。</p>
</blockquote>
<p><img src="https://user-images.githubusercontent.com/6218739/75503501-bde37b00-5a10-11ea-99ac-9c9015aa1bb0.png" alt="image"></p>
<blockquote>
<p>UNet是一个对称的网络结构，左侧为下采样，右侧为上采样。<br>按照功能可以将左侧的一系列下采样操作称为encoder，将右侧的一系列上采样操作称为decoder。<br>Skip Connection中间四条灰色的平行线，Skip Connection就是在上采样的过程中，融合下采样过过程中的feature map。<br>Skip Connection用到的融合的操作也很简单，就是将feature map的通道进行叠加，俗称Concat。</p>
</blockquote>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>将整个UNet结构拆分为多个模块进行分析。（后面的文字依然是摘抄自崔家华博客，不再加引用标识，见谅）</p>
<h3 id="DoubleConv模块"><a href="#DoubleConv模块" class="headerlink" title="DoubleConv模块"></a>DoubleConv模块</h3><p>从UNet网络中可以看出，不管是下采样过程还是上采样过程，每一层都会连续进行两次卷积操作，这种操作在UNet网络中重复很多次，可以单独写一个DoubleConv模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubleConv</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;(convolution =&gt; [BN] =&gt; ReLU) * 2&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.double_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.double_conv(x)</span><br></pre></td></tr></table></figure>
<p>上述的Pytorch代码：torch.nn.Sequential是一个时序容器，Modules 会以它们传入的顺序被添加到容器中。比如上述代码的操作顺序：卷积-&gt;BN-&gt;ReLU-&gt;卷积-&gt;BN-&gt;ReLU。<br>DoubleConv模块的in_channels和out_channels可以灵活设定，以便扩展使用。<br>输出矩阵的高度和宽度（即输出的特征图feature map）这两个维度的尺寸由输入矩阵、卷积核、扫描方式所共同决定，计算公式为：<br><img src="https://user-images.githubusercontent.com/6218739/75505152-71e70500-5a15-11ea-9784-2b27b6731e80.png" alt="image"></p>
<h3 id="Down模块"><a href="#Down模块" class="headerlink" title="Down模块"></a>Down模块</h3><p>UNet网络一共有4次下采样过程，模块化代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Down</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Downscaling with maxpool then double conv&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.maxpool_conv = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            DoubleConv(in_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.maxpool_conv(x)</span><br></pre></td></tr></table></figure>
<p>这里的代码很简单，就是一个maxpool池化层，进行下采样，然后接一个DoubleConv模块。<br>其中，池化层选的是2乘以2的窗口大小，那么默认获得的也是这样大小的填充步长，池化以后的feature map的大小计算方式跟上面卷积的相同。</p>
<p>至此，UNet网络的左半部分的下采样过程的代码都写好了，接下来是右半部分的上采样过程。</p>
<h3 id="Up模块"><a href="#Up模块" class="headerlink" title="Up模块"></a>Up模块</h3><p>上采样过程用到的最多的当然就是上采样了，除了常规的上采样操作，还有进行特征的融合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Up</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Upscaling then double conv&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, bilinear=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if bilinear, use the normal convolutions to reduce the number of channels</span></span><br><span class="line">        <span class="keyword">if</span> bilinear:</span><br><span class="line">            self.up = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.up = nn.ConvTranspose2d(in_channels, in_channels // <span class="number">2</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv = DoubleConv(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x1, x2</span>):</span></span><br><span class="line">        x1 = self.up(x1)</span><br><span class="line">        <span class="comment"># input is CHW</span></span><br><span class="line">        diffY = torch.tensor([x2.size()[<span class="number">2</span>] - x1.size()[<span class="number">2</span>]])</span><br><span class="line">        diffX = torch.tensor([x2.size()[<span class="number">3</span>] - x1.size()[<span class="number">3</span>]])</span><br><span class="line">        x1 = F.pad(x1, [diffX // <span class="number">2</span>, diffX - diffX // <span class="number">2</span>,</span><br><span class="line">                        diffY // <span class="number">2</span>, diffY - diffY // <span class="number">2</span>])</span><br><span class="line">        <span class="comment"># if you have padding issues, see</span></span><br><span class="line">        <span class="comment"># https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a</span></span><br><span class="line">        <span class="comment"># https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd</span></span><br><span class="line">        x = torch.cat([x2, x1], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.conv(x)</span><br></pre></td></tr></table></figure>
<p>代码复杂一些，我们可以分开来看。<br>首先是__init__初始化函数里定义的上采样方法以及卷积采用DoubleConv。<br>上采样，定义了两种方法：Upsample和ConvTranspose2d，也就是双线性插值和反卷积。<br>双线性插值很好理解，示意图：<br><img src="https://user-images.githubusercontent.com/6218739/75517269-49710200-5a39-11ea-8ae3-7316d43727cf.png" alt="image"></p>
<p>简单地讲：已知Q11、Q12、Q21、Q22四个点坐标，通过Q11和Q21求R1，再通过Q12和Q22求R2，最后通过R1和R2求P，这个过程就是双线性插值。对于一个feature map而言，其实就是在像素点中间补点，补的点的值是多少，是由相邻像素点的值决定的。<br>反卷积，顾名思义，就是反着卷积。卷积是让featuer map越来越小，反卷积就是让feature map越来越大，示意图：</p>
<p><img src="https://user-images.githubusercontent.com/6218739/75517361-76bdb000-5a39-11ea-85a2-a869f7443195.png" alt="image"></p>
<p>下面蓝色为原始图片，周围白色的虚线方块为padding结果，通常为0，上面绿色为卷积后的图片。 这个示意图，就是一个从2<em>2的feature map-&gt;4</em>4的feature map过程。<br>在forward前向传播函数中，x1接收的是上采样的数据，x2接收的是特征融合的数据。<br>如果两个feature map大小不同，那么特征融合方法可以有两种：<br>（1）将大的feature进行裁剪，再进行concat；<br>（2）将小的feature进行填充，再进行concat。</p>
<p>这里是使用的第二种，先对小的feature map进行padding，再进行concat。</p>
<h3 id="OutConv模块"><a href="#OutConv模块" class="headerlink" title="OutConv模块"></a>OutConv模块</h3><p>用上述的DoubleConv模块、Down模块、Up模块就可以拼出UNet的主体网络结构了。UNet网络的输出需要根据分割数量，整合输出通道。具体操作就是channel的变换：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OutConv</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(OutConv, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.conv(x)</span><br></pre></td></tr></table></figure>
<p>需要注意的是原notebook中没有加入Sigmoid层，但实测加入后精度提高很多。这样得到的就是一个0到1的概率分布。</p>
<h3 id="UNet网络"><a href="#UNet网络" class="headerlink" title="UNet网络"></a>UNet网络</h3><p>这一部分是将上面的模块组合起来形成整个UNet网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; Full assembly of the parts to form the complete network &quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;Refer https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_model.py&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_channels, n_classes, bilinear=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(UNet, self).__init__()</span><br><span class="line">        self.n_channels = n_channels</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        self.bilinear = bilinear</span><br><span class="line">        self.inc = DoubleConv(n_channels, <span class="number">64</span>)</span><br><span class="line">        self.down1 = Down(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.down2 = Down(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">        self.down3 = Down(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line">        self.down4 = Down(<span class="number">512</span>, <span class="number">1024</span>)</span><br><span class="line">        self.up1 = Up(<span class="number">1024</span>, <span class="number">512</span>, bilinear)</span><br><span class="line">        self.up2 = Up(<span class="number">512</span>, <span class="number">256</span>, bilinear)</span><br><span class="line">        self.up3 = Up(<span class="number">256</span>, <span class="number">128</span>, bilinear)</span><br><span class="line">        self.up4 = Up(<span class="number">128</span>, <span class="number">64</span>, bilinear)</span><br><span class="line">        self.outc = OutConv(<span class="number">64</span>, n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x1 = self.inc(x)</span><br><span class="line">        x2 = self.down1(x1)</span><br><span class="line">        x3 = self.down2(x2)</span><br><span class="line">        x4 = self.down3(x3)</span><br><span class="line">        x5 = self.down4(x4)</span><br><span class="line">        x = self.up1(x5, x4)</span><br><span class="line">        x = self.up2(x, x3)</span><br><span class="line">        x = self.up3(x, x2)</span><br><span class="line">        x = self.up4(x, x1)</span><br><span class="line">        logits = self.outc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">model = UNet(n_channels=<span class="number">1</span>, n_classes=<span class="number">1</span>).cuda()</span><br></pre></td></tr></table></figure>
<p>这里还有另外一位网友写的UNet网络进行辅助理解，<a target="_blank" rel="noopener" href="https://github.com/JavisPeng/u_net_liver/blob/master/unet.py">点击这里</a>。</p>
<h1 id="定义损失函数和优化器"><a href="#定义损失函数和优化器" class="headerlink" title="定义损失函数和优化器"></a>定义损失函数和优化器</h1><p>这里使用Dice系数定义损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">soft_dice_loss</span>(<span class="params">inputs, targets</span>):</span></span><br><span class="line">        num = targets.size(<span class="number">0</span>)</span><br><span class="line">        m1  = inputs.view(num,-<span class="number">1</span>)</span><br><span class="line">        m2  = targets.view(num,-<span class="number">1</span>)</span><br><span class="line">        intersection = (m1 * m2)</span><br><span class="line">        score = <span class="number">2.</span> * (intersection.<span class="built_in">sum</span>(<span class="number">1</span>)+<span class="number">1</span>) / (m1.<span class="built_in">sum</span>(<span class="number">1</span>) + m2.<span class="built_in">sum</span>(<span class="number">1</span>)+<span class="number">1</span>)</span><br><span class="line">        score = <span class="number">1</span> - score.<span class="built_in">sum</span>()/num</span><br><span class="line">        <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure>

<p>具体的原理可以参见如下一篇博客：<br><a target="_blank" rel="noopener" href="https://www.aiuai.cn/aifarm1159.html">医学图像分割之 Dice Loss</a></p>
<p>优化器选择Adam：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters(),lr = <span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>

<h1 id="训练和保存模型"><a href="#训练和保存模型" class="headerlink" title="训练和保存模型"></a>训练和保存模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader, <span class="number">0</span>):</span><br><span class="line">        x_train, y_train = data</span><br><span class="line">        x_train = x_train.cuda()</span><br><span class="line">        y_train = y_train.cuda()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        predict = model(x_train)</span><br><span class="line">        loss = soft_dice_loss(predict, y_train)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">20</span> == <span class="number">19</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">20</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Finish training&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这里在GPU上进行训练。<br>训练完后及时地将模型保存下来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MODEL_PATH = <span class="string">&#x27;model.pth&#x27;</span></span><br><span class="line">torch.save(model.state_dict(), MODEL_PATH)</span><br></pre></td></tr></table></figure>

<p>#测试</p>
<h2 id="创建测试数据集和加载器"><a href="#创建测试数据集和加载器" class="headerlink" title="创建测试数据集和加载器"></a>创建测试数据集和加载器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestDataset</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, path, source_transform</span>):</span></span><br><span class="line">        self.datas = torch.load(path)</span><br><span class="line">        self.s_transform = source_transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        data = self.datas[index]</span><br><span class="line">        img = data[<span class="string">&#x27;img&#x27;</span>].numpy()</span><br><span class="line">        img = self.s_transform(img)</span><br><span class="line">        <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.datas)</span><br><span class="line"></span><br><span class="line">test_dataset = TestDataset(TEST_PATH, s_trans)</span><br><span class="line">test_dataloader = data.DataLoader(test_dataset,num_workers=<span class="number">2</span>,batch_size=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h2 id="查看一下测试集"><a href="#查看一下测试集" class="headerlink" title="查看一下测试集"></a>查看一下测试集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataiter = <span class="built_in">iter</span>(test_dataloader)</span><br><span class="line">imgs = dataiter.<span class="built_in">next</span>()</span><br></pre></td></tr></table></figure>
<p>注意上面的imgs的size()是这样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">128</span>, <span class="number">128</span>])</span><br></pre></td></tr></table></figure>
<p>最前面的2是batch size，说明dataloader中一个元素包含两张图像，所以保存时需要这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">io.imsave(<span class="string">&quot;1.png&quot;</span>, imgs[<span class="number">0</span>][<span class="number">0</span>].data.numpy())</span><br></pre></td></tr></table></figure>

<h2 id="加载模型（可选）"><a href="#加载模型（可选）" class="headerlink" title="加载模型（可选）"></a>加载模型（可选）</h2><p>如有必要的话，先从模型文件中加载模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = UNet(<span class="number">1</span>, <span class="number">1</span>).cuda()</span><br><span class="line">model.load_state_dict(torch.load(MODEL_PATH))</span><br></pre></td></tr></table></figure>

<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">output = []</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">        data = data.cuda()</span><br><span class="line">        <span class="comment"># io.imsave(&quot;train.png&quot;, data[1][0].data.cpu().numpy())</span></span><br><span class="line">        predict = model(data)</span><br><span class="line">        <span class="comment"># io.imsave(&quot;test.png&quot;, o[1][0].data.cpu().numpy())</span></span><br><span class="line">        output.append(predict)</span><br><span class="line"></span><br><span class="line">    result = torch.cat(output, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>然后得到的result可以通过设置阈值来二值化显示最终结果，比如第i张图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pred = result[i][<span class="number">0</span>].data.cpu().numpy()</span><br><span class="line">pred[pred&gt;<span class="number">0.7</span>] = <span class="number">255</span></span><br><span class="line">pred[pred&lt;=<span class="number">0.7</span>] = <span class="number">0</span></span><br><span class="line">io.imsave(<span class="string">&quot;i.png&quot;</span>, pred)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2020/02/26/ImagePy_17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Be interesting!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="亓欣波">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/26/ImagePy_17/" class="post-title-link" itemprop="url">ImagePy解析：17 -- 重构版ImagePy解析</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-26 00:00:00" itemprop="dateCreated datePublished" datetime="2020-02-26T00:00:00+08:00">2020-02-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-25 15:20:30" itemprop="dateModified" datetime="2021-03-25T15:20:30+08:00">2021-03-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/computational-material-science/" itemprop="url" rel="index"><span itemprop="name">computational material science</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/02/26/ImagePy_17/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/02/26/ImagePy_17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>%%%%%%%%<br>2021.2.14更新:增加了sciapp和sciwx的介绍<br>%%%%%%%%</p>
<p>新版ImagePy有如下特点：<br>（1）将原版ImagePy非常特色的可视化组件完全解耦，比如画布、表格、对话框等组件，将其重构为sciwx库，这样第三方开发人员就可以更加方便地使用这些组件而构建自己的特定应用；<br>（2）创建了一套适用于图像处理的接口标准sciapp，其中定义了图像类Image、表格类Table、几何矢量类Shape，并实现了对这些类的常用操作，即sciapp作为后端支持；<br>（2）新版ImagePy在sciwx库和sciapp库的基础上进行再集成开发，即底层符合sciapp标准、前端使用sciwx显示，然后提供一整套完善的管理系统和丰富的插件库，从而实现复杂的图像处理功能。</p>
<p>因此，sciwx等价于napari等库，着重于可视化；sciapp等价于ImageJ等库，着重于通用数据接口，ImagePy则是基于两者的插件库。新版ImagePy架构思路清晰，集成更加自然契合，因此，ImagePy/sciapp/sciwx无论是对底层开发人员还是图像处理小白都有着无可比拟的优势：小到开发一个图像处理小工具，大到作为一个大型软件“开箱即用”，都可以轻松应对。</p>
<p>多说一句题外话，多谢龙哥的精辟的总结：对于图像处理问题，图像+矢量+图论三条腿走路。</p>
<h1 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h1><p>特别地，对sciapp和sciwx包进行一个更为详细的介绍。<br>如上所述，sciapp负责后端数据操作，sciwx负责前端组件。</p>
<h2 id="sciapp"><a href="#sciapp" class="headerlink" title="sciapp"></a>sciapp</h2><p>sciapp包的介绍主要引用了<a target="_blank" rel="noopener" href="https://github.com/Image-Py/imagepy/blob/master/sciapp/doc/cn_readme.md">官方的介绍</a>。<br>sciapp包主要有三个重要的模块：Object模块、App模块和Action模块。</p>
<h3 id="Object模块"><a href="#Object模块" class="headerlink" title="Object模块"></a>Object模块</h3><p>Object模块定义了科学计算中常用的基础数据结构封装类，当然，如果仅仅为了计算，绝大多数时候，Numpy，Pandas等数据类型已经可以胜任，这里的封装，主要是面向交互与展示的，例如Image对象是图像数据，里面带了一个lut成员，用于在展示时映射成伪彩色。<br>（1）Image：多维图像，基于Numpy<br>（2）Table：表格，基于DataFrame<br>（3）Shape: 点线面，任意多边形，可与GeoJson，Shapely互转<br>（4）Surface：三维表面</p>
<h3 id="App模块"><a href="#App模块" class="headerlink" title="App模块"></a>App模块</h3><p>App模块是一个科学容器，里面包含若干管理器managers，用于管理App所持有的上面各类对象object，这里的管理功能包括增加、删除、查询等，即对象object的生命周期都在App管理器中。以图像对象Image为例，App管理器有如下功能：<br>（1）show_img(self, img, title=None): 展示一个Image对象，并添加到app.img_manager管理器中；<br>（2）get_img(self, title=None): 根据title获取Image，如果缺省则返回管理器中的第一个Image；<br>（3）img_names(self): 返回当前app持有的Image对象名称列表；<br>（4）active_img(self, title=None): 将指定名称的Image对象置顶，以便于get_img可以优先获得；<br>（5）close_img(self, title=None): 关闭指定图像，并从app.img_manager管理器中移除。</p>
<p>除了这些特定于某种对象的功能，还有一些与用户交互的功能，比如：<br>（1）alert(self, info, title=’sciapp’): 弹出一个提示框，需要用户确认；<br>（2）yes_no(self, info, title=’sciapp’): 要求用户输入True/False；<br>（3）show_txt(self, cont, title=’sciapp’): 对用户进行文字提示；<br>（4）show_md(self, cont, title=’sciapp’): 以MarkDown语法书写，向用户弹出格式化文档；<br>（5）show_para(self, title, para, view, on_handle=None, on_ok=None, on_cancel=None, on_help=None, preview=False, modal=True): 展示交互对话框，para是参数字典，view指定了交互方式。</p>
<p>但是，需要特别注意的是，这里的App中的这些交互功能，都只是在命令行中print信息，具体使用时需要在子类中用UI框架（比如sciwx）重载这些方法。</p>
<h3 id="Action模块"><a href="#Action模块" class="headerlink" title="Action模块"></a>Action模块</h3><p>Action模块是对App所管理的对象的操作，比如对图像做滤波等。因此，该模块也是后面自定义开发时打交道最多的模块。<br>该模块与App的交互只需通过它的start函数即可，即将App类的实例app传入即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SciAction</span>:</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;base action, just has a start method, alert a hello&#x27;&#x27;&#x27;</span></span><br><span class="line">    name = <span class="string">&#x27;SciAction&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span>(<span class="params">self, app, para=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.app = app</span><br><span class="line">        app.alert(<span class="string">&#x27;Hello, I am SciAction!\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">app = App()</span><br><span class="line">SciAction().start(app)</span><br></pre></td></tr></table></figure>
<p>SciAction是所有Action的基类，它定义了最基本的功能，同时，sciapp提供了更高级的模板，供开发者的自定义action用于继承，比如：<br>（1）ImgAction：用于处理图像，自动获取当前图像，需要重载para、view进行交互，重载run进行图像处理；<br>（2）Tool：工具，用于在某种控件上的鼠标交互，同时其派生出了图像工具ImageTool、表格工具TableTool、矢量编辑工具ShapeTool（如点线面绘制）。</p>
<p>另外，Advanced目录下有一些高级模板（如支持图像多通道、批量操作、多线程支持等），供扩展插件时使用；Plugins目录下也有一些带有具体功能的、开箱即用的Action。</p>
<h2 id="sciwx"><a href="#sciwx" class="headerlink" title="sciwx"></a>sciwx</h2><p>sciwx提供了一系列基于wxPython的前端可视化组件，其中最重要的就是可视化2D图像的画布功能。</p>
<h3 id="Canvas画布"><a href="#Canvas画布" class="headerlink" title="Canvas画布"></a>Canvas画布</h3><p>Canvas画布是定制化的wxPython的Panel，其详细解析可见<a target="_blank" rel="noopener" href="https://qixinbo.info/2019/10/29/imagepy_12/">该文</a>。</p>
<h3 id="ICanvas画布、MCanvas组件、CanvasNoteBook组件、CanvasFrame应用和CanvasNoteFrame应用"><a href="#ICanvas画布、MCanvas组件、CanvasNoteBook组件、CanvasFrame应用和CanvasNoteFrame应用" class="headerlink" title="ICanvas画布、MCanvas组件、CanvasNoteBook组件、CanvasFrame应用和CanvasNoteFrame应用"></a>ICanvas画布、MCanvas组件、CanvasNoteBook组件、CanvasFrame应用和CanvasNoteFrame应用</h3><p>ICanvas是在Canvas基础上对于位图的展示提供进一步的接口支持，比如默认绑定ImageTool这种Action，提供set_img设置图像、set_rg设置数值范围、set_lut设置快速查找表、set_cn设置通道、set_tool设置工具等接口。<br>MCanvas是对ICanvas的进一步包装，比如在顶部添加显示图像信息的信息条、在底部增加可以切换某一通道、某一slice的滑动条。<br>CanvasNoteBook组件是对MCanvas的多标签页管理，即每一个标签页都可以添加一个MCanvas。<br>以上几个组件实际都是深度定制的前端组件，而接下来的CanvasFrame和CanvasNoteFrame是同时拥有前端和后端的功能，它们的父类同时是wx.Frame和上面的sciapp的App类，应该可以说这两个是可以独立运行的开箱即用的应用。<br>CanvasFrame是对MCanvas的封装，可以使用上面MCanvas的设置接口，同时还可以增加菜单栏、工具栏以及显示对话框等。<br>CanvasNoteFrame是对CanvasNoteBook的封装，即增加了标签页管理。</p>
<h3 id="VCanvas画布、SCanvas组件、VectorNoteBook组件、VectorFrame应用和VectorNoteFrame应用"><a href="#VCanvas画布、SCanvas组件、VectorNoteBook组件、VectorFrame应用和VectorNoteFrame应用" class="headerlink" title="VCanvas画布、SCanvas组件、VectorNoteBook组件、VectorFrame应用和VectorNoteFrame应用"></a>VCanvas画布、SCanvas组件、VectorNoteBook组件、VectorFrame应用和VectorNoteFrame应用</h3><p>VCanvas是在Canvas基础上对于矢量形状的展示提供进一步的接口支持，比如默认绑定ShapeTool这种Action，提供set_shp设置形状、set_tool设置工具等接口。<br>Scanvas是对VCanvas的进一步包装，比如在顶部添加显示形状信息的信息条。<br>VectorNoteBook组件时对VCanvas的多标签页管理。<br>同上，VectorFrame和VectorNoteFrame也都是兼具后端和前端功能的应用。</p>
<h2 id="前端和后端耦合"><a href="#前端和后端耦合" class="headerlink" title="前端和后端耦合"></a>前端和后端耦合</h2><p>如上所述，sciapp负责后端，sciwx负责前端，两者联动的机理如下：<br>（1）通过sciapp的dataio模块来控制输入输出，将图像等对象添加进App管理器；<br>（2）将App管理器传入其他action模块的start()入口函数，即可实现对图像等对象的操作；<br>（3）sciwx前端组件通过set_img()等接口接收App管理器，并将之可视化。<br>其中，第二步可以通过代码执行，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Gaussian().start(app)</span><br></pre></td></tr></table></figure>
<p>但也可以通过前端交互，比如通过菜单命令和工具栏的鼠标操作。那么菜单栏和工具栏又是怎样识别这些命令的呢？<br>对于菜单栏：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MenuBar</span>(<span class="params">wx.MenuBar</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, app</span>):</span></span><br><span class="line">        wx.MenuBar.__init__(self)</span><br><span class="line">        self.app = app</span><br><span class="line">        app.SetMenuBar(self)</span><br><span class="line">....</span><br><span class="line">....</span><br><span class="line">            f = <span class="keyword">lambda</span> e, p=vs: p().start(self.app)</span><br><span class="line">            self.Bind(wx.EVT_MENU, f, item)</span><br></pre></td></tr></table></figure>
<p>注意两个地方：<br>一个是MenuBar的初始化函数，需要传入app实例；第二是在添加菜单项时，其功能通过lambda函数调用了命令的start()函数。<br>对于工具栏：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToolBar</span>(<span class="params">wx.Panel</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, parent, vertical=<span class="literal">False</span></span>):</span></span><br><span class="line">        self.app = parent</span><br><span class="line"></span><br><span class="line">        btn.Bind( wx.EVT_LEFT_DOWN, <span class="keyword">lambda</span> e, obj=obj: self.on_tool(e, obj))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_tool</span>(<span class="params">self, evt, tol</span>):</span></span><br><span class="line">        tol.start(self.app)</span><br></pre></td></tr></table></figure>
<p>仍然是两个地方：ToolBar的初始化函数也传入parent了，它实际也是app实例；第二也是鼠标按下操作绑定了工具命令的start()函数。</p>
<p>这个作为后端服务的app实例可以额外创建，但是通常做法是将前端和后端联合起来，创建一个组合体，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageApp</span>(<span class="params">wx.Frame, App</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"> self, parent </span>):</span></span><br><span class="line">        wx.Frame.__init__ ( self, parent, <span class="built_in">id</span> = wx.ID_ANY, title = <span class="string">&#x27;ImageApp&#x27;</span>,</span><br><span class="line">                            size = wx.Size(<span class="number">800</span>,<span class="number">600</span>), pos = wx.DefaultPosition,</span><br><span class="line">                            style = wx.RESIZE_BORDER|wx.DEFAULT_FRAME_STYLE|wx.TAB_TRAVERSAL )</span><br><span class="line">        App.__init__(self)</span><br></pre></td></tr></table></figure>
<p>因此，添加菜单栏和工具栏时，传入的都是self，即自身，因为其自身就有App管理器的能力。</p>
<p>下面是对ImagePy所基于的sciwx库各个组件的demo进行逐步解析（最好是直接运行一下，以获得直观感受）。</p>
<h1 id="画布"><a href="#画布" class="headerlink" title="画布"></a>画布</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage.data <span class="keyword">import</span> camera</span><br><span class="line"><span class="keyword">from</span> sciwx.canvas <span class="keyword">import</span> Canvas</span><br><span class="line"><span class="keyword">import</span> wx</span><br><span class="line"></span><br><span class="line">app = wx.App()</span><br><span class="line">frame = wx.Frame(<span class="literal">None</span>, title=<span class="string">&#x27;gray test&#x27;</span>)</span><br><span class="line">canvas = Canvas(frame, autofit=<span class="literal">True</span>)</span><br><span class="line">canvas.set_img(camera())</span><br><span class="line">frame.Show()</span><br><span class="line">app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>ImagePy/sciwx展示一张图像所使用的是Canvas类，它是对wxPython的面板Panel类的深度定制，可以实现对图像的区域缩放、拖动、标注等。之前写过一篇对该类的详细解释，见<a target="_blank" rel="noopener" href="http://qixinbo.info/2019/10/29/ImagePy_12/">这里</a>。</p>
<h1 id="通道、图像序列展示"><a href="#通道、图像序列展示" class="headerlink" title="通道、图像序列展示"></a>通道、图像序列展示</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage.data <span class="keyword">import</span> astronaut</span><br><span class="line"><span class="keyword">from</span> sciwx.canvas <span class="keyword">import</span> MCanvas</span><br><span class="line"><span class="keyword">import</span> wx</span><br><span class="line"></span><br><span class="line">app = wx.App()</span><br><span class="line">frame = wx.Frame(<span class="literal">None</span>, title=<span class="string">&#x27;gray test&#x27;</span>)</span><br><span class="line">canvas = MCanvas(frame, autofit=<span class="literal">True</span>)</span><br><span class="line">canvas.set_imgs([astronaut(), <span class="number">255</span>-astronaut()])</span><br><span class="line">canvas.set_cn(<span class="number">0</span>)</span><br><span class="line">frame.Show()</span><br><span class="line">app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>上面的Canvas类仅能展示一张图像，这里的MCanvas类则是用于展示图像序列和多通道：<br>（1）图像序列：将多个图像组合成列表list，然后传入set_imgs()方法中；<br>（2）通道：对于多通道图像，可以传入单个通道，如0或1或2，这时是单通道灰度显示，也可以组合成(0, 1, 2)，即RGB彩色显示，甚至可以任意按不同顺序组合通道，比如(1, 0, 2)，即将原来的通道1变成现在的通道0，再彩色显示。对于灰度图，则只有通道0。</p>
<h1 id="内部图像类Image"><a href="#内部图像类Image" class="headerlink" title="内部图像类Image"></a>内部图像类Image</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage.data <span class="keyword">import</span> camera</span><br><span class="line"><span class="keyword">from</span> sciwx.canvas <span class="keyword">import</span> Canvas, Image</span><br><span class="line"><span class="keyword">import</span> wx</span><br><span class="line"></span><br><span class="line">app = wx.App()</span><br><span class="line">obj = Image()</span><br><span class="line">obj.img = camera()</span><br><span class="line">obj.cn = <span class="number">0</span></span><br><span class="line">frame = wx.Frame(<span class="literal">None</span>, title=<span class="string">&#x27;gray test&#x27;</span>)</span><br><span class="line">canvas = Canvas(frame, autofit=<span class="literal">True</span>)</span><br><span class="line">canvas.set_img(obj)</span><br><span class="line">frame.Show()   </span><br><span class="line">app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>这个例子是测试ImagePy的内部图像类Image，该类是源图像的一个包装，同时提供多种属性供调用，比如图像名称title、源图像img、通道数channels、整个图像序列中包含图像个数slices、图像尺寸shape、色彩范围range、快照snapshot等。</p>
<p>这里是先构造一个Image对象，然后将camera这张图像传给该对象的img属性，然后再传给Canvas。<br>之前例子中Canvas是直接接收camera，这两种方式都可以，因为Canvas类中对类型做了判断和处理。</p>
<h1 id="自定义鼠标事件"><a href="#自定义鼠标事件" class="headerlink" title="自定义鼠标事件"></a>自定义鼠标事件</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage.data <span class="keyword">import</span> camera</span><br><span class="line"><span class="keyword">from</span> sciwx.canvas <span class="keyword">import</span> Canvas</span><br><span class="line"><span class="keyword">import</span> wx</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestTool</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_down</span>(<span class="params">self, image, x, y, btn, **key</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;x:%d y:%d btn:%d ctrl:%s alt:%s shift:%s&#x27;</span>%</span><br><span class="line">              (x, y, btn, key[<span class="string">&#x27;ctrl&#x27;</span>], key[<span class="string">&#x27;alt&#x27;</span>], key[<span class="string">&#x27;shift&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_up</span>(<span class="params">self, image, x, y, btn, **key</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_move</span>(<span class="params">self, image, x, y, btn, **key</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_wheel</span>(<span class="params">self, image, x, y, d, **key</span>):</span></span><br><span class="line">        image.img[:] = image.img + d</span><br><span class="line">        key[<span class="string">&#x27;canvas&#x27;</span>].update()</span><br><span class="line"></span><br><span class="line">app = wx.App()</span><br><span class="line">frame = wx.Frame(<span class="literal">None</span>)</span><br><span class="line">canvas = Canvas(frame, autofit=<span class="literal">True</span>)</span><br><span class="line">canvas.set_img(camera())</span><br><span class="line">canvas.set_tool(TestTool())</span><br><span class="line">frame.Show()</span><br><span class="line">app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>这一步实际是将画布中的默认绑定的DefaultTool改成了自定义的TestTool，然后再将动作反馈给画布。<br>从中也可以看出自定义工具中可以调用的接口：<br>（1）image：即画布承载的Image对象，具体的源图像则是image.img<br>（2）x和y：当前鼠标所在的图像像素坐标，水平方向是x方向，垂直方向是y方向，这两个坐标是与图像的Numpy数据存储一一对应的：因为Numpy的元素获取是先row后column，所以这里的y对应row，x对应column。另外因为这两个坐标有可能因为缩放、移动而产生负值，此时如果要做画笔这类的应用的话，就需要clip一下，保证画笔始终在图像中；不过另一方面，这样的设置也使得可以draw出超出图像的更大的ROI。还有下面一种坐标，是代表鼠标在面板中的像素坐标。btn是鼠标按键，1为左键按下，2为中键按下，3为右键按下，key是字典，里面有多个字段，key[‘alt’]代表是否按下alt键，key[‘ctrl’]代表是否按下ctrl键，key[‘shift’]代表是否按下shift键，key[‘px’]返回鼠标当前的画布x坐标，key[‘py’]返回画布y坐标，key[‘canvas’]返回该画布自身。</p>
<h1 id="工具栏"><a href="#工具栏" class="headerlink" title="工具栏"></a>工具栏</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> wx</span><br><span class="line"><span class="keyword">from</span> sciwx.widgets <span class="keyword">import</span> ToolBar</span><br><span class="line"><span class="keyword">from</span> sciwx.action <span class="keyword">import</span> Tool</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestTool</span>(<span class="params">Tool</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span>(<span class="params">self, app</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;i am a tool&quot;</span>)</span><br><span class="line"></span><br><span class="line">app = wx.App()</span><br><span class="line">frame = wx.Frame(<span class="literal">None</span>)</span><br><span class="line">tool = ToolBar(frame)</span><br><span class="line">tool.add_tool(<span class="string">&#x27;A&#x27;</span>, TestTool)</span><br><span class="line">tool.add_tools(<span class="string">&#x27;B&#x27;</span>, [(<span class="string">&#x27;A&#x27;</span>, TestTool), (<span class="string">&#x27;C&#x27;</span>, <span class="literal">None</span>)])</span><br><span class="line">tool.Layout()</span><br><span class="line">frame.Fit()</span><br><span class="line">frame.Show()</span><br><span class="line">app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>上面的自定义鼠标事件是在后台将默认的鼠标事件进行了重载，无法显示成一个工具。且如果多个工具，每个工具点击后的鼠标事件都不同，是需要将这些事件分开来写的。<br>这个例子是用来显示工具栏，里面的TestTool纯粹是为了该例子的完整运行，没有任何的实际意义，正规的自定义工具是要重载各种鼠标动作。<br>实际上的工具栏的鼠标动作是与具体的Canvas相绑定的，所以在没有添加canvas的情况下是无法执行具体工具的。<br>可以看出：<br>（1）sciwx现在不仅支持使用icon作为工具图标，也支持使用单个英文字母作为图标，更加方便易用；<br>（2）sciwx支持一次性添加多个工具。<br>后面的例子会展示将画布与工具进行绑定。</p>
<h1 id="集成面板"><a href="#集成面板" class="headerlink" title="集成面板"></a>集成面板</h1><p>CanvasFrame是将画布与菜单栏、工具栏集成显示（实际无法与菜单栏集成，因为菜单栏需要传入app实例，下方具体解释，且后面有具体的集成菜单栏的方法），CanvasNoteFrame进一步地提供标签页功能。<br>不过这一步仅是测试这两个类能否正确使用，还没有加上特定的菜单栏和工具栏，下面的例子中将具体展示。<br>同时注意，这一步调用时不再需要像之前的例子那样事先生成一个wxPython的Frame，因为这两个类本身就是Frame，所以省了这一步，相当于定制化了wxPython的frame。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage.data <span class="keyword">import</span> astronaut, camera</span><br><span class="line"><span class="keyword">from</span> sciwx.canvas <span class="keyword">import</span> CanvasFrame, CanvasNoteFrame</span><br><span class="line"><span class="keyword">import</span> wx</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">canvas_frame_test</span>():</span></span><br><span class="line">    cf = CanvasFrame(<span class="literal">None</span>, autofit=<span class="literal">True</span>)</span><br><span class="line">    cf.set_imgs([camera(), <span class="number">255</span>-camera()])</span><br><span class="line">    cf.Show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">canvas_note_test</span>():</span></span><br><span class="line">    cnf = CanvasNoteFrame(<span class="literal">None</span>)</span><br><span class="line">    cv1 = cnf.add_canvas()</span><br><span class="line">    cv1.set_img(camera())</span><br><span class="line">    cv2 = cnf.add_canvas()</span><br><span class="line">    cv2.set_img(astronaut())</span><br><span class="line">    cv2.set_cn((<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>))</span><br><span class="line">    cnf.Show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app = wx.App()</span><br><span class="line">    canvas_frame_test()</span><br><span class="line">    canvas_note_test()</span><br><span class="line">    app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>CanvasFrame和CanvasNoteFrame归根结底都是调用了Canvas类，且将Canvas一系列的设定方法传递给了它们。</p>
<h1 id="画布集成工具栏"><a href="#画布集成工具栏" class="headerlink" title="画布集成工具栏"></a>画布集成工具栏</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage.draw <span class="keyword">import</span> line</span><br><span class="line"><span class="keyword">from</span> sciwx.canvas <span class="keyword">import</span> CanvasFrame</span><br><span class="line"><span class="keyword">from</span> sciwx.action <span class="keyword">import</span> Tool, DefaultTool</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pencil</span>(<span class="params">Tool</span>):</span></span><br><span class="line">    title = <span class="string">&#x27;Pencil&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.status = <span class="literal">False</span></span><br><span class="line">        self.oldp = (<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_down</span>(<span class="params">self, ips, x, y, btn, **key</span>):</span></span><br><span class="line">        self.status = <span class="literal">True</span></span><br><span class="line">        self.oldp = (y, x)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_up</span>(<span class="params">self, ips, x, y, btn, **key</span>):</span></span><br><span class="line">        self.status = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_move</span>(<span class="params">self, ips, x, y, btn, **key</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.status:<span class="keyword">return</span></span><br><span class="line">        se = self.oldp + (y,x)</span><br><span class="line">        rs,cs = line(*[<span class="built_in">int</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> se])</span><br><span class="line">        rs.clip(<span class="number">0</span>, ips.shape[<span class="number">1</span>], out=rs)</span><br><span class="line">        cs.clip(<span class="number">0</span>, ips.shape[<span class="number">0</span>], out=cs)</span><br><span class="line">        ips.img[rs,cs] = (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">        self.oldp = (y, x)</span><br><span class="line">        key[<span class="string">&#x27;canvas&#x27;</span>].update()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_wheel</span>(<span class="params">self, ips, x, y, d, **key</span>):</span><span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> skimage.data <span class="keyword">import</span> camera, astronaut</span><br><span class="line">    <span class="keyword">from</span> skimage.io <span class="keyword">import</span> imread</span><br><span class="line"></span><br><span class="line">    app = wx.App()</span><br><span class="line">    cf = CanvasFrame(<span class="literal">None</span>, autofit=<span class="literal">False</span>)</span><br><span class="line">    cf.set_imgs([astronaut(), <span class="number">255</span>-astronaut()])</span><br><span class="line">    cf.set_cn((<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">    bar = cf.add_toolbar()</span><br><span class="line">    bar.add_tool(<span class="string">&#x27;M&#x27;</span>, DefaultTool)</span><br><span class="line">    bar.add_tool(<span class="string">&#x27;P&#x27;</span>, Pencil)</span><br><span class="line">    cf.Show()</span><br><span class="line">    app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>这个例子就是将画布与默认工具和画笔工具集成起来。<br>由于这里是两个工具，那么这个画布是怎样知道该响应哪个工具了吗？原理如下：<br>（1）ToolBar类中将具体工具绑定了鼠标单击事件，当某一工具被点击后，就会触发它继承自父类Tool的start()方法，将该工具自身传给Tool.default；<br>（2）Canvas画布类会时刻监听鼠标事件，其中会调用Tool.default，于是，两者就关联了起来。此处Tool.default是种类似“多态”的用法，即直接调用父类，无需知道其具体的子类类型，具体调用则是看运行时类型决定。</p>
<h1 id="菜单栏"><a href="#菜单栏" class="headerlink" title="菜单栏"></a>菜单栏</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> wx</span><br><span class="line"><span class="keyword">from</span> sciwx.widgets <span class="keyword">import</span> MenuBar</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">P</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span>(<span class="params">self, app</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(self.name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">data = (<span class="string">&#x27;menu&#x27;</span>, [</span><br><span class="line">        (<span class="string">&#x27;File&#x27;</span>, [(<span class="string">&#x27;Open&#x27;</span>, P(<span class="string">&#x27;O&#x27;</span>)),</span><br><span class="line">                  <span class="string">&#x27;-&#x27;</span>,</span><br><span class="line">                  (<span class="string">&#x27;Close&#x27;</span>, P(<span class="string">&#x27;C&#x27;</span>))]),</span><br><span class="line">        (<span class="string">&#x27;Edit&#x27;</span>, [(<span class="string">&#x27;Copy&#x27;</span>, P(<span class="string">&#x27;C&#x27;</span>)),</span><br><span class="line">                  (<span class="string">&#x27;A&#x27;</span>, [(<span class="string">&#x27;B&#x27;</span>, P(<span class="string">&#x27;B&#x27;</span>)),</span><br><span class="line">                         (<span class="string">&#x27;C&#x27;</span>, P(<span class="string">&#x27;C&#x27;</span>))]),</span><br><span class="line">                  (<span class="string">&#x27;Paste&#x27;</span>, P(<span class="string">&#x27;P&#x27;</span>))])])</span><br><span class="line"></span><br><span class="line">app = wx.App()</span><br><span class="line">frame = wx.Frame(<span class="literal">None</span>)</span><br><span class="line">menubar = MenuBar(frame)</span><br><span class="line">menubar.load(data)</span><br><span class="line">frame.SetMenuBar(menubar)</span><br><span class="line">frame.Show()</span><br><span class="line">app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>该例有两个特点：<br>（1）菜单项是以元组的形式传入MenuBar中，且能多级解析，这就提供了非常大的灵活性，后面ImagePy的丰富的插件系统也能顺利地加载进来；<br>（2）类P实际就是一个最小的可运行的插件，其重要的一个成员函数就是start()方法，注意到其需要传入一个app参数（这里因为没有用到图像，所以app没有实际用处）。</p>
<p>从菜单栏开始，相比于以前的ImagePy的管理机制，新解耦的sciwx拥有了一个新的管理方式，即App类，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">App</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.img_manager = Manager()</span><br><span class="line">        self.wimg_manager = Manager()</span><br><span class="line">        self.tab_manager = Manager()</span><br><span class="line">        self.wtab_manager = Manager()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_img</span>(<span class="params">self, img</span>):</span> <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_table</span>(<span class="params">self, img</span>):</span> <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_md</span>(<span class="params">self, img, title=<span class="string">&#x27;&#x27;</span></span>):</span> <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_txt</span>(<span class="params">self, img, title=<span class="string">&#x27;&#x27;</span></span>):</span> <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span>(<span class="params">self</span>):</span> <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot3d</span>(<span class="params">self</span>):</span> <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_img</span>(<span class="params">self, img</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;add&#x27;</span>, img.name)</span><br><span class="line">        self.img_manager.add(img)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">remove_img</span>(<span class="params">self, img</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;remove&#x27;</span>, img.name)</span><br><span class="line">        self.img_manager.remove(img)</span><br><span class="line">…</span><br><span class="line">…</span><br></pre></td></tr></table></figure>
<p>图像、表格属于基础元素，其管理和展示，归为app自身的功能（以前是通过管理器），创建app实例就可以维护基础元素的信息。这样可以有如下优点：<br>（1）实现了UI定制，元素的操作与具体的Desktop端、Web端或Headless样式进行解耦。比如sciwx自带一个sciapp模块，它就是一个具体的wxPython的前端实现，同理，也可以自己创建一个web的前端或headless的接口调用；对于headless形式的接口，可以ssh远程登录调用它处理图像，也可以将使用GUI前端形成的处理流程转成headless形式的流程，然后放到服务器上进行运行，这就适用于需要长时间处理图像的大型任务；<br>（2）创建某个插件时，可以将app对象传入进去，相当于拿着app对象，就可以获取当前打开的各种元素，以显示各种信息。换句话说，插件所需要干的三件事：获取数据、处理数据和展示数据，处理数据时插件自身的功能，而第一个和第三个都是通过与app交互来实现的。与之前相比，不再需要全局的处理函数IPy。</p>
<p>下面就是实际对app的应用实例。</p>
<h1 id="集成菜单栏"><a href="#集成菜单栏" class="headerlink" title="集成菜单栏"></a>集成菜单栏</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.ndimage <span class="keyword">import</span> gaussian_filter</span><br><span class="line"><span class="keyword">from</span> sciwx.canvas <span class="keyword">import</span> CanvasFrame</span><br><span class="line"><span class="keyword">from</span> sciwx.action <span class="keyword">import</span> ImgAction</span><br><span class="line"><span class="keyword">from</span> sciwx.app.manager <span class="keyword">import</span> App</span><br><span class="line"><span class="keyword">from</span> sciwx.widgets <span class="keyword">import</span> MenuBar</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Gaussian</span>(<span class="params">ImgAction</span>):</span></span><br><span class="line">    title = <span class="string">&#x27;Gaussian&#x27;</span></span><br><span class="line">    note = [<span class="string">&#x27;auto_snap&#x27;</span>, <span class="string">&#x27;preview&#x27;</span>]</span><br><span class="line">    para = &#123;<span class="string">&#x27;sigma&#x27;</span>:<span class="number">2</span>&#125;</span><br><span class="line">    view = [(<span class="built_in">float</span>, <span class="string">&#x27;sigma&#x27;</span>, (<span class="number">0</span>, <span class="number">30</span>), <span class="number">1</span>, <span class="string">&#x27;sigma&#x27;</span>, <span class="string">&#x27;pix&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self, ips, img, snap, para</span>):</span></span><br><span class="line">        gaussian_filter(snap, para[<span class="string">&#x27;sigma&#x27;</span>], output=img)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Undo</span>(<span class="params">ImgAction</span>):</span></span><br><span class="line">    title = <span class="string">&#x27;Undo&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self, ips, img, snap, para</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(ips.img.mean(), ips.snap.mean())</span><br><span class="line">        ips.swap()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestFrame</span>(<span class="params">CanvasFrame, App</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span> (<span class="params">self, parent</span>):</span></span><br><span class="line">        CanvasFrame.__init__(self, parent)</span><br><span class="line">        App.__init__(self)</span><br><span class="line"></span><br><span class="line">        self.Bind(wx.EVT_ACTIVATE, self.init_image)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_image</span>(<span class="params">self, event</span>):</span></span><br><span class="line">        self.add_img(self.canvas.image)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_menubar</span>(<span class="params">self</span>):</span></span><br><span class="line">        menubar = MenuBar(self)</span><br><span class="line">        self.SetMenuBar(menubar)</span><br><span class="line">        <span class="keyword">return</span> menubar</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> skimage.data <span class="keyword">import</span> camera, astronaut</span><br><span class="line">    <span class="keyword">from</span> skimage.io <span class="keyword">import</span> imread</span><br><span class="line"></span><br><span class="line">    app = wx.App()</span><br><span class="line">    cf = TestFrame(<span class="literal">None</span>)</span><br><span class="line">    cf.set_img(camera())</span><br><span class="line">    cf.set_cn(<span class="number">0</span>)</span><br><span class="line">    bar = cf.add_menubar()</span><br><span class="line">    bar.load((<span class="string">&#x27;menu&#x27;</span>,[(<span class="string">&#x27;Filter&#x27;</span>,[(<span class="string">&#x27;Gaussian&#x27;</span>, Gaussian),</span><br><span class="line">                                 (<span class="string">&#x27;Unto&#x27;</span>, Undo)]),</span><br><span class="line">                      ]))</span><br><span class="line">    cf.Show()</span><br><span class="line">    app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>一个最小可用的例子如上。<br>需要说明的是：<br>（1）CanvasFrame定位是带窗口的画布，但因为它默认添加了菜单栏，而菜单栏是插件的组合，所以需要传入app，但当前的CanvasFrame没有提供app接口，所以这里新定义了一个类TestFrame，它继承自原始的CanvasFrame和App，然后将原生的CanvasFrame中的add_menu()方法直接复制并粘贴到新的TestFrame下。<br>（2）此处的TestFrame是与sciapp等价的，只是TestFrame是最小可用的一个前端实现，而sciapp是大型框架ImagePy的实现。</p>
<h1 id="集成工具栏和菜单栏"><a href="#集成工具栏和菜单栏" class="headerlink" title="集成工具栏和菜单栏"></a>集成工具栏和菜单栏</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.ndimage <span class="keyword">import</span> gaussian_filter</span><br><span class="line"><span class="keyword">from</span> skimage.draw <span class="keyword">import</span> line</span><br><span class="line"><span class="keyword">from</span> sciwx.canvas <span class="keyword">import</span> CanvasFrame</span><br><span class="line"><span class="keyword">from</span> sciwx.action <span class="keyword">import</span> ImgAction, Tool, DefaultTool</span><br><span class="line"><span class="keyword">from</span> sciwx.app <span class="keyword">import</span> App</span><br><span class="line"><span class="keyword">from</span> sciwx.widgets <span class="keyword">import</span> MenuBar</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Gaussian</span>(<span class="params">ImgAction</span>):</span></span><br><span class="line">    title = <span class="string">&#x27;Gaussian&#x27;</span></span><br><span class="line">    note = [<span class="string">&#x27;auto_snap&#x27;</span>, <span class="string">&#x27;preview&#x27;</span>]</span><br><span class="line">    para = &#123;<span class="string">&#x27;sigma&#x27;</span>:<span class="number">2</span>&#125;</span><br><span class="line">    view = [(<span class="built_in">float</span>, <span class="string">&#x27;sigma&#x27;</span>, (<span class="number">0</span>, <span class="number">30</span>), <span class="number">1</span>, <span class="string">&#x27;sigma&#x27;</span>, <span class="string">&#x27;pix&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self, ips, img, snap, para</span>):</span></span><br><span class="line">        gaussian_filter(snap, para[<span class="string">&#x27;sigma&#x27;</span>], output=img)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Undo</span>(<span class="params">ImgAction</span>):</span></span><br><span class="line">    title = <span class="string">&#x27;Undo&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self, ips, img, snap, para</span>):</span> ips.swap()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pencil</span>(<span class="params">Tool</span>):</span></span><br><span class="line">    title = <span class="string">&#x27;Pencil&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.status = <span class="literal">False</span></span><br><span class="line">        self.oldp = (<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_down</span>(<span class="params">self, ips, x, y, btn, **key</span>):</span></span><br><span class="line">        self.status = <span class="literal">True</span></span><br><span class="line">        self.oldp = (y, x)</span><br><span class="line">        ips.snapshot()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_up</span>(<span class="params">self, ips, x, y, btn, **key</span>):</span></span><br><span class="line">        self.status = <span class="literal">False</span></span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_move</span>(<span class="params">self, ips, x, y, btn, **key</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.status:<span class="keyword">return</span></span><br><span class="line">        se = self.oldp + (y,x)</span><br><span class="line">        rs,cs = line(*[<span class="built_in">int</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> se])</span><br><span class="line">        rs.clip(<span class="number">0</span>, ips.shape[<span class="number">1</span>], out=rs)</span><br><span class="line">        cs.clip(<span class="number">0</span>, ips.shape[<span class="number">0</span>], out=cs)</span><br><span class="line">        ips.img[rs,cs] = <span class="number">255</span></span><br><span class="line">        self.oldp = (y, x)</span><br><span class="line">        key[<span class="string">&#x27;canvas&#x27;</span>].update()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mouse_wheel</span>(<span class="params">self, ips, x, y, d, **key</span>):</span><span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestFrame</span>(<span class="params">CanvasFrame, App</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span> (<span class="params">self, parent</span>):</span></span><br><span class="line">        CanvasFrame.__init__(self, parent)</span><br><span class="line">        App.__init__(self)</span><br><span class="line"> </span><br><span class="line">        self.Bind(wx.EVT_ACTIVATE, self.init_image)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_image</span>(<span class="params">self, event</span>):</span></span><br><span class="line">        self.add_img(self.canvas.image)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_menubar</span>(<span class="params">self</span>):</span></span><br><span class="line">        menubar = MenuBar(self)</span><br><span class="line">        self.SetMenuBar(menubar)</span><br><span class="line">        <span class="keyword">return</span> menubar</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> skimage.data <span class="keyword">import</span> camera, astronaut</span><br><span class="line">    <span class="keyword">from</span> skimage.io <span class="keyword">import</span> imread</span><br><span class="line"></span><br><span class="line">    app = wx.App()</span><br><span class="line">    cf = TestFrame(<span class="literal">None</span>)</span><br><span class="line">    cf.set_img(camera())</span><br><span class="line">    cf.set_cn(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    bar = cf.add_menubar()</span><br><span class="line">    bar.load((<span class="string">&#x27;menu&#x27;</span>,[(<span class="string">&#x27;Filter&#x27;</span>,[(<span class="string">&#x27;Gaussian&#x27;</span>, Gaussian),</span><br><span class="line">                                 (<span class="string">&#x27;Unto&#x27;</span>, Undo)]),]))</span><br><span class="line"></span><br><span class="line">    bar = cf.add_toolbar()</span><br><span class="line">    bar.add_tool(<span class="string">&#x27;M&#x27;</span>, DefaultTool)</span><br><span class="line">    bar.add_tool(<span class="string">&#x27;P&#x27;</span>, Pencil)</span><br><span class="line">    cf.Show()</span><br><span class="line">    app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>这个例子又在上面例子的基础上加了工具栏。</p>
<h1 id="全组件集合版"><a href="#全组件集合版" class="headerlink" title="全组件集合版"></a>全组件集合版</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sciwx.app <span class="keyword">import</span> SciApp</span><br><span class="line"><span class="keyword">from</span> sciwx.action <span class="keyword">import</span> ImgAction, Tool, DefaultTool</span><br><span class="line"><span class="keyword">from</span> sciwx.plugins.curve <span class="keyword">import</span> Curve</span><br><span class="line"><span class="keyword">from</span> sciwx.plugins.channels <span class="keyword">import</span> Channels</span><br><span class="line"><span class="keyword">from</span> sciwx.plugins.histogram <span class="keyword">import</span> Histogram</span><br><span class="line"><span class="keyword">from</span> sciwx.plugins.viewport <span class="keyword">import</span> ViewPort</span><br><span class="line"><span class="keyword">from</span> sciwx.plugins.filters <span class="keyword">import</span> Gaussian, Undo</span><br><span class="line"><span class="keyword">from</span> sciwx.plugins.pencil <span class="keyword">import</span> Pencil</span><br><span class="line"><span class="keyword">from</span> sciwx.plugins.io <span class="keyword">import</span> Open, Save</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> skimage.data <span class="keyword">import</span> camera</span><br><span class="line">   </span><br><span class="line">    app = wx.App(<span class="literal">False</span>)</span><br><span class="line">    frame = SciApp(<span class="literal">None</span>)</span><br><span class="line">   </span><br><span class="line">    frame.load_menu((<span class="string">&#x27;menu&#x27;</span>,[(<span class="string">&#x27;File&#x27;</span>,[(<span class="string">&#x27;Open&#x27;</span>, Open),</span><br><span class="line">                                      (<span class="string">&#x27;Save&#x27;</span>, Save)]),</span><br><span class="line">                             (<span class="string">&#x27;Filters&#x27;</span>, [(<span class="string">&#x27;Gaussian&#x27;</span>, Gaussian),</span><br><span class="line">                                          (<span class="string">&#x27;Undo&#x27;</span>, Undo)])]))</span><br><span class="line"></span><br><span class="line">    frame.load_tool((<span class="string">&#x27;tools&#x27;</span>,[(<span class="string">&#x27;standard&#x27;</span>, [(<span class="string">&#x27;P&#x27;</span>, Pencil),</span><br><span class="line">                                            (<span class="string">&#x27;D&#x27;</span>, DefaultTool)]),</span><br><span class="line">                              (<span class="string">&#x27;draw&#x27;</span>, [(<span class="string">&#x27;X&#x27;</span>, Pencil),</span><br><span class="line">                                        (<span class="string">&#x27;X&#x27;</span>, Pencil)])]), <span class="string">&#x27;draw&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    frame.load_widget((<span class="string">&#x27;widgets&#x27;</span>, [(<span class="string">&#x27;Histogram&#x27;</span>, [(<span class="string">&#x27;Histogram&#x27;</span>, Histogram),</span><br><span class="line">                                                  (<span class="string">&#x27;Curve&#x27;</span>, Curve),</span><br><span class="line">                                                  (<span class="string">&#x27;Channels&#x27;</span>, Channels)]),</span><br><span class="line">                                   (<span class="string">&#x27;Navigator&#x27;</span>, [(<span class="string">&#x27;Viewport&#x27;</span>, ViewPort)])]))</span><br><span class="line"></span><br><span class="line">    frame.show_img(camera())</span><br><span class="line">    frame.show_img(camera())</span><br><span class="line">    frame.Show()</span><br><span class="line">    app.MainLoop()</span><br></pre></td></tr></table></figure>
<p>该例子可以说是基于sciwx的重构版ImagePy的雏形了，面板、菜单栏、工具栏、直方图、鹰眼灯组件悉数登场，可以说非常完善了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2020/02/16/kaggle-steel-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Be interesting!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="亓欣波">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/16/kaggle-steel-2/" class="post-title-link" itemprop="url">Kaggle钢铁赛：基于PyTorch/UNet算法的钢材表面缺陷检测——（2）算法分析</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-16 00:00:00" itemprop="dateCreated datePublished" datetime="2020-02-16T00:00:00+08:00">2020-02-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-25 15:20:30" itemprop="dateModified" datetime="2021-03-25T15:20:30+08:00">2021-03-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">programming</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/02/16/kaggle-steel-2/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/02/16/kaggle-steel-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>Kaggle上有一个<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/severstal-steel-defect-detection/overview">钢材表面缺陷检测的竞赛</a>，是一个很好的将深度学习应用于传统材料检测的例子。对该赛题和解法的剖析，可以辅助理解深度学习的流程，及其应用于具体问题的套路。</p>
<p>这次解析分为两部分：</p>
<p>（1）第一部分，即<a target="_blank" rel="noopener" href="https://qixinbo.info/2020/02/15/kaggle-steel/">上一篇文章</a>，是一个预备性工作，即对该竞赛的数据集的分析和可视化，参考的是这个notebook<a target="_blank" rel="noopener" href="https://www.kaggle.com/go1dfish/clear-mask-visualization-and-simple-eda">clear mask visualization and simple eda</a>。感谢GoldFish的分享。</p>
<p>（2）第二部分，即本文，参考的是Rishabh Agrahari的<a target="_blank" rel="noopener" href="https://www.kaggle.com/rishabhiitbhu/unet-starter-kernel-pytorch-lb-0-88">使用PyTorch框架及UNet算法的notebook</a>，中间穿插了很多背景知识介绍。</p>
<p>再次声明一下，本次训练是在Google Colab上进行，所以有的命令行命令有些许不同，比如sh命令都加上了叹号，不过不影响理解。</p>
<p>一些关于卷积神经网络的预备知识：<br>-<a target="_blank" rel="noopener" href="https://blog.csdn.net/v_JULY_v/article/details/51812459">CNN笔记：通俗理解卷积神经网络</a><br>-<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29119239">CNN中卷积层的计算细节</a><br>-<a target="_blank" rel="noopener" href="https://flat2010.github.io/2018/06/15/%E6%89%8B%E7%AE%97CNN%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0/">CNN中的参数解释及计算</a></p>
<h1 id="加载预训练模型"><a href="#加载预训练模型" class="headerlink" title="加载预训练模型"></a>加载预训练模型</h1><p>这一部分的参考文献有：<br>-<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/KI-9z7FBjfoWfZK3PEPXJA">Pytorch深度学习实战教程（一）：语义分割基础与环境搭建</a><br>-<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/6tZVUbyEjLVewM8vGK9Zhw">Pytorch深度学习实战教程（二）：UNet语义分割网络</a><br>-<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37477175/article/details/83861678">UNet以ResNet34为backbone in keras</a><br>-<a target="_blank" rel="noopener" href="https://www.qbitai.com/2019/05/2157.html">一大波PyTorch图像分割模型来袭，俄罗斯程序员出品新model zoo</a></p>
<p>这个notebook没有用UNet传统的编码器和解码器，而是用的预训练的resnet18网络作为编码器，再在此基础上，构建相应的解码器。这个带预训练resnet18的UNet是借用了这个开源库<a target="_blank" rel="noopener" href="https://github.com/qubvel/segmentation_models.pytorch">segmentation_models.pytorch</a>。该库目前提供了5种模型结构，每个架构有46种可用的编码器，且所有的编码器都具有预训练权重，因此广受好评。以上内容的具体原理可以参见上面的参考文献的描述。</p>
<p>在原notebook中，Rishabh Agrahari在Kaggle的kernel中没有正确通过pip安装这个库，所以他把整个库下载下来然后传到kaggle数据集中了，但实测在Colab中可以通过pip成功安装，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install git+https://github.com/qubvel/segmentation_models.pytorch</span><br></pre></td></tr></table></figure>
<p>测试一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> segmentation_models_pytorch <span class="keyword">as</span> smp</span><br><span class="line">model = smp.Unet()</span><br></pre></td></tr></table></figure>
<p>会显示成功下载预训练模型resnet34，这是因为没有给它传参，默认下载并使用该模型。<br>另外，如果下一次再使用该notebook时，会发现之前安装的包都没了，此时需要永久安装这些python包，见下面的参考文献：<br>-<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/55253498/how-do-i-install-a-library-permanently-in-colab">How do I install a library permanently in Colab?</a></p>
<h1 id="导入必要的Python包"><a href="#导入必要的Python包" class="headerlink" title="导入必要的Python包"></a>导入必要的Python包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm_notebook <span class="keyword">as</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> ReduceLROnPlateau</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset, sampler</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> albumentations <span class="keyword">import</span> (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)</span><br><span class="line"><span class="keyword">from</span> albumentations.pytorch <span class="keyword">import</span> ToTensor</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line">seed = <span class="number">69</span></span><br><span class="line">random.seed(seed)</span><br><span class="line">os.environ[<span class="string">&quot;PYTHONHASHSEED&quot;</span>] = <span class="built_in">str</span>(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line">torch.cuda.manual_seed(seed)</span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<h1 id="掩膜的编码及解码"><a href="#掩膜的编码及解码" class="headerlink" title="掩膜的编码及解码"></a>掩膜的编码及解码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mask2rle</span>(<span class="params">img</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    img: numpy array, 1 -&gt; mask, 0 -&gt; background</span></span><br><span class="line"><span class="string">    Returns run length as string formated</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    pixels= img.T.flatten()</span><br><span class="line">    pixels = np.concatenate([[<span class="number">0</span>], pixels, [<span class="number">0</span>]])</span><br><span class="line">    runs = np.where(pixels[<span class="number">1</span>:] != pixels[:-<span class="number">1</span>])[<span class="number">0</span>] + <span class="number">1</span></span><br><span class="line">    runs[<span class="number">1</span>::<span class="number">2</span>] -= runs[::<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(<span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> runs)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_mask</span>(<span class="params">row_id, df</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Given a row index, return image_id and mask (256, 1600, 4) from the dataframe `df`&#x27;&#x27;&#x27;</span></span><br><span class="line">    fname = df.iloc[row_id].name</span><br><span class="line">    labels = df.iloc[row_id][:<span class="number">4</span>]</span><br><span class="line">    masks = np.zeros((<span class="number">256</span>, <span class="number">1600</span>, <span class="number">4</span>), dtype=np.float32) <span class="comment"># float32 is V.Imp</span></span><br><span class="line">    <span class="comment"># 4:class 1～4 (ch:0～3)</span></span><br><span class="line">    <span class="keyword">for</span> idx, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels.values):</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> np.nan:</span><br><span class="line">            label = label.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            positions = <span class="built_in">map</span>(<span class="built_in">int</span>, label[<span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line">            length = <span class="built_in">map</span>(<span class="built_in">int</span>, label[<span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line">            mask = np.zeros(<span class="number">256</span> * <span class="number">1600</span>, dtype=np.uint8)</span><br><span class="line">            <span class="keyword">for</span> pos, le <span class="keyword">in</span> <span class="built_in">zip</span>(positions, length):</span><br><span class="line">                mask[pos:(pos + le)] = <span class="number">1</span></span><br><span class="line">            masks[:, :, idx] = mask.reshape(<span class="number">256</span>, <span class="number">1600</span>, order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> fname, masks</span><br></pre></td></tr></table></figure>
<p>这一步是定义了两个函数：<br>mask2rle函数是将mask使用RLE编码，RLE的全称是Run-length encoding，称为游程编码，是一种无损数据压缩技术。<br>make_mask是为了对csv文件中mask进行RLE解码。<br>比如’1 3’代表起点是像素1，然后长度为3个像素，即像素索引为(1, 2, 3)。然后不同游程之间也是用空格分隔，比如’1 3 10 5’代表的就是(1, 2, 3, 10, 11, 12, 13, 14)这8个像素。</p>
<p>参考资料：<br>-<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%B8%B8%E7%A8%8B%E7%BC%96%E7%A0%81">游程编码</a><br>-<a target="_blank" rel="noopener" href="https://blog.csdn.net/appleyuchi/article/details/102938491">对mask进行rle编码然后进行解码-详细注释</a><br>-<a target="_blank" rel="noopener" href="https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode">RLE functions - Run Lenght Encode &amp; Decode</a></p>
<p>#数据加载</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SteelDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, df, data_folder, mean, std, phase</span>):</span></span><br><span class="line">        self.df = df</span><br><span class="line">        self.root = data_folder</span><br><span class="line">        self.mean = mean</span><br><span class="line">        self.std = std</span><br><span class="line">        self.phase = phase</span><br><span class="line">        self.transforms = get_transforms(phase, mean, std)</span><br><span class="line">        self.fnames = self.df.index.tolist()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        image_id, mask = make_mask(idx, self.df)</span><br><span class="line">        image_path = os.path.join(self.root, <span class="string">&quot;train_images&quot;</span>,  image_id)</span><br><span class="line">        img = cv2.imread(image_path)</span><br><span class="line">        augmented = self.transforms(image=img, mask=mask)</span><br><span class="line">        img = augmented[<span class="string">&#x27;image&#x27;</span>]</span><br><span class="line">        mask = augmented[<span class="string">&#x27;mask&#x27;</span>] <span class="comment"># 1x256x1600x4</span></span><br><span class="line">        mask = mask[<span class="number">0</span>].permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>) <span class="comment"># 4x256x1600</span></span><br><span class="line">        <span class="keyword">return</span> img, mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.fnames)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_transforms</span>(<span class="params">phase, mean, std</span>):</span></span><br><span class="line">    list_transforms = []</span><br><span class="line">    <span class="keyword">if</span> phase == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">        list_transforms.extend(</span><br><span class="line">            [</span><br><span class="line">                HorizontalFlip(p=<span class="number">0.5</span>), <span class="comment"># only horizontal flip as of now</span></span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">    list_transforms.extend(</span><br><span class="line">        [</span><br><span class="line">            Normalize(mean=mean, std=std, p=<span class="number">1</span>),</span><br><span class="line">            ToTensor(),</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">    list_trfms = Compose(list_transforms)</span><br><span class="line">    <span class="keyword">return</span> list_trfms</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">provider</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">    data_folder,</span></span></span><br><span class="line"><span class="function"><span class="params">    df_path,</span></span></span><br><span class="line"><span class="function"><span class="params">    phase,</span></span></span><br><span class="line"><span class="function"><span class="params">    mean=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    std=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    num_workers=<span class="number">4</span>,</span></span></span><br><span class="line"><span class="function"><span class="params"></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Returns dataloader for the model training&#x27;&#x27;&#x27;</span></span><br><span class="line">    df = pd.read_csv(df_path)</span><br><span class="line">    <span class="comment"># https://www.kaggle.com/amanooo/defect-detection-starter-u-net</span></span><br><span class="line">    df[<span class="string">&#x27;ImageId&#x27;</span>], df[<span class="string">&#x27;ClassId&#x27;</span>] = <span class="built_in">zip</span>(*df[<span class="string">&#x27;ImageId_ClassId&#x27;</span>].<span class="built_in">str</span>.split(<span class="string">&#x27;_&#x27;</span>))</span><br><span class="line">    df[<span class="string">&#x27;ClassId&#x27;</span>] = df[<span class="string">&#x27;ClassId&#x27;</span>].astype(<span class="built_in">int</span>)</span><br><span class="line">    df = df.pivot(index=<span class="string">&#x27;ImageId&#x27;</span>,columns=<span class="string">&#x27;ClassId&#x27;</span>,values=<span class="string">&#x27;EncodedPixels&#x27;</span>)</span><br><span class="line">    df[<span class="string">&#x27;defects&#x27;</span>] = df.count(axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    train_df, val_df = train_test_split(df, test_size=<span class="number">0.2</span>, stratify=df[<span class="string">&quot;defects&quot;</span>], random_state=<span class="number">69</span>)</span><br><span class="line">    df = train_df <span class="keyword">if</span> phase == <span class="string">&quot;train&quot;</span> <span class="keyword">else</span> val_df</span><br><span class="line">    image_dataset = SteelDataset(df, data_folder, mean, std, phase)</span><br><span class="line">    dataloader = DataLoader(</span><br><span class="line">        image_dataset,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        num_workers=num_workers,</span><br><span class="line">        pin_memory=<span class="literal">True</span>,</span><br><span class="line">        shuffle=<span class="literal">True</span>,   </span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure>

<p>这一步是依据PyTorch的规范建立自己的数据集、数据变换、数据加载器等。<br>具体可以参考官方的创建自定义数据集及加载器一节：<br><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html">Writing Custom Datasets, DataLoaders and Transforms</a></p>
<h1 id="衡量指标定义"><a href="#衡量指标定义" class="headerlink" title="衡量指标定义"></a>衡量指标定义</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">X, threshold</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;X is sigmoid output of the model&#x27;&#x27;&#x27;</span></span><br><span class="line">    X_p = np.copy(X)</span><br><span class="line">    preds = (X_p &gt; threshold).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> preds</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">metric</span>(<span class="params">probability, truth, threshold=<span class="number">0.5</span>, reduction=<span class="string">&#x27;none&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Calculates dice of positive and negative images seperately&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;probability and truth must be torch tensors&#x27;&#x27;&#x27;</span></span><br><span class="line">    batch_size = <span class="built_in">len</span>(truth)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        probability = probability.view(batch_size, -<span class="number">1</span>)</span><br><span class="line">        truth = truth.view(batch_size, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">assert</span>(probability.shape == truth.shape)</span><br><span class="line">        p = (probability &gt; threshold).<span class="built_in">float</span>()</span><br><span class="line">        t = (truth &gt; <span class="number">0.5</span>).<span class="built_in">float</span>()</span><br><span class="line">        t_sum = t.<span class="built_in">sum</span>(-<span class="number">1</span>)</span><br><span class="line">        p_sum = p.<span class="built_in">sum</span>(-<span class="number">1</span>)</span><br><span class="line">        neg_index = torch.nonzero(t_sum == <span class="number">0</span>)</span><br><span class="line">        pos_index = torch.nonzero(t_sum &gt;= <span class="number">1</span>)</span><br><span class="line">        dice_neg = (p_sum == <span class="number">0</span>).<span class="built_in">float</span>()</span><br><span class="line">        dice_pos = <span class="number">2</span> * (p*t).<span class="built_in">sum</span>(-<span class="number">1</span>)/((p+t).<span class="built_in">sum</span>(-<span class="number">1</span>))</span><br><span class="line">        dice_neg = dice_neg[neg_index]</span><br><span class="line">        dice_pos = dice_pos[pos_index]</span><br><span class="line">        dice = torch.cat([dice_pos, dice_neg])</span><br><span class="line"><span class="comment">#         dice_neg = np.nan_to_num(dice_neg.mean().item(), 0)</span></span><br><span class="line"><span class="comment">#         dice_pos = np.nan_to_num(dice_pos.mean().item(), 0)</span></span><br><span class="line"><span class="comment">#         dice = dice.mean().item()</span></span><br><span class="line"></span><br><span class="line">        num_neg = <span class="built_in">len</span>(neg_index)</span><br><span class="line">        num_pos = <span class="built_in">len</span>(pos_index)</span><br><span class="line">    <span class="keyword">return</span> dice, dice_neg, dice_pos, num_neg, num_pos</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Meter</span>:</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;A meter to keep track of iou and dice scores throughout an epoch&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, phase, epoch</span>):</span></span><br><span class="line">        self.base_threshold = <span class="number">0.5</span> <span class="comment"># &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; here&#x27;s the threshold</span></span><br><span class="line">        self.base_dice_scores = []</span><br><span class="line">        self.dice_neg_scores = []</span><br><span class="line">        self.dice_pos_scores = []</span><br><span class="line">        self.iou_scores = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, targets, outputs</span>):</span></span><br><span class="line">        probs = torch.sigmoid(outputs)</span><br><span class="line">        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)</span><br><span class="line">        self.base_dice_scores.extend(dice.tolist())</span><br><span class="line">        self.dice_pos_scores.extend(dice_pos.tolist())</span><br><span class="line">        self.dice_neg_scores.extend(dice_neg.tolist())</span><br><span class="line">        preds = predict(probs, self.base_threshold)</span><br><span class="line">        iou = compute_iou_batch(preds, targets, classes=[<span class="number">1</span>])</span><br><span class="line">        self.iou_scores.append(iou)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span>(<span class="params">self</span>):</span></span><br><span class="line">        dice = np.nanmean(self.base_dice_scores)</span><br><span class="line">        dice_neg = np.nanmean(self.dice_neg_scores)</span><br><span class="line">        dice_pos = np.nanmean(self.dice_pos_scores)</span><br><span class="line">        dices = [dice, dice_neg, dice_pos]</span><br><span class="line">        iou = np.nanmean(self.iou_scores)</span><br><span class="line">        <span class="keyword">return</span> dices, iou</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_log</span>(<span class="params">phase, epoch, epoch_loss, meter, start</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;logging the metrics at the end of an epoch&#x27;&#x27;&#x27;</span></span><br><span class="line">    dices, iou = meter.get_metrics()</span><br><span class="line">    dice, dice_neg, dice_pos = dices</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Loss: %0.4f | IoU: %0.4f | dice: %0.4f | dice_neg: %0.4f | dice_pos: %0.4f&quot;</span> % (epoch_loss, iou, dice, dice_neg, dice_pos))</span><br><span class="line">    <span class="keyword">return</span> dice, iou</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_ious</span>(<span class="params">pred, label, classes, ignore_index=<span class="number">255</span>, only_present=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;computes iou for one ground truth mask and predicted mask&#x27;&#x27;&#x27;</span></span><br><span class="line">    pred[label == ignore_index] = <span class="number">0</span></span><br><span class="line">    ious = []</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> classes:</span><br><span class="line">        label_c = label == c</span><br><span class="line">        <span class="keyword">if</span> only_present <span class="keyword">and</span> np.<span class="built_in">sum</span>(label_c) == <span class="number">0</span>:</span><br><span class="line">            ious.append(np.nan)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        pred_c = pred == c</span><br><span class="line">        intersection = np.logical_and(pred_c, label_c).<span class="built_in">sum</span>()</span><br><span class="line">        union = np.logical_or(pred_c, label_c).<span class="built_in">sum</span>()</span><br><span class="line">        <span class="keyword">if</span> union != <span class="number">0</span>:</span><br><span class="line">            ious.append(intersection / union)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ious <span class="keyword">if</span> ious <span class="keyword">else</span> [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_iou_batch</span>(<span class="params">outputs, labels, classes=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;computes mean iou for a batch of ground truth masks and predicted masks&#x27;&#x27;&#x27;</span></span><br><span class="line">    ious = []</span><br><span class="line">    preds = np.copy(outputs) <span class="comment"># copy is imp</span></span><br><span class="line">    labels = np.array(labels) <span class="comment"># tensor to np</span></span><br><span class="line">    <span class="keyword">for</span> pred, label <span class="keyword">in</span> <span class="built_in">zip</span>(preds, labels):</span><br><span class="line">        ious.append(np.nanmean(compute_ious(pred, label, classes)))</span><br><span class="line">    iou = np.nanmean(ious)</span><br><span class="line">    <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure>
<p>这一部分是计算衡量指标Dice和IoU。<br>各个指标的意义可以参考如下资料：<br><a target="_blank" rel="noopener" href="https://www.zdaiot.com/MachineLearning/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/">图像分割评价指标</a></p>
<h1 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> segmentation_models_pytorch <span class="keyword">as</span> smp</span><br><span class="line">model = smp.Unet(<span class="string">&quot;resnet18&quot;</span>, encoder_weights=<span class="string">&quot;imagenet&quot;</span>, classes=<span class="number">4</span>, activation=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>对UNet网络设定具体的参数，如backbone选择resnet18，预训练权重为imagenet，四分类，无激活函数等。<br>可以具体看一下该模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model</span><br></pre></td></tr></table></figure>
<p>输出结果太长了，不再具体显示。</p>
<h1 id="模型训练和验证"><a href="#模型训练和验证" class="headerlink" title="模型训练和验证"></a>模型训练和验证</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">class Trainer(object):</span><br><span class="line"></span><br><span class="line">    &#39;&#39;&#39;This class takes care of training and validation of our model&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">    def __init__(self, model):</span><br><span class="line">        self.num_workers &#x3D; 6</span><br><span class="line">        self.batch_size &#x3D; &#123;&quot;train&quot;: 4, &quot;val&quot;: 4&#125;</span><br><span class="line">        self.accumulation_steps &#x3D; 32 &#x2F;&#x2F; self.batch_size[&#39;train&#39;]</span><br><span class="line">        self.lr &#x3D; 5e-4</span><br><span class="line">        self.num_epochs &#x3D; 20</span><br><span class="line">        self.best_loss &#x3D; float(&quot;inf&quot;)</span><br><span class="line">        self.phases &#x3D; [&quot;train&quot;, &quot;val&quot;]</span><br><span class="line">        self.device &#x3D; torch.device(&quot;cuda:0&quot;)</span><br><span class="line">        torch.set_default_tensor_type(&quot;torch.cuda.FloatTensor&quot;)</span><br><span class="line">        self.net &#x3D; model</span><br><span class="line">        self.criterion &#x3D; torch.nn.BCEWithLogitsLoss()</span><br><span class="line">        self.optimizer &#x3D; optim.Adam(self.net.parameters(), lr&#x3D;self.lr)</span><br><span class="line">        self.scheduler &#x3D; ReduceLROnPlateau(self.optimizer, mode&#x3D;&quot;min&quot;, patience&#x3D;3, verbose&#x3D;True)</span><br><span class="line">        self.net &#x3D; self.net.to(self.device)</span><br><span class="line">        cudnn.benchmark &#x3D; True</span><br><span class="line">        self.dataloaders &#x3D; &#123;</span><br><span class="line">            phase: provider(</span><br><span class="line">                data_folder&#x3D;data_folder,</span><br><span class="line">                df_path&#x3D;train_df_path,</span><br><span class="line">                phase&#x3D;phase,</span><br><span class="line">                mean&#x3D;(0.485, 0.456, 0.406),</span><br><span class="line">                std&#x3D;(0.229, 0.224, 0.225),</span><br><span class="line">                batch_size&#x3D;self.batch_size[phase],</span><br><span class="line">                num_workers&#x3D;self.num_workers,</span><br><span class="line">            )</span><br><span class="line">            for phase in self.phases</span><br><span class="line">        &#125;</span><br><span class="line">        self.losses &#x3D; &#123;phase: [] for phase in self.phases&#125;</span><br><span class="line">        self.iou_scores &#x3D; &#123;phase: [] for phase in self.phases&#125;</span><br><span class="line">        self.dice_scores &#x3D; &#123;phase: [] for phase in self.phases&#125;</span><br><span class="line"></span><br><span class="line">    def forward(self, images, targets):</span><br><span class="line">        images &#x3D; images.to(self.device)</span><br><span class="line">        masks &#x3D; targets.to(self.device)</span><br><span class="line">        outputs &#x3D; self.net(images)</span><br><span class="line">        loss &#x3D; self.criterion(outputs, masks)</span><br><span class="line">        return loss, outputs</span><br><span class="line"></span><br><span class="line">    def iterate(self, epoch, phase):</span><br><span class="line">        meter &#x3D; Meter(phase, epoch)</span><br><span class="line">        start &#x3D; time.strftime(&quot;%H:%M:%S&quot;)</span><br><span class="line">        print(f&quot;Starting epoch: &#123;epoch&#125; | phase: &#123;phase&#125; | ⏰: &#123;start&#125;&quot;)</span><br><span class="line">        batch_size &#x3D; self.batch_size[phase]</span><br><span class="line">        self.net.train(phase &#x3D;&#x3D; &quot;train&quot;)</span><br><span class="line">        dataloader &#x3D; self.dataloaders[phase]</span><br><span class="line">        running_loss &#x3D; 0.0</span><br><span class="line">        total_batches &#x3D; len(dataloader)</span><br><span class="line">#         tk0 &#x3D; tqdm(dataloader, total&#x3D;total_batches)</span><br><span class="line"></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        for itr, batch in enumerate(dataloader): # replace &#96;dataloader&#96; with &#96;tk0&#96; for tqdm</span><br><span class="line">            images, targets &#x3D; batch</span><br><span class="line">            loss, outputs &#x3D; self.forward(images, targets)</span><br><span class="line">            loss &#x3D; loss &#x2F; self.accumulation_steps</span><br><span class="line">            if phase &#x3D;&#x3D; &quot;train&quot;:</span><br><span class="line">                loss.backward()</span><br><span class="line"></span><br><span class="line">                if (itr + 1 ) % self.accumulation_steps &#x3D;&#x3D; 0:</span><br><span class="line">                    self.optimizer.step()</span><br><span class="line">                    self.optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            running_loss +&#x3D; loss.item()</span><br><span class="line">            outputs &#x3D; outputs.detach().cpu()</span><br><span class="line">            meter.update(targets, outputs)</span><br><span class="line"></span><br><span class="line">#             tk0.set_postfix(loss&#x3D;(running_loss &#x2F; ((itr + 1))))</span><br><span class="line"></span><br><span class="line">        epoch_loss &#x3D; (running_loss * self.accumulation_steps) &#x2F; total_batches</span><br><span class="line">        dice, iou &#x3D; epoch_log(phase, epoch, epoch_loss, meter, start)</span><br><span class="line">        self.losses[phase].append(epoch_loss)</span><br><span class="line">        self.dice_scores[phase].append(dice)</span><br><span class="line">        self.iou_scores[phase].append(iou)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line">        return epoch_loss</span><br><span class="line"></span><br><span class="line">    def start(self):</span><br><span class="line">        for epoch in range(self.num_epochs):</span><br><span class="line">            self.iterate(epoch, &quot;train&quot;)</span><br><span class="line">            state &#x3D; &#123;</span><br><span class="line">                &quot;epoch&quot;: epoch,</span><br><span class="line">                &quot;best_loss&quot;: self.best_loss,</span><br><span class="line">                &quot;state_dict&quot;: self.net.state_dict(),</span><br><span class="line">                &quot;optimizer&quot;: self.optimizer.state_dict(),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            with torch.no_grad():</span><br><span class="line">                val_loss &#x3D; self.iterate(epoch, &quot;val&quot;)</span><br><span class="line">                self.scheduler.step(val_loss)</span><br><span class="line"></span><br><span class="line">            if val_loss &lt; self.best_loss:</span><br><span class="line">                print(&quot;******** New optimal found, saving state ********&quot;)</span><br><span class="line">                state[&quot;best_loss&quot;] &#x3D; self.best_loss &#x3D; val_loss</span><br><span class="line">                torch.save(state, &quot;.&#x2F;model.pth&quot;)</span><br><span class="line">            print()</span><br></pre></td></tr></table></figure>

<p>定义了Trainer类，用来处理模型的训练和验证。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sample_submission_path = <span class="string">&#x27;/content/drive/My Drive/severstal/sample_submission.csv&#x27;</span></span><br><span class="line">train_df_path = <span class="string">&#x27;/content/drive/My Drive/severstal/train.csv&#x27;</span></span><br><span class="line">data_folder = <span class="string">&quot;/content/drive/My Drive/severstal&quot;</span></span><br><span class="line">test_data_folder = <span class="string">&quot;/content/drive/My Drive/severstal/test_images&quot;</span></span><br></pre></td></tr></table></figure>
<p>设定一系列路径。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_trainer = Trainer(model)</span><br><span class="line">model_trainer.start()</span><br></pre></td></tr></table></figure>

<p>下面就进入漫长的训练和验证阶段，因为这里是分析源码功能，不对精确性做考虑，所以这里将默认的20个epoches改为了2个，输出结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Starting epoch: <span class="number">0</span> | phase: train | ⏰: 03:<span class="number">39</span>:08</span><br><span class="line">Loss: <span class="number">0.0386</span> | IoU: <span class="number">0.1436</span> | dice: <span class="number">0.3747</span> | dice_neg: <span class="number">0.5761</span> | dice_pos: <span class="number">0.1963</span></span><br><span class="line">Starting epoch: <span class="number">0</span> | phase: val | ⏰: 03:<span class="number">59</span>:<span class="number">18</span></span><br><span class="line">Loss: <span class="number">0.0205</span> | IoU: <span class="number">0.3210</span> | dice: <span class="number">0.5299</span> | dice_neg: <span class="number">0.6596</span> | dice_pos: <span class="number">0.4149</span></span><br><span class="line">******** New optimal found, saving state ********</span><br><span class="line"></span><br><span class="line">Starting epoch: <span class="number">1</span> | phase: train | ⏰: 04:03:09</span><br><span class="line">Loss: <span class="number">0.0196</span> | IoU: <span class="number">0.2905</span> | dice: <span class="number">0.5555</span> | dice_neg: <span class="number">0.7579</span> | dice_pos: <span class="number">0.3763</span></span><br><span class="line">Starting epoch: <span class="number">1</span> | phase: val | ⏰: 04:<span class="number">21</span>:04</span><br><span class="line">Loss: <span class="number">0.0166</span> | IoU: <span class="number">0.2829</span> | dice: <span class="number">0.6151</span> | dice_neg: <span class="number">0.9035</span> | dice_pos: <span class="number">0.3596</span></span><br><span class="line">******** New optimal found, saving state ********</span><br></pre></td></tr></table></figure>

<p>可以看出，一个epoch大约需要25分钟时间。<br>查看一下此时Colab所使用的GPU：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.get_device_name(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>可以看出此时是Tesla T4，Google还挺给力。<br>如果是需要长时间的训练，当前Google Colab还有一些小tricks，比如最好不要关闭浏览器窗口，因为关闭后90mins后就会该实例就会被切断。但一直保持浏览器窗口也只能最多训练12 hours。见：<br><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/55050988/can-i-run-a-google-colab-free-edition-script-and-then-shutdown-my-computer">Can i run a google colab (free edition) script and then shutdown my computer?</a><br>也有网友分享了一个简单的JS函数来模拟点击，自动重连，见：<br><a target="_blank" rel="noopener" href="https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0">How to prevent Google Colab from disconnecting?</a></p>
<h1 id="得分作图"><a href="#得分作图" class="headerlink" title="得分作图"></a>得分作图</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PLOT TRAINING</span></span><br><span class="line">losses = model_trainer.losses</span><br><span class="line">dice_scores = model_trainer.dice_scores <span class="comment"># overall dice</span></span><br><span class="line">iou_scores = model_trainer.iou_scores</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span>(<span class="params">scores, name</span>):</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(scores[<span class="string">&quot;train&quot;</span>])), scores[<span class="string">&quot;train&quot;</span>], label=<span class="string">f&#x27;train <span class="subst">&#123;name&#125;</span>&#x27;</span>)</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(scores[<span class="string">&quot;train&quot;</span>])), scores[<span class="string">&quot;val&quot;</span>], label=<span class="string">f&#x27;val <span class="subst">&#123;name&#125;</span>&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span> plot&#x27;</span>); plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>); plt.ylabel(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span>&#x27;</span>);</span><br><span class="line">    plt.legend(); </span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot(losses, <span class="string">&quot;BCE loss&quot;</span>)</span><br><span class="line">plot(dice_scores, <span class="string">&quot;Dice score&quot;</span>)</span><br><span class="line">plot(iou_scores, <span class="string">&quot;IoU score&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>如图，因为仅训练了2个epoches，所以这里的作图只是展示一下。<br><img src="https://user-images.githubusercontent.com/6218739/74802202-19787f00-5314-11ea-8ec0-bbe861ccfd1e.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74802207-1ed5c980-5314-11ea-88f3-f04125adf692.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74802212-2301e700-5314-11ea-83b6-d1a22c538149.png" alt="image"></p>
<h1 id="推理和提交"><a href="#推理和提交" class="headerlink" title="推理和提交"></a>推理和提交</h1><p>因为原文中作者是在Kaggle GPU上进行训练，整个训练过程约400分钟，超过了Kaggle的60min的限制，所以作者没有在这个notebook中进行推理和提交。他又写了两个一个notebook来进行后面的推理和提交，见<a target="_blank" rel="noopener" href="https://www.kaggle.com/rishabhiitbhu/unet-pytorch-inference-kernel">这里</a>。</p>
<p>因为这里没有实际的训练，所以这里将这一个后续的notebook也直接附在这里。</p>
<h2 id="定义测试数据集"><a href="#定义测试数据集" class="headerlink" title="定义测试数据集"></a>定义测试数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Dataset for test prediction&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, root, df, mean, std</span>):</span></span><br><span class="line">        self.root = root</span><br><span class="line">        df[<span class="string">&#x27;ImageId&#x27;</span>] = df[<span class="string">&#x27;ImageId_ClassId&#x27;</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">        self.fnames = df[<span class="string">&#x27;ImageId&#x27;</span>].unique().tolist()</span><br><span class="line">        self.num_samples = <span class="built_in">len</span>(self.fnames)</span><br><span class="line">        self.transform = Compose(</span><br><span class="line">            [</span><br><span class="line">                Normalize(mean=mean, std=std, p=<span class="number">1</span>),</span><br><span class="line">                ToTensor(),</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        fname = self.fnames[idx]</span><br><span class="line">        path = os.path.join(self.root, fname)</span><br><span class="line">        image = cv2.imread(path)</span><br><span class="line">        images = self.transform(image=image)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line">        <span class="keyword">return</span> fname, images</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.num_samples</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize test dataloader</span></span><br><span class="line">best_threshold = <span class="number">0.5</span></span><br><span class="line">num_workers = <span class="number">2</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;best_threshold&#x27;</span>, best_threshold)</span><br><span class="line">min_size = <span class="number">3500</span></span><br><span class="line">mean = (<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>)</span><br><span class="line">std = (<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>)</span><br><span class="line">df = pd.read_csv(sample_submission_path)</span><br><span class="line">testset = DataLoader(</span><br><span class="line">    TestDataset(test_data_folder, df, mean, std),</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="literal">False</span>,</span><br><span class="line">    num_workers=num_workers,</span><br><span class="line">    pin_memory=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为测试集创建PyTorch规范的Dataset和DataLoader。</p>
<h2 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post_process</span>(<span class="params">probability, threshold, min_size</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Post processing of each predicted mask, components with lesser number of pixels</span></span><br><span class="line"><span class="string">    than `min_size` are ignored&#x27;&#x27;&#x27;</span></span><br><span class="line">    mask = cv2.threshold(probability, threshold, <span class="number">1</span>, cv2.THRESH_BINARY)[<span class="number">1</span>]</span><br><span class="line">    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))</span><br><span class="line">    predictions = np.zeros((<span class="number">256</span>, <span class="number">1600</span>), np.float32)</span><br><span class="line"></span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_component):</span><br><span class="line">        p = (component == c)</span><br><span class="line">        <span class="keyword">if</span> p.<span class="built_in">sum</span>() &gt; min_size:</span><br><span class="line">            predictions[p] = <span class="number">1</span></span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> predictions, num</span><br></pre></td></tr></table></figure>
<p>这一步是对掩膜进行后处理。</p>
<h2 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize mode and load trained weights</span></span><br><span class="line">ckpt_path = <span class="string">&quot;model.pth&quot;</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">model = smp.Unet(<span class="string">&quot;resnet18&quot;</span>, encoder_weights=<span class="literal">None</span>, classes=<span class="number">4</span>, activation=<span class="literal">None</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">state = torch.load(ckpt_path, map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line">model.load_state_dict(state[<span class="string">&quot;state_dict&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>这一步是初始化模型及加载模型。这也是持久化的需求。<br>该模型文件pth是之前训练时自动存储的。</p>
<h2 id="推理并生成提交文件"><a href="#推理并生成提交文件" class="headerlink" title="推理并生成提交文件"></a>推理并生成提交文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># start prediction</span></span><br><span class="line">predictions = []</span><br><span class="line"><span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(testset)):</span><br><span class="line">    fnames, images = batch</span><br><span class="line">    batch_preds = torch.sigmoid(model(images.to(device)))</span><br><span class="line">    batch_preds = batch_preds.detach().cpu().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> fname, preds <span class="keyword">in</span> <span class="built_in">zip</span>(fnames, batch_preds):</span><br><span class="line">        <span class="keyword">for</span> cls, pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(preds):</span><br><span class="line">            pred, num = post_process(pred, best_threshold, min_size)</span><br><span class="line">            rle = mask2rle(pred)</span><br><span class="line">            name = fname + <span class="string">f&quot;_<span class="subst">&#123;cls+<span class="number">1</span>&#125;</span>&quot;</span></span><br><span class="line">            predictions.append([name, rle])</span><br><span class="line"></span><br><span class="line"><span class="comment"># save predictions to submission.csv</span></span><br><span class="line">df = pd.DataFrame(predictions, columns=[<span class="string">&#x27;ImageId_ClassId&#x27;</span>, <span class="string">&#x27;EncodedPixels&#x27;</span>])</span><br><span class="line">df.to_csv(<span class="string">&quot;submission.csv&quot;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>可以发现，此时当前目录下回生成submission.csv文件，里面的掩膜信息也是用RLE游程编码的。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2020/02/15/kaggle-steel/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Be interesting!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="亓欣波">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/15/kaggle-steel/" class="post-title-link" itemprop="url">Kaggle钢铁赛：基于PyTorch/UNet算法的钢材表面缺陷检测——（1）数据集分析和可视化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-15 00:00:00" itemprop="dateCreated datePublished" datetime="2020-02-15T00:00:00+08:00">2020-02-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-25 15:20:30" itemprop="dateModified" datetime="2021-03-25T15:20:30+08:00">2021-03-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">programming</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/02/15/kaggle-steel/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/02/15/kaggle-steel/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>Kaggle上有一个<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/severstal-steel-defect-detection/overview">钢材表面缺陷检测的竞赛</a>，是一个很好的将深度学习应用于传统材料检测的例子。对该赛题和解法的剖析，可以辅助理解深度学习的流程，及其应用于具体问题的套路。</p>
<p>这次解析分为两部分：<br>（1）第一部分，即本文，是一个预备性工作，即对该竞赛的数据集的分析和可视化，参考的是这个notebook——<a target="_blank" rel="noopener" href="https://www.kaggle.com/go1dfish/clear-mask-visualization-and-simple-eda">clear mask visualization and simple eda</a>。感谢GoldFish的分享。<br>（2）第二部分，参见<a target="_blank" rel="noopener" href="https://qixinbo.info/2020/02/16/kaggle-steel-2/">另一篇文章</a>，即算法分析，参考的是Rishabh Agrahari的<a target="_blank" rel="noopener" href="https://www.kaggle.com/rishabhiitbhu/unet-starter-kernel-pytorch-lb-0-88">使用PyTorch框架及UNet算法的notebook</a>。</p>
<p>其他参考文献：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://kakack.github.io/2017/09/%E8%BD%AC-Kaggle%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/">kaggle数据挖掘比赛基本流程</a></li>
<li><a target="_blank" rel="noopener" href="https://medium.com/pyradise/jupyter-notebook-tricks-kaggle-public-api-a578b99341d0">Jupyter Notebook 使用技巧彙整：與 Kaggle 資料集互動</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/setting-up-kaggle-in-google-colab-ebb281b61463">Setting Up Kaggle in Google Colab</a></li>
</ul>
<h1 id="将kaggle数据迁移到Google-Colab"><a href="#将kaggle数据迁移到Google-Colab" class="headerlink" title="将kaggle数据迁移到Google Colab"></a>将kaggle数据迁移到Google Colab</h1><p>这一步其实不用做，可以将数据集直接下载下来，用自己的电脑训练。但此时中国境内新型非冠病毒肆虐，按要求在家隔离（希望这场疫情赶紧过去，中国加油！），手头只有一个工作用的笔记本，无法胜任该训练任务。所以考虑云端训练。<br>（备注：使用Google Colab需要自备梯子）<br>当然也可以使用Kaggle的notebook，但此时发现在Kaggle运行notebook非常慢，根本加载不出来。而Google Colab跟Kaggle是一家，Colab中GPU训练也很方便快捷，同时迁移数据速度也很快，所以做这一步在当下是一个很好的选择。</p>
<h2 id="获取Kaggle账户的Token"><a href="#获取Kaggle账户的Token" class="headerlink" title="获取Kaggle账户的Token"></a>获取Kaggle账户的Token</h2><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kaggle-&gt; my account-&gt;Create New API Token</span><br></pre></td></tr></table></figure>
<h2 id="将该文件上传到Google-Colab的root账户下"><a href="#将该文件上传到Google-Colab的root账户下" class="headerlink" title="将该文件上传到Google Colab的root账户下"></a>将该文件上传到Google Colab的root账户下</h2><p>打开一个Colab notebook，然后：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">!mkdir /root/.kaggle</span><br><span class="line">token = &#123;<span class="string">&quot;username&quot;</span>: <span class="string">&quot;YOUR-USERNAME&quot;</span>, <span class="string">&quot;key&quot;</span>: <span class="string">&quot;YOUR-KEY&quot;</span>&#125;</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/root/.kaggle/kaggle.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    json.dump(token, file)</span><br><span class="line"></span><br><span class="line">!chmod <span class="number">600</span> /root/.kaggle/kaggle.json</span><br></pre></td></tr></table></figure>

<h2 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h2><p>第一种方法：（Attention！该方法下载的数据不全）<br>到kaggle钢铁赛的Data一栏中找到下载数据集的kaggle API 命令，，即<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/severstal-steel-defect-detection/data">这里</a>，然后在Colab中执行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions download -c severstal-steel-defect-detection</span><br></pre></td></tr></table></figure>
<p>为了保证是下载到/content目录下，最好在该命令后面加上-p /content选项。<br>虽然是官方页面上给出的API，但是下载后发现仅有几十张图片，明显不是完整的数据集。<br>第二种方法：（有效）<br>（1）首先搜索kaggle与Sevelstal有关的数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle datasets <span class="built_in">list</span> -s severstal</span><br></pre></td></tr></table></figure>
<p>此时会列出很多带有该关键字的数据集名称，通过与该竞赛Data页面上的数据集大小对比，发现lyubovrogovaya/severstal数据集大小是2G，所以猜测该数据集是正确的，但实际上下载下来看了一下（10秒下载完成），不是原始的数据集，又重新找了一下，发现duongnh1/severstal 这个数据集是正确的。<br>（2）下载该数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle datasets download -d duongnh1/severstal</span><br></pre></td></tr></table></figure>
<p>也是10秒就下载完了。<br>（3）解压查看该数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!unzip severstal.<span class="built_in">zip</span></span><br><span class="line">!ls severstal</span><br></pre></td></tr></table></figure>
<p>可以发现该数据集包含了赛题中完整的数据集信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_submission.csv test_images train.csv train_images</span><br></pre></td></tr></table></figure>
<p>这四个文件和文件夹的意义分别是：</p>
<ul>
<li>train_images：该文件夹中存储训练图像</li>
<li>test_images：该文件夹中存储测试图像</li>
<li>train.csv：该文件中存储训练图像的缺陷标注，有4类缺陷，ClassId = [1, 2, 3, 4]</li>
<li>sample_submission.csv：该文件是一个上传文件的样例，每个ImageID要有4排，每一排对应一类缺陷</li>
</ul>
<p>（4）将数据集转存到Google Drive中<br>Google会重置在临时的这个空间中存储的数据，因此第二天一看原来下载的数据都没了。所以要把这个数据集转存到Google Drive中。<br>首先要先在左侧Mount Drive，这样就会出现drive这个文件夹，然后：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!mv severstal drive/<span class="string">&quot;My Drive&quot;</span>/</span><br></pre></td></tr></table></figure>

<h1 id="数据集分析"><a href="#数据集分析" class="headerlink" title="数据集分析"></a>数据集分析</h1><p>这一部分主要就是根据这个notebook<a target="_blank" rel="noopener" href="https://www.kaggle.com/go1dfish/clear-mask-visualization-and-simple-eda">clear mask visualization and simple eda</a>来探究的。</p>
<h2 id="加载必要的Python模块"><a href="#加载必要的Python模块" class="headerlink" title="加载必要的Python模块"></a>加载必要的Python模块</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.set_option(<span class="string">&quot;display.max_rows&quot;</span>, <span class="number">101</span>)</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="built_in">print</span>(os.listdir(<span class="string">&quot;drive/My Drive/severstal&quot;</span>))</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">&quot;font.size&quot;</span>] = <span class="number">15</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure>

<h2 id="读取和分析文本数据"><a href="#读取和分析文本数据" class="headerlink" title="读取和分析文本数据"></a>读取和分析文本数据</h2><h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_df = pd.read_csv(<span class="string">&quot;drive/My Drive/severstal/train.csv&quot;</span>)</span><br><span class="line">sample_df = pd.read_csv(<span class="string">&quot;drive/My Drive/severstal/sample_submission.csv&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>初步查看一下里面的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>
<p>结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">     ImageId_ClassId        EncodedPixels</span><br><span class="line"><span class="number">0</span>    0002cc93b.jpg_1        <span class="number">29102</span> <span class="number">12</span> <span class="number">29346</span> <span class="number">24</span> <span class="number">29602</span> <span class="number">24</span> <span class="number">29858</span> <span class="number">24</span> <span class="number">30114</span> <span class="number">24</span> <span class="number">3.</span>..</span><br><span class="line"><span class="number">1</span>    0002cc93b.jpg_2        NaN</span><br><span class="line"><span class="number">2</span>    0002cc93b.jpg_3        NaN</span><br><span class="line"><span class="number">3</span>    0002cc93b.jpg_4        NaN</span><br><span class="line"><span class="number">4</span>    00031f466.jpg_1        NaN</span><br></pre></td></tr></table></figure>
<p>以及sample的开头：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ImageId_ClassId    EncodedPixels</span><br><span class="line"><span class="number">0</span>                  004f40c73.jpg_1   <span class="number">1</span> <span class="number">1</span></span><br><span class="line"><span class="number">1</span>                  004f40c73.jpg_2   <span class="number">1</span> <span class="number">1</span></span><br><span class="line"><span class="number">2</span>                  004f40c73.jpg_3   <span class="number">1</span> <span class="number">1</span></span><br><span class="line"><span class="number">3</span>                  004f40c73.jpg_4   <span class="number">1</span> <span class="number">1</span></span><br><span class="line"><span class="number">4</span>                  006f39c41.jpg_1   <span class="number">1</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>

<h3 id="有无缺陷及每类缺陷的图像数量"><a href="#有无缺陷及每类缺陷的图像数量" class="headerlink" title="有无缺陷及每类缺陷的图像数量"></a>有无缺陷及每类缺陷的图像数量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class_dict = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">kind_class_dict = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">no_defects_num = <span class="number">0</span></span><br><span class="line">defects_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(train_df), <span class="number">4</span>):</span><br><span class="line">    img_names = [<span class="built_in">str</span>(i).split(<span class="string">&quot;_&quot;</span>)[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> train_df.iloc[col:col+<span class="number">4</span>, <span class="number">0</span>].values]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (img_names[<span class="number">0</span>] == img_names[<span class="number">1</span>] == img_names[<span class="number">2</span>] == img_names[<span class="number">3</span>]):</span><br><span class="line">        <span class="keyword">raise</span> ValueError</span><br><span class="line"></span><br><span class="line">    labels = train_df.iloc[col:col+<span class="number">4</span>, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> labels.isna().<span class="built_in">all</span>():</span><br><span class="line">        no_defects_num += <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        defects_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    kind_class_dict[<span class="built_in">sum</span>(labels.isna().values == <span class="literal">False</span>)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels.isna().values.tolist()):</span><br><span class="line">        <span class="keyword">if</span> label == <span class="literal">False</span>:</span><br><span class="line">            class_dict[idx+<span class="number">1</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;the number of images with no defects: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(no_defects_num))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;the number of images with defects: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(defects_num))</span><br></pre></td></tr></table></figure>
<p>得到的输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">the number of images <span class="keyword">with</span> no defects: <span class="number">5902</span></span><br><span class="line">the number of images <span class="keyword">with</span> defects: <span class="number">6666</span></span><br></pre></td></tr></table></figure>
<p>即无缺陷的图像有5902张，有缺陷的图像有6666张。<br>再对有缺陷的图像进行缺陷分类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line">sns.barplot(x=<span class="built_in">list</span>(class_dict.keys()), y=<span class="built_in">list</span>(class_dict.values()), ax=ax)</span><br><span class="line">ax.set_title(<span class="string">&quot;the number of images for each class&quot;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&quot;class&quot;</span>)</span><br><span class="line">class_dict</span><br></pre></td></tr></table></figure>
<p>得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(<span class="built_in">int</span>, &#123;<span class="number">1</span>: <span class="number">897</span>, <span class="number">2</span>: <span class="number">247</span>, <span class="number">3</span>: <span class="number">5150</span>, <span class="number">4</span>: <span class="number">801</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>以及可视化结果：<br><img src="https://user-images.githubusercontent.com/6218739/74599423-90b3d600-50bc-11ea-84b7-a9955d762c00.png" alt="defect_class"></p>
<p>从这一步得出的结论有两个：<br>（1）有缺陷和无缺陷的图像数量大致相当；<br>（2）缺陷的类别是不平衡的。</p>
<h3 id="一张图像中包含的缺陷数量"><a href="#一张图像中包含的缺陷数量" class="headerlink" title="一张图像中包含的缺陷数量"></a>一张图像中包含的缺陷数量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line">sns.barplot(x=<span class="built_in">list</span>(kind_class_dict.keys()), y=<span class="built_in">list</span>(kind_class_dict.values()), ax=ax)</span><br><span class="line">ax.set_title(<span class="string">&quot;Number of classes included in each image&quot;</span>);</span><br><span class="line">ax.set_xlabel(<span class="string">&quot;number of classes in the image&quot;</span>)</span><br><span class="line">kind_class_dict</span><br></pre></td></tr></table></figure>
<p>得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(<span class="built_in">int</span>, &#123;<span class="number">0</span>: <span class="number">5902</span>, <span class="number">1</span>: <span class="number">6239</span>, <span class="number">2</span>: <span class="number">425</span>, <span class="number">3</span>: <span class="number">2</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>以及可视化结果：<br><img src="https://user-images.githubusercontent.com/6218739/74599478-76c6c300-50bd-11ea-870c-ab2032635f04.png" alt="classes_in_one_image"><br>这一步得到的结论是：<br>大多数图像没有缺陷或仅含一种缺陷。</p>
<h2 id="读取和分析图像数据"><a href="#读取和分析图像数据" class="headerlink" title="读取和分析图像数据"></a>读取和分析图像数据</h2><h3 id="读取数据-1"><a href="#读取数据-1" class="headerlink" title="读取数据"></a>读取数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_size_dict = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">train_path = Path(<span class="string">&quot;drive/My Drive/severstal/train_images/&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> img_name <span class="keyword">in</span> train_path.iterdir():</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_name)</span><br><span class="line">    train_size_dict[img.size] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>看一下训练集中图像的尺寸和数目：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_size_dict</span><br></pre></td></tr></table></figure>
<p>得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(<span class="built_in">int</span>, &#123;(<span class="number">1600</span>, <span class="number">256</span>): <span class="number">12568</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>即，训练集中图像大小为1600乘以256大小，一共有12568张。<br>再读取和查看一下测试集中的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">test_size_dict = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">test_path = Path(<span class="string">&quot;drive/My Drive/severstal/test_images/&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> img_name <span class="keyword">in</span> test_path.iterdir():</span><br><span class="line">    img = Image.<span class="built_in">open</span>(img_name)</span><br><span class="line">    test_size_dict[img.size] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">test_size_dict</span><br></pre></td></tr></table></figure>
<p>得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">defaultdict(<span class="built_in">int</span>, &#123;(<span class="number">1600</span>, <span class="number">256</span>): <span class="number">1801</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>测试集中的图像也是1600乘以256，共1801张。</p>
<h2 id="可视化标注"><a href="#可视化标注" class="headerlink" title="可视化标注"></a>可视化标注</h2><h3 id="为不同的缺陷类别设置颜色显示"><a href="#为不同的缺陷类别设置颜色显示" class="headerlink" title="为不同的缺陷类别设置颜色显示"></a>为不同的缺陷类别设置颜色显示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">palet = [(<span class="number">249</span>, <span class="number">192</span>, <span class="number">12</span>), (<span class="number">0</span>, <span class="number">185</span>, <span class="number">241</span>), (<span class="number">114</span>, <span class="number">0</span>, <span class="number">218</span>), (<span class="number">249</span>,<span class="number">50</span>,<span class="number">12</span>)]</span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">4</span>, figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    ax[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    ax[i].imshow(np.ones((<span class="number">50</span>, <span class="number">50</span>, <span class="number">3</span>), dtype=np.uint8) * palet[i])</span><br><span class="line">    ax[i].set_title(<span class="string">&quot;class color: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">fig.suptitle(<span class="string">&quot;each class colors&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>不同的缺陷类别用如下颜色表示：<br><img src="https://user-images.githubusercontent.com/6218739/74599680-ca86db80-50c0-11ea-8872-27216caa4df1.png" alt="class-color"></p>
<h3 id="将不同的缺陷标识归类"><a href="#将不同的缺陷标识归类" class="headerlink" title="将不同的缺陷标识归类"></a>将不同的缺陷标识归类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">idx_no_defect = []</span><br><span class="line">idx_class_1 = []</span><br><span class="line">idx_class_2 = []</span><br><span class="line">idx_class_3 = []</span><br><span class="line">idx_class_4 = []</span><br><span class="line">idx_class_multi = []</span><br><span class="line">idx_class_triple = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(train_df), <span class="number">4</span>):</span><br><span class="line">    img_names = [<span class="built_in">str</span>(i).split(<span class="string">&quot;_&quot;</span>)[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> train_df.iloc[col:col+<span class="number">4</span>, <span class="number">0</span>].values]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (img_names[<span class="number">0</span>] == img_names[<span class="number">1</span>] == img_names[<span class="number">2</span>] == img_names[<span class="number">3</span>]):</span><br><span class="line">        <span class="keyword">raise</span> ValueError</span><br><span class="line">        </span><br><span class="line">    labels = train_df.iloc[col:col+<span class="number">4</span>, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> labels.isna().<span class="built_in">all</span>():</span><br><span class="line">        idx_no_defect.append(col)</span><br><span class="line">    <span class="keyword">elif</span> (labels.isna() == [<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]).<span class="built_in">all</span>():</span><br><span class="line">        idx_class_1.append(col)</span><br><span class="line">    <span class="keyword">elif</span> (labels.isna() == [<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">True</span>]).<span class="built_in">all</span>():</span><br><span class="line">        idx_class_2.append(col)</span><br><span class="line">    <span class="keyword">elif</span> (labels.isna() == [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>]).<span class="built_in">all</span>():</span><br><span class="line">        idx_class_3.append(col)</span><br><span class="line">    <span class="keyword">elif</span> (labels.isna() == [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>]).<span class="built_in">all</span>():</span><br><span class="line">        idx_class_4.append(col)</span><br><span class="line">    <span class="keyword">elif</span> labels.isna().<span class="built_in">sum</span>() == <span class="number">1</span>:</span><br><span class="line">        idx_class_triple.append(col)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        idx_class_multi.append(col)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>即将有不同缺陷的图像进行归类，同时注意最后有两个类别id_class_triple和id_class_multi用于存储同时有三类缺陷和同时有两类缺陷的图像。</p>
<h3 id="创建可视化标注的函数"><a href="#创建可视化标注的函数" class="headerlink" title="创建可视化标注的函数"></a>创建可视化标注的函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name_and_mask</span>(<span class="params">start_idx</span>):</span></span><br><span class="line">    col = start_idx</span><br><span class="line">    img_names = [<span class="built_in">str</span>(i).split(<span class="string">&quot;_&quot;</span>)[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> train_df.iloc[col:col+<span class="number">4</span>, <span class="number">0</span>].values]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (img_names[<span class="number">0</span>] == img_names[<span class="number">1</span>] == img_names[<span class="number">2</span>] == img_names[<span class="number">3</span>]):</span><br><span class="line">        <span class="keyword">raise</span> ValueError</span><br><span class="line"></span><br><span class="line">    labels = train_df.iloc[col:col+<span class="number">4</span>, <span class="number">1</span>]</span><br><span class="line">    mask = np.zeros((<span class="number">256</span>, <span class="number">1600</span>, <span class="number">4</span>), dtype=np.uint8)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(labels.values):</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> np.nan:</span><br><span class="line">            mask_label = np.zeros(<span class="number">1600</span>*<span class="number">256</span>, dtype=np.uint8)</span><br><span class="line">            label = label.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            positions = <span class="built_in">map</span>(<span class="built_in">int</span>, label[<span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line">            length = <span class="built_in">map</span>(<span class="built_in">int</span>, label[<span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line">            <span class="keyword">for</span> pos, le <span class="keyword">in</span> <span class="built_in">zip</span>(positions, length):</span><br><span class="line">                mask_label[pos-<span class="number">1</span>:pos+le-<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">            mask[:, :, idx] = mask_label.reshape(<span class="number">256</span>, <span class="number">1600</span>, order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img_names[<span class="number">0</span>], mask</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_mask_image</span>(<span class="params">col</span>):</span></span><br><span class="line">    name, mask = name_and_mask(col)</span><br><span class="line">    img = cv2.imread(<span class="built_in">str</span>(train_path / name))</span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">15</span>))</span><br><span class="line">    <span class="keyword">for</span> ch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        contours, _ = cv2.findContours(mask[:, :, ch], cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(contours)):</span><br><span class="line">            cv2.polylines(img, contours[i], <span class="literal">True</span>, palet[ch], <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    ax.set_title(name)</span><br><span class="line">    ax.imshow(img)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>第一个函数name_and_mask是得到图像的名称及其标注信息，第二个函数show_mask_image是使用opencv的findContours函数将标注画出来。</p>
<h3 id="无缺陷的图像展示"><a href="#无缺陷的图像展示" class="headerlink" title="无缺陷的图像展示"></a>无缺陷的图像展示</h3><p>首先看一下五张没有任何缺陷的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> idx_no_defect[:<span class="number">5</span>]:</span><br><span class="line">    show_mask_image(idx)</span><br></pre></td></tr></table></figure>
<p>如图：<br><img src="https://user-images.githubusercontent.com/6218739/74599826-5f8ad400-50c3-11ea-909e-b0d12a6e4fdc.png" alt="no_defects_imgs_1"><br><img src="https://user-images.githubusercontent.com/6218739/74599827-61549780-50c3-11ea-9506-f537d46ce518.png" alt="no_defects_imgs_2"><br><img src="https://user-images.githubusercontent.com/6218739/74599828-61ed2e00-50c3-11ea-816b-a3f0e190764c.png" alt="no_defects_imgs_3"><br><img src="https://user-images.githubusercontent.com/6218739/74599829-6285c480-50c3-11ea-8be5-c57a63e1db5c.png" alt="no_defects_imgs_4"><br><img src="https://user-images.githubusercontent.com/6218739/74599830-631e5b00-50c3-11ea-9a5e-8f13228f00f1.png" alt="no_defects_imgs_5"></p>
<h3 id="仅含第1类缺陷的图像展示"><a href="#仅含第1类缺陷的图像展示" class="headerlink" title="仅含第1类缺陷的图像展示"></a>仅含第1类缺陷的图像展示</h3><p>看一下五张仅含第1类缺陷的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> idx_class_1[:<span class="number">5</span>]:</span><br><span class="line">    show_mask_image(idx)</span><br></pre></td></tr></table></figure>

<p>如图：<br><img src="https://user-images.githubusercontent.com/6218739/74599879-0f604180-50c4-11ea-9116-84bf75fb2295.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599880-171fe600-50c4-11ea-8e99-8795de3504c7.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599882-1b4c0380-50c4-11ea-848b-f7d9b8376c5f.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599885-1edf8a80-50c4-11ea-9026-b9033a2bcc31.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599886-22731180-50c4-11ea-8c27-fa84fd41d847.png" alt="image"></p>
<h3 id="仅含第2类缺陷的图像展示"><a href="#仅含第2类缺陷的图像展示" class="headerlink" title="仅含第2类缺陷的图像展示"></a>仅含第2类缺陷的图像展示</h3><p>看一下五张仅含第2类缺陷的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> idx_class_2[:<span class="number">5</span>]:</span><br><span class="line">    show_mask_image(idx)</span><br></pre></td></tr></table></figure>
<p>如图：<br><img src="https://user-images.githubusercontent.com/6218739/74599907-87c70280-50c4-11ea-81ca-50662bc02c0a.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599908-8b5a8980-50c4-11ea-9951-c268e8e0c55a.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599910-91506a80-50c4-11ea-9d1a-7f8fb60cb9d8.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599912-96adb500-50c4-11ea-8d3d-2ab3bb835ebe.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599914-9ca39600-50c4-11ea-897e-59f238b2c918.png" alt="image"></p>
<h3 id="仅含第3类缺陷的图像展示"><a href="#仅含第3类缺陷的图像展示" class="headerlink" title="仅含第3类缺陷的图像展示"></a>仅含第3类缺陷的图像展示</h3><p>看一下五张仅含第3类缺陷的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> idx_class_3[:<span class="number">5</span>]:</span><br><span class="line">    show_mask_image(idx)</span><br></pre></td></tr></table></figure>

<p>如图：<br><img src="https://user-images.githubusercontent.com/6218739/74599934-e3918b80-50c4-11ea-91d5-f4ba53775314.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599937-e68c7c00-50c4-11ea-988c-0138a6cb3155.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599944-fa37e280-50c4-11ea-8219-d2df60d4fc5b.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599939-eee4b700-50c4-11ea-9bb5-8d65494fd248.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599941-f310d480-50c4-11ea-8444-cf8f85fe410b.png" alt="image"></p>
<h3 id="仅含第4类缺陷的图像展示"><a href="#仅含第4类缺陷的图像展示" class="headerlink" title="仅含第4类缺陷的图像展示"></a>仅含第4类缺陷的图像展示</h3><p>看一下五张仅含第4类缺陷的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> idx_class_4[:<span class="number">5</span>]:</span><br><span class="line">    show_mask_image(idx)</span><br></pre></td></tr></table></figure>
<p>如图：<br><img src="https://user-images.githubusercontent.com/6218739/74599955-2bb0ae00-50c5-11ea-9cc9-b7402a480ea5.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599957-2eab9e80-50c5-11ea-8946-051e81198e72.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599959-323f2580-50c5-11ea-8cd2-1cfcf6c23017.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599960-366b4300-50c5-11ea-85f2-e3f1e1b53003.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599964-39feca00-50c5-11ea-8f7f-a783740fdb1f.png" alt="image"></p>
<h3 id="同时含有两类缺陷的图像展示"><a href="#同时含有两类缺陷的图像展示" class="headerlink" title="同时含有两类缺陷的图像展示"></a>同时含有两类缺陷的图像展示</h3><p>看一下五张同时含有两类缺陷的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> idx_class_multi[:<span class="number">5</span>]:</span><br><span class="line">    show_mask_image(idx)</span><br></pre></td></tr></table></figure>
<p>如图：<br><img src="https://user-images.githubusercontent.com/6218739/74599988-864a0a00-50c5-11ea-8260-9507a1834174.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599989-88ac6400-50c5-11ea-9874-d6e4447fd49d.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599990-8d711800-50c5-11ea-968d-f0b9d5bccfd5.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599993-94982600-50c5-11ea-9079-9dfb217e9d14.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74599995-98c44380-50c5-11ea-8370-52dda33ecca5.png" alt="image"></p>
<h3 id="同时含有三类缺陷的图像展示"><a href="#同时含有三类缺陷的图像展示" class="headerlink" title="同时含有三类缺陷的图像展示"></a>同时含有三类缺陷的图像展示</h3><p>同时含有三类缺陷的图像只有两张，所以都显示出来了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> idx_class_triple:</span><br><span class="line">    show_mask_image(idx)</span><br></pre></td></tr></table></figure>
<p>如图：<br><img src="https://user-images.githubusercontent.com/6218739/74600020-fd7f9e00-50c5-11ea-8a30-fa06e6642f88.png" alt="image"><br><img src="https://user-images.githubusercontent.com/6218739/74600021-007a8e80-50c6-11ea-805c-1061fdc62a2c.png" alt="image"></p>
<h3 id="是否有像素属于多个缺陷"><a href="#是否有像素属于多个缺陷" class="headerlink" title="是否有像素属于多个缺陷"></a>是否有像素属于多个缺陷</h3><p>这一步查看是否有某个像素属于多个缺陷：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(train_df), <span class="number">4</span>)):</span><br><span class="line">    name, mask = name_and_mask(col)</span><br><span class="line">    <span class="keyword">if</span> (mask.<span class="built_in">sum</span>(axis=<span class="number">2</span>) &gt;= <span class="number">2</span>).<span class="built_in">any</span>():</span><br><span class="line">        show_mask_image(col)</span><br></pre></td></tr></table></figure>
<p>可以看出，所有的像素都是仅对应一个或0个缺陷。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xin-Bo Qi(亓欣波)"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Xin-Bo Qi(亓欣波)</p>
  <div class="site-description" itemprop="description">Be interesting!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">120</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">41</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/qixinbo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qixinbo" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qixinbo@gmail.com" title="E-Mail → mailto:qixinbo@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.tsinghua.edu.cn/" title="https:&#x2F;&#x2F;www.tsinghua.edu.cn" rel="noopener" target="_blank">THU</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.imr.cas.cn/" title="http:&#x2F;&#x2F;www.imr.cas.cn" rel="noopener" target="_blank">IMR</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.sdu.edu.cn/" title="http:&#x2F;&#x2F;www.sdu.edu.cn" rel="noopener" target="_blank">SDU</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://liam0205.me/" title="http:&#x2F;&#x2F;liam0205.me&#x2F;" rel="noopener" target="_blank">黄晨成</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xin-Bo Qi(亓欣波)</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://qixinbo.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>

</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"qixinbo.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="参考文献 保姆级教程：图解Transformer Transformer模型详解 Transformer 详解 盘点 | 2021年paper大热的Transformer (ViT) “未来”的经典之作ViT：transformer is all you need! ViT( Vision Transformer)  简介 Attention Is All You Need是一篇Google于20">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP霸主Transformer及CV新秀Vision Transformer解析">
<meta property="og:url" content="http://qixinbo.github.io/2021/11/09/transformer/index.html">
<meta property="og:site_name" content="数字旗手">
<meta property="og:description" content="参考文献 保姆级教程：图解Transformer Transformer模型详解 Transformer 详解 盘点 | 2021年paper大热的Transformer (ViT) “未来”的经典之作ViT：transformer is all you need! ViT( Vision Transformer)  简介 Attention Is All You Need是一篇Google于20">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140851930-34149770-8d8b-4fed-bdf2-43cce255f0b1.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140852001-0fbc70ae-83f7-40c7-b784-4ed205ac30a9.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140852095-d48c5aaa-d167-4169-8b56-5451a8db4416.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140852188-a8f1dee3-5b59-489b-ba4e-df8d6664f6da.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140853558-6b30feb1-7ca3-495b-8c6f-a1456d67be8d.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140853602-a6f68d15-8308-4b0d-b067-abb5d9a7e098.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140854019-3e363d94-cefe-4772-a7f2-fa87de1eb41b.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140857139-71ca395e-ec3a-40c4-8c0d-4c815ceb9e09.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140857357-91caf198-e459-471b-a800-8ae705ef1434.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140857512-c22f2546-25e5-449d-8632-6cdffd1dc4fe.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140857653-468ca02a-3dfb-4d33-a8b9-36f8596f9f15.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140857883-d06b029d-99ec-4a28-8711-35b0d4425f53.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140857941-c6f16054-2104-4205-850b-d15bc96a6659.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140868505-278beff7-e9dc-440d-87ad-b2a021310a59.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140868544-311c039e-252c-425f-b957-3d1f2eece542.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140868642-638e7a9f-6543-4068-b569-12984bc7b5be.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140868817-3f900670-e211-4395-bb5a-2e9625ed3644.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140869917-059093b6-d8c7-462e-9926-ecdb6101f898.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140870020-f7fb5f4e-d6de-4838-9dbb-e809e0f2fdec.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140872426-955ab9c0-842e-444c-947b-b6d9292a4fe1.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140873135-92be1693-5efe-4998-9c6b-9071a3c96fae.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140873707-8ab175f3-df87-40bc-901f-62305d01b2cd.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140873785-a14ea99e-09c5-4610-a97f-39039bd99ad6.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140874351-f075901e-7869-4f2d-8c03-c94b12116938.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140877989-64b07f86-fe4f-45df-8f4c-a30a11391cb0.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/6218739/140884681-87fb368e-87a6-4e6a-a9df-3ea19a31012b.png">
<meta property="article:published_time" content="2021-11-08T16:00:00.000Z">
<meta property="article:modified_time" content="2021-11-09T07:58:46.805Z">
<meta property="article:author" content="Xin-Bo Qi(亓欣波)">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://user-images.githubusercontent.com/6218739/140851930-34149770-8d8b-4fed-bdf2-43cce255f0b1.png">

<link rel="canonical" href="http://qixinbo.github.io/2021/11/09/transformer/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>NLP霸主Transformer及CV新秀Vision Transformer解析 | 数字旗手</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="数字旗手" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">数字旗手</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">电气化、自动化、数字化、智能化、智慧化</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-scholar">

    <a href="/scholar/" rel="section"><i class="fa fa-chart-bar fa-fw"></i>scholar</a>

  </li>
        <li class="menu-item menu-item-sources">

    <a href="/sources/" rel="section"><i class="fa fa-rss fa-fw"></i>sources</a>

  </li>
        <li class="menu-item menu-item-gallery">

    <a href="/gallery/" rel="section"><i class="fa fa-file-image fa-fw"></i>gallery</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404.html" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Commonweal 404</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://qixinbo.github.io/2021/11/09/transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Xin-Bo Qi(亓欣波)">
      <meta itemprop="description" content="Digitize everything to realize Digitalization!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数字旗手">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP霸主Transformer及CV新秀Vision Transformer解析
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-11-09 00:00:00 / Modified: 15:58:46" itemprop="dateCreated datePublished" datetime="2021-11-09T00:00:00+08:00">2021-11-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2021/11/09/transformer/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/11/09/transformer/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a target="_blank" rel="noopener" href="https://cuijiahua.com/blog/2021/01/dl-basics-3.html">保姆级教程：图解Transformer</a><br><a target="_blank" rel="noopener" href="https://terrifyzhao.github.io/2019/01/11/Transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3.html">Transformer模型详解</a><br><a target="_blank" rel="noopener" href="https://wmathor.com/index.php/archives/1438/">Transformer 详解</a><br><a target="_blank" rel="noopener" href="https://picture.iczhiku.com/weixin/message1610942723056.html">盘点 | 2021年paper大热的Transformer (ViT)</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/356155277">“未来”的经典之作ViT：transformer is all you need!</a><br><a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/classification/ViT.html">ViT( Vision Transformer)</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a>是一篇Google于2017年提出的将Attention思想发挥到极致的论文。这篇论文中提出一个全新的模型，叫 Transformer，抛弃了以往深度学习任务里面使用到的 CNN 和 RNN ，目前大热的Bert、GPT和DALL-E就是基于Transformer构建的，这个模型广泛应用于NLP领域，例如机器翻译，问答系统，文本摘要和语音识别等等方向。<br><img src="https://user-images.githubusercontent.com/6218739/140851930-34149770-8d8b-4fed-bdf2-43cce255f0b1.png" alt="1"></p>
<p>相比于NLP领域，在CV领域中，卷积神经网络CNN却是绝对的霸主。对于图像问题，CNN具有天然的先天优势（inductive bias）：平移不变性（translation equivariance）和局部性（locality）。而transformer虽然不并具备这些优势，但是transformer的核心self-attention的优势不像卷积那样有固定且有限的感受野，self-attention操作可以获得long-range信息（相比之下CNN要通过不断堆积Conv layers来获取更大的感受野），但训练的难度就比CNN要稍大一些。<br>仍然是Google，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929.pdf">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>这篇2020年的论文将Transformer引入了CV中，形成了Vision Transformer，简称为ViT。<br>本文尝试理解一下原始Transformer及其衍生品ViT。</p>
<h1 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h1><p>Transformer 的内部，在本质上是一个 Encoder-Decoder 的结构，即 编码器-解码器。<br><img src="https://user-images.githubusercontent.com/6218739/140852001-0fbc70ae-83f7-40c7-b784-4ed205ac30a9.png" alt="2"><br>Transformer 中抛弃了传统的 CNN 和 RNN，整个网络结构完全由 Attention 机制组成，并且采用了 6 层 Encoder-Decoder 结构。<br><img src="https://user-images.githubusercontent.com/6218739/140852095-d48c5aaa-d167-4169-8b56-5451a8db4416.png" alt="3"><br>显然，Transformer 主要分为两大部分，分别是编码器和解码器。<br>整个 Transformer 是由 6 个这样的结构组成，为了方便理解，我们只看其中一个Encoder-Decoder 结构。<br>以一个简单的例子进行说明：<br><img src="https://user-images.githubusercontent.com/6218739/140852188-a8f1dee3-5b59-489b-ba4e-df8d6664f6da.png" alt="4"></p>
<p>Why do we work?，我们为什么工作？<br>左侧红框是编码器，右侧红框是解码器，<br>编码器负责把自然语言序列映射成为隐藏层（上图第2步），即含有自然语言序列的数学表达。<br>解码器把隐藏层再映射为自然语言序列，从而使我们可以解决各种问题，如情感分析、机器翻译、摘要生成、语义关系抽取等。<br>简单说下，上图每一步都做了什么：<br>（1）输入自然语言序列到编码器: Why do we work?；<br>（2）编码器输出的隐藏层，再输入到解码器；<br>（3）输入 $&lt;𝑠𝑡𝑎𝑟𝑡&gt;$符号到解码器；<br>（4）解码器得到第一个字”为”；<br>（5）将得到的第一个字”为”落下来再输入到解码器；<br>（6）解码器得到第二个字”什”；<br>（7）将得到的第二字再落下来，直到解码器输出 $&lt;𝑒𝑛𝑑&gt;$，即序列生成完成。</p>
<h2 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h2><p>编码器即是把自然语言序列映射为隐藏层的数学表达的过程。<br>为了方便学习，编码器可以分为 4 个部分：<br><img src="https://user-images.githubusercontent.com/6218739/140853558-6b30feb1-7ca3-495b-8c6f-a1456d67be8d.png" alt="5"></p>
<h3 id="位置嵌入（𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑎𝑙-𝑒𝑛𝑐𝑜𝑑𝑖𝑛𝑔）"><a href="#位置嵌入（𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑎𝑙-𝑒𝑛𝑐𝑜𝑑𝑖𝑛𝑔）" class="headerlink" title="位置嵌入（𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑎𝑙 𝑒𝑛𝑐𝑜𝑑𝑖𝑛𝑔）"></a>位置嵌入（𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑎𝑙 𝑒𝑛𝑐𝑜𝑑𝑖𝑛𝑔）</h3><p>我们输入数据 X 维度为[batch size, sequence length]的数据，比如我们为什么工作。<br>batch size 就是 batch 的大小，这里只有一句话，所以 batch size 为 1，sequence length 是句子的长度，一共 7 个字，所以输入的数据维度是 [1, 7]。<br>我们不能直接将这句话输入到编码器中，因为 Tranformer 不认识，我们需要先进行字嵌入，即得到图中的$X_{embedding}$。</p>
<p>简单点说，就是文字到字向量的转换，这种转换是将文字转换为计算机认识的数学表示，用到的方法就是Word2Vec，Word2Vec的具体细节，对于初学者暂且不用了解，这个是可以直接使用的。</p>
<p>得到的$X{embedding}$的维度是[batch size, sequence length, embedding dimension]，embedding dimension 的大小由 Word2Vec 算法决定，Tranformer 采用 512 长度的字向量。所以$X_{embedding}$的维度是[1, 7, 512]。</p>
<p>至此，输入的我们为什么工作，可以用一个矩阵来简化表示。<br><img src="https://user-images.githubusercontent.com/6218739/140853602-a6f68d15-8308-4b0d-b067-abb5d9a7e098.png" alt="6"><br>我们知道，文字的先后顺序，很重要。<br>比如吃饭没、没吃饭、没饭吃、饭吃没、饭没吃，同样三个字，顺序颠倒，所表达的含义就不同了。<br>文字的位置信息很重要，Tranformer 没有类似 RNN 的循环结构，没有捕捉顺序序列的能力。<br>为了保留这种位置信息交给 Tranformer 学习，我们需要用到位置嵌入。<br>加入位置信息的方式非常多，最简单的可以是直接将绝对坐标 0,1,2 编码。<br>Tranformer 采用的是 sin-cos 规则，使用了 sin 和 cos 函数的线性变换来提供给模型位置信息：</p>
<script type="math/tex; mode=display">
\begin{aligned} P E_{(p o s, 2 i)} &=\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\ P E_{(\text {pos }, 2 i+1)} &=\cos \left(\text { pos } / 10000^{2 i / d_{\text {model }}}\right) \end{aligned}</script><p>上式中 pos 指的是句中字的位置，取值范围是 [0, 𝑚𝑎𝑥 𝑠𝑒𝑞𝑢𝑒𝑛𝑐𝑒 𝑙𝑒𝑛𝑔𝑡ℎ)，i 指的是字嵌入的维度, 取值范围是 [0, 𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔 𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛)。<br>上面有 sin 和 cos 一组公式，也就是对应着 𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔 𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛 维度的一组奇数和偶数的序号的维度，从而产生不同的周期性变化。<br>可以用代码，简单看下效果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入依赖库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_positional_encoding</span>(<span class="params">max_seq_len, embed_dim</span>):</span></span><br><span class="line">    <span class="comment"># 初始化一个positional encoding</span></span><br><span class="line">    <span class="comment"># embed_dim: 字嵌入的维度</span></span><br><span class="line">    <span class="comment"># max_seq_len: 最大的序列长度</span></span><br><span class="line">    positional_encoding = np.array([</span><br><span class="line">        [pos / np.power(<span class="number">10000</span>, <span class="number">2</span> * i / embed_dim) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(embed_dim)]</span><br><span class="line">        <span class="keyword">if</span> pos != <span class="number">0</span> <span class="keyword">else</span> np.zeros(embed_dim) <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(max_seq_len)])</span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i 偶数</span></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1 奇数</span></span><br><span class="line">    <span class="comment"># 归一化, 用位置嵌入的每一行除以它的模长</span></span><br><span class="line">    <span class="comment"># denominator = np.sqrt(np.sum(position_enc**2, axis=1, keepdims=True))</span></span><br><span class="line">    <span class="comment"># position_enc = position_enc / (denominator + 1e-8)</span></span><br><span class="line">    <span class="keyword">return</span> positional_encoding</span><br><span class="line">    </span><br><span class="line">positional_encoding = get_positional_encoding(max_seq_len=<span class="number">100</span>, embed_dim=<span class="number">16</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">sns.heatmap(positional_encoding)</span><br><span class="line">plt.title(<span class="string">&quot;Sinusoidal Function&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;hidden dimension&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;sequence length&quot;</span>)</span><br></pre></td></tr></table></figure><br>可以看到，位置嵌入在 𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔 𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛 （也是hidden dimension ）维度上随着维度序号增大，周期变化会越来越慢，而产生一种包含位置信息的纹理。<br><img src="https://user-images.githubusercontent.com/6218739/140854019-3e363d94-cefe-4772-a7f2-fa87de1eb41b.png" alt="embed"><br>就这样，产生独一的纹理位置信息，模型从而学到位置之间的依赖关系和自然语言的时序特性。<br>最后，将$X_{\text {embedding }}$和 位置嵌入 相加（维度相同，可以直接相加），得到该字真正的向量表示，然后送给下一层。</p>
<h3 id="自注意力层（𝑠𝑒𝑙𝑓-𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛-𝑚𝑒𝑐ℎ𝑎𝑛𝑖𝑠𝑚）"><a href="#自注意力层（𝑠𝑒𝑙𝑓-𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛-𝑚𝑒𝑐ℎ𝑎𝑛𝑖𝑠𝑚）" class="headerlink" title="自注意力层（𝑠𝑒𝑙𝑓 𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 𝑚𝑒𝑐ℎ𝑎𝑛𝑖𝑠𝑚）"></a>自注意力层（𝑠𝑒𝑙𝑓 𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 𝑚𝑒𝑐ℎ𝑎𝑛𝑖𝑠𝑚）</h3><p>这部分介绍来自于<a target="_blank" rel="noopener" href="https://terrifyzhao.github.io/2019/01/11/Transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3.html">这篇博客</a><br>self-attention，其思想和attention类似，但是self-attention是Transformer用来将其他相关单词的“理解”转换成我们正在处理的单词的一种思路，我们看个例子： The animal didn’t cross the street because it was too tired 这里的it到底代表的是animal还是street呢，对于我们来说能很简单的判断出来，但是对于机器来说，是很难判断的，self-attention就能够让机器把it和animal联系起来，接下来我们看下详细的处理过程。<br>（1）首先，self-attention会计算出三个新的向量，在论文中，向量的维度是512维，我们把这三个向量分别称为Query、Key、Value，这三个向量是用embedding向量与一个矩阵相乘得到的结果，这个矩阵是随机初始化的，维度为（64，512），注意第二个维度需要和embedding的维度一样，其值在BP的过程中会一直进行更新，得到的这三个向量的维度是64低于embedding维度的。<br><img src="https://user-images.githubusercontent.com/6218739/140857139-71ca395e-ec3a-40c4-8c0d-4c815ceb9e09.png" alt="qkv"><br>那么Query、Key、Value这三个向量又是什么呢？这三个向量对于attention来说很重要，当你理解了下文后，你将会明白这三个向量扮演者什么的角色。<br>（2）计算self-attention的分数值，该分数值决定了当我们在某个位置encode一个词时，对输入句子的其他部分的关注程度。这个分数值的计算方法是Query与Key做点乘，以下图为例，首先我们需要针对Thinking这个词，计算出其他词对于该词的一个分数值，首先是针对于自己本身即$q1·k1$，然后是针对于第二个词即$q1·k2$。<br><img src="https://user-images.githubusercontent.com/6218739/140857357-91caf198-e459-471b-a800-8ae705ef1434.png" alt="score"><br>（3）接下来，把点乘的结果除以一个常数，这里我们除以8，这个值一般是采用上文提到的矩阵的第一个维度的开方即64的开方8，当然也可以选择其他的值，然后把得到的结果做一个softmax的计算。得到的结果即是每个词对于当前位置的词的相关性大小，当然，当前位置的词相关性肯定会很大。<br><img src="https://user-images.githubusercontent.com/6218739/140857512-c22f2546-25e5-449d-8632-6cdffd1dc4fe.png" alt="score2"><br>（4）下一步就是把Value和softmax得到的值进行相乘，并相加，得到的结果即是self-attention在当前节点的值。<br><img src="https://user-images.githubusercontent.com/6218739/140857653-468ca02a-3dfb-4d33-a8b9-36f8596f9f15.png" alt="score3"><br>在实际的应用场景，为了提高计算速度，我们采用的是矩阵的方式，直接计算出Query, Key, Value的矩阵，然后把embedding的值与三个矩阵直接相乘，把得到的新矩阵Q与K相乘，乘以一个常数，做softmax操作，最后乘上V矩阵：<br><img src="https://user-images.githubusercontent.com/6218739/140857883-d06b029d-99ec-4a28-8711-35b0d4425f53.png" alt="attention"><br><img src="https://user-images.githubusercontent.com/6218739/140857941-c6f16054-2104-4205-850b-d15bc96a6659.png" alt="attention2"><br>这种通过 query 和 key 的相似性程度来确定 value 的权重分布的方法被称为scaled dot-product attention。</p>
<h4 id="Multi-Headed-Attention"><a href="#Multi-Headed-Attention" class="headerlink" title="Multi-Headed Attention"></a>Multi-Headed Attention</h4><p>这篇论文更牛的地方是给self-attention加入了另外一个机制，被称为“multi-headed” attention，该机制理解起来很简单，就是说不仅仅只初始化一组Q、K、V的矩阵，而是初始化多组，tranformer是使用了8组，所以最后得到的结果是8个矩阵。<br><img src="https://user-images.githubusercontent.com/6218739/140868505-278beff7-e9dc-440d-87ad-b2a021310a59.png" alt="multihead1"><br><img src="https://user-images.githubusercontent.com/6218739/140868544-311c039e-252c-425f-b957-3d1f2eece542.png" alt="multihead2"><br>这给我们留下了一个小的挑战，前馈神经网络没法输入8个矩阵呀，这该怎么办呢？所以我们需要一种方式，把8个矩阵降为1个，首先，我们把8个矩阵连在一起，这样会得到一个大的矩阵，再随机初始化一个矩阵和这个组合好的矩阵相乘，最后得到一个最终的矩阵。<br><img src="https://user-images.githubusercontent.com/6218739/140868642-638e7a9f-6543-4068-b569-12984bc7b5be.png" alt="multihead3"><br>这就是multi-headed attention的全部流程了，这里其实已经有很多矩阵了，我们把所有的矩阵放到一张图内看一下总体的流程。<br><img src="https://user-images.githubusercontent.com/6218739/140868817-3f900670-e211-4395-bb5a-2e9625ed3644.png" alt="multihead4"></p>
<h3 id="残差链接和层归一化"><a href="#残差链接和层归一化" class="headerlink" title="残差链接和层归一化"></a>残差链接和层归一化</h3><p>加入了残差设计和层归一化操作，目的是为了防止梯度消失，加快收敛。</p>
<h4 id="残差设计"><a href="#残差设计" class="headerlink" title="残差设计"></a>残差设计</h4><p>我们在上一步得到了经过注意力矩阵加权之后的 $𝑉$， 也就是$𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑄, 𝐾, 𝑉)$，我们对它进行一下转置，使其和$X_{\text {embedding }}$ 的维度一致, 也就是 [𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒, 𝑠𝑒𝑞𝑢𝑒𝑛𝑐𝑒 𝑙𝑒𝑛𝑔𝑡ℎ, 𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔 𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛]，然后把他们加起来做残差连接，直接进行元素相加，因为他们的维度一致:</p>
<script type="math/tex; mode=display">
X_{embedding} + Attention(Q, \ K, \ V)</script><p>在之后的运算里，每经过一个模块的运算，都要把运算之前的值和运算之后的值相加，从而得到残差连接，训练的时候可以使梯度直接走捷径反传到最初始层：</p>
<script type="math/tex; mode=display">
X + SubLayer(X)</script><h4 id="层归一化"><a href="#层归一化" class="headerlink" title="层归一化"></a>层归一化</h4><p>Normalization有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为0方差为1的数据。我们在把数据送入激活函数之前进行normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。<br>说到 normalization，那就肯定得提到 Batch Normalization。BN的主要思想就是：在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。<br>BN的具体做法就是对每一小批数据，在批这个方向上做归一化。如下图所示：<br><img src="https://user-images.githubusercontent.com/6218739/140869917-059093b6-d8c7-462e-9926-ecdb6101f898.png" alt="BN"><br>可以看到，右半边求均值是沿着数据 batch_size的方向进行的，其计算公式如下：</p>
<script type="math/tex; mode=display">
BN(x_i)=\alpha × \frac{x_i-\mu_b}{\sqrt{\sigma^2_B+\epsilon}}+\beta</script><p>那么什么是 Layer normalization 呢？它也是归一化数据的一种方式，不过 LN 是在每一个样本上计算均值和方差，而不是BN那种在批方向计算均值和方差！<br><img src="https://user-images.githubusercontent.com/6218739/140870020-f7fb5f4e-d6de-4838-9dbb-e809e0f2fdec.png" alt="LN"><br>LN的公式为：</p>
<script type="math/tex; mode=display">
LN(x_i)=\alpha × \frac{x_i-\mu_L}{\sqrt{\sigma^2_L+\epsilon}}+\beta</script><h3 id="前馈网络"><a href="#前馈网络" class="headerlink" title="前馈网络"></a>前馈网络</h3><p>前馈网络FeedForward，其实就是两层线性映射并用激活函数激活。<br>然后经过这个网络激活后，再经过一个残差连接和层归一化，即可输出。<br>直接看代码可能更直观：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; A two-feed-forward-layer module &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_in, d_hid, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 两个fc层，对最后的512维度进行变换</span></span><br><span class="line">        self.w_1 = nn.Linear(d_in, d_hid) <span class="comment"># position-wise</span></span><br><span class="line">        self.w_2 = nn.Linear(d_hid, d_in) <span class="comment"># position-wise</span></span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_in, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        x = self.w_2(F.relu(self.w_1(x)))</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x += residual</span><br><span class="line"></span><br><span class="line">        x = self.layer_norm(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h3 id="编码器总结"><a href="#编码器总结" class="headerlink" title="编码器总结"></a>编码器总结</h3><p>经过上面 3 个步骤，我们已经基本了解了 Encoder 的主要构成部分。<br>用一个更直观的图表示如下：<br><img src="https://user-images.githubusercontent.com/6218739/140872426-955ab9c0-842e-444c-947b-b6d9292a4fe1.png" alt="encoder"><br>文字描述为：<br>输入$x_1,x_2$经 self-attention 层之后变成$z_1,z_2$ ，然后和输入$x_1,x_2$进行残差连接，经过 LayerNorm 后输出给全连接层。全连接层也有一个残差连接和一个 LayerNorm，最后再输出给下一个 Encoder（每个 Encoder Block 中的 FeedForward 层权重都是共享的）<br>公式描述为：<br>（1）字向量与位置编码</p>
<script type="math/tex; mode=display">
X = \text{Embedding-Lookup}(X) + \text{Positional-Encoding}</script><p>（2）自注意力机制</p>
<script type="math/tex; mode=display">
\begin{align}
Q &= \text{Linear}_q(X) = XW_{Q}\\
K &= \text{Linear}_k(X) = XW_{K}\\
V &= \text{Linear}_v(X) = XW_{V}\\
X_{attention} &= \text{Self-Attention}(Q,K,V)
\end{align}</script><p>（3）self-attention 残差连接与 Layer Normalization</p>
<script type="math/tex; mode=display">
\begin{align}
X_{attention} &= X + X_{attention}\\
X_{attention} &= \text{LayerNorm}(X_{attention})
\end{align}</script><p>（4）前馈网络FeedForward</p>
<script type="math/tex; mode=display">
X_{hidden} = \text{Linear}(\text{ReLU}(\text{Linear}(X_{attention})))</script><p>（5）FeedForward 残差连接与 Layer Normalization</p>
<script type="math/tex; mode=display">
\begin{align}
X_{hidden} &= X_{attention} + X_{hidden}\\
X_{hidden} &= \text{LayerNorm}(X_{hidden})
\end{align}</script><p>其中：</p>
<script type="math/tex; mode=display">
X_{hidden} \in \mathbb{R}^{batch\_size  \ * \  seq\_len. \  * \  embed\_dim}</script><h2 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h2><p>见<a target="_blank" rel="noopener" href="https://wmathor.com/index.php/archives/1438/">原文</a>。<br>Decoder架构如下：<br><img src="https://user-images.githubusercontent.com/6218739/140873135-92be1693-5efe-4998-9c6b-9071a3c96fae.png" alt="decoder"><br>我们先从 HighLevel 的角度观察一下 Decoder 结构，从下到上依次是：<br>（1）Masked Multi-Head Self-Attention<br>（2）Multi-Head Encoder-Decoder Attention<br>（3）FeedForward Network<br>和 Encoder 一样，上面三个部分的每一个部分，都有一个残差连接，后接一个 Layer Normalization。Decoder 的中间部件并不复杂，大部分在前面 Encoder 里我们已经介绍过了，但是 Decoder 由于其特殊的功能，因此在训练时会涉及到一些细节。</p>
<h3 id="Masked-Self-Attention"><a href="#Masked-Self-Attention" class="headerlink" title="Masked Self-Attention"></a>Masked Self-Attention</h3><p>具体来说，传统 Seq2Seq 中 Decoder 使用的是 RNN 模型，因此在训练过程中输入$t$时刻的词，模型无论如何也看不到未来时刻的词，因为循环神经网络是时间驱动的，只有当$t$时刻运算结束了，才能看到$t+1$时刻的词。而 Transformer Decoder 抛弃了 RNN，改为 Self-Attention，由此就产生了一个问题，在训练过程中，整个 ground truth 都暴露在 Decoder 中，这显然是不对的，我们需要对 Decoder 的输入进行一些处理，该处理被称为 Mask。<br>举个例子，Decoder 的 ground truth 为 “start起始符号 I am fine”，我们将这个句子输入到 Decoder 中，经过 WordEmbedding 和 Positional Encoding 之后，将得到的矩阵做三次线性变换$W_Q,W_K,W_V$。然后进行 self-attention 操作，首先通过$\frac {Q\times K^T}{\sqrt {d_k}}$得到 Scaled Scores，接下来非常关键，我们要对 Scaled Scores 进行 Mask，举个例子，当我们输入 “I” 时，模型目前仅知道包括 “I” 在内之前所有字的信息，即 “start起始符号” 和 “I” 的信息，不应该让其知道 “I” 之后词的信息。道理很简单，我们做预测的时候是按照顺序一个字一个字的预测，怎么能这个字都没预测完，就已经知道后面字的信息了呢？Mask 非常简单，首先生成一个下三角全 0，上三角全为负无穷的矩阵，然后将其与 Scaled Scores 相加即可：<br><img src="https://user-images.githubusercontent.com/6218739/140873707-8ab175f3-df87-40bc-901f-62305d01b2cd.png" alt="mask"><br>之后再做 softmax，就能将 - inf 变为 0，得到的这个矩阵即为每个字之间的权重：<br><img src="https://user-images.githubusercontent.com/6218739/140873785-a14ea99e-09c5-4610-a97f-39039bd99ad6.png" alt="unmask"><br>Multi-Head Self-Attention 无非就是并行的对上述步骤多做几次，前面 Encoder 也介绍了，这里就不多赘述了。</p>
<h3 id="Masked-Encoder-Decoder-Attention"><a href="#Masked-Encoder-Decoder-Attention" class="headerlink" title="Masked Encoder-Decoder Attention"></a>Masked Encoder-Decoder Attention</h3><p>其实这一部分的计算流程和前面 Masked Self-Attention 很相似，结构也一摸一样，唯一不同的是这里的$K,V$为 Encoder 的输出（不然Encoder辛辛苦苦做的输出就没用了），$Q$为 Decoder 中 Masked Self-Attention 的输出。<br><img src="https://user-images.githubusercontent.com/6218739/140874351-f075901e-7869-4f2d-8c03-c94b12116938.png" alt="e-d"></p>
<h1 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h1><p>使用Transformer进行视觉任务的研究已经成了一个新的热点，大家为了更低的模型复杂度以及训练的效率，都在研究如何将这一技术应用在视觉任务上。<br>通常来说，在所有的关于Transformer的论文以及工作中，有两个比较大的架构，其中一个就是传统的CNNs加Transformer组合而成的结构，另一种是纯粹的Transformers。<br>ViT使用的就是纯粹的Transformer去完成视觉任务，也就是说，它没有使用任何的CNNs。这也是它的价值所在，谷歌大脑团队在几乎没有修改任何基于NLP的Transformer的结构基础之上，只是将输入进行了一个适配，将图片切分成许多的小格，然后将这些作为序列输入到模型，最终完成了分类任务，并且效果可以直追基于CNNs的SOTA。<br>ViT的思路很简单：直接把图像分成固定大小的patchs，然后通过线性变换得到patch embedding，这就类比NLP的words和word embedding，由于transformer的输入就是a sequence of token embeddings，所以将图像的patch embeddings送入transformer后就能够进行特征提取从而分类了。ViT模型原理如下图所示，其实ViT模型只是用了transformer的Encoder来提取特征（原始的transformer还有decoder部分，用于实现sequence to sequence，比如机器翻译）。<br><img src="https://user-images.githubusercontent.com/6218739/140877989-64b07f86-fe4f-45df-8f4c-a30a11391cb0.png" alt="ViT"></p>
<p>ViT架构相对于原始Transformer，有一些特殊处理：</p>
<h2 id="图像分块嵌入"><a href="#图像分块嵌入" class="headerlink" title="图像分块嵌入"></a>图像分块嵌入</h2><p>考虑到在Transformer结构中，输入是一个二维的矩阵，矩阵的形状可以表示为$(N,D)$，其中$N$是sequence的长度，而$D$是sequence中每个向量的维度。因此，在ViT算法中，首先需要设法将$H \times W \times C$的三维图像转化为$(N,D)$的二维输入。<br>ViT中的具体实现方式为：将$H \times W \times C$的图像，变为一个$N \times (P^2 \times C)$的序列。这个序列可以看作是一系列展平的图像块，也就是将图像切分成小块后，再将其展平。该序列中一共包含了$N=HW/P^2$个图像块，每个图像块的维度则是$(P^2\times C)$。其中$P$是图像块的大小，$C$是通道数量。经过如上变换，就可以将$N$视为sequence的长度了。但是，此时每个图像块的维度是$(P^2\times C)$，而我们实际需要的向量维度是$D$，因此我们还需要对图像块进行 Embedding。这里 Embedding 的方式非常简单，只需要对每个$(P^2 \times C)$的图像块做一个线性变换，将维度压缩为$D$即可。</p>
<h2 id="Class-Token"><a href="#Class-Token" class="headerlink" title="Class Token"></a>Class Token</h2><p>ViT借鉴BERT增加了一个特殊的class token。transformer的encoder输入是a sequence patch embeddings，输出也是同样长度的a sequence patch features，但图像分类最后需要获取image feature，简单的策略是采用pooling，比如求patch features的平均来获取image feature，但是ViT并没有采用类似的pooling策略，而是直接增加一个特殊的class token，其最后输出的特征加一个linear classifier就可以实现对图像的分类（ViT的pre-training时是接一个MLP head），所以输入ViT的sequence长度是$N+1$。<br>class token对应的embedding在训练时随机初始化，然后通过训练得到。</p>
<h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>按照 Transformer 结构中的位置编码习惯，这个工作也使用了位置编码。不同的是，ViT 中的位置编码没有采用原版 Transformer 中的 sin-cos编码，而是直接设置为可学习的 Positional Encoding。对训练好的 Positional Encoding 进行可视化，可以看到，位置越接近，往往具有更相似的位置编码。此外，出现了行列结构，同一行/列中的 patch 具有相似的位置编码。<br><img src="https://user-images.githubusercontent.com/6218739/140884681-87fb368e-87a6-4e6a-a9df-3ea19a31012b.png" alt="vit-pos"></p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat_reward.png" alt="Xin-Bo Qi(亓欣波) WeChat Pay">
        <p>WeChat Pay</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Transformer/" rel="tag"># Transformer</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/11/05/python-on-web/" rel="prev" title="Python爱浏览器，但浏览器不爱它：如何让Python运行在浏览器上">
      <i class="fa fa-chevron-left"></i> Python爱浏览器，但浏览器不爱它：如何让Python运行在浏览器上
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/11/15/electron/" rel="next" title="Electron初探：基于Web的跨平台桌面应用开发">
      Electron初探：基于Web的跨平台桌面应用开发 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">1.</span> <span class="nav-text">参考文献</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">2.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer%E6%9E%B6%E6%9E%84"><span class="nav-number">3.</span> <span class="nav-text">Transformer架构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-number">3.1.</span> <span class="nav-text">编码器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E5%B5%8C%E5%85%A5%EF%BC%88%F0%9D%91%9D%F0%9D%91%9C%F0%9D%91%A0%F0%9D%91%96%F0%9D%91%A1%F0%9D%91%96%F0%9D%91%9C%F0%9D%91%9B%F0%9D%91%8E%F0%9D%91%99-%F0%9D%91%92%F0%9D%91%9B%F0%9D%91%90%F0%9D%91%9C%F0%9D%91%91%F0%9D%91%96%F0%9D%91%9B%F0%9D%91%94%EF%BC%89"><span class="nav-number">3.1.1.</span> <span class="nav-text">位置嵌入（𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑎𝑙 𝑒𝑛𝑐𝑜𝑑𝑖𝑛𝑔）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82%EF%BC%88%F0%9D%91%A0%F0%9D%91%92%F0%9D%91%99%F0%9D%91%93-%F0%9D%91%8E%F0%9D%91%A1%F0%9D%91%A1%F0%9D%91%92%F0%9D%91%9B%F0%9D%91%A1%F0%9D%91%96%F0%9D%91%9C%F0%9D%91%9B-%F0%9D%91%9A%F0%9D%91%92%F0%9D%91%90%E2%84%8E%F0%9D%91%8E%F0%9D%91%9B%F0%9D%91%96%F0%9D%91%A0%F0%9D%91%9A%EF%BC%89"><span class="nav-number">3.1.2.</span> <span class="nav-text">自注意力层（𝑠𝑒𝑙𝑓 𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 𝑚𝑒𝑐ℎ𝑎𝑛𝑖𝑠𝑚）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-Headed-Attention"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">Multi-Headed Attention</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E9%93%BE%E6%8E%A5%E5%92%8C%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">3.1.3.</span> <span class="nav-text">残差链接和层归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.1.3.1.</span> <span class="nav-text">残差设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">3.1.3.2.</span> <span class="nav-text">层归一化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C"><span class="nav-number">3.1.4.</span> <span class="nav-text">前馈网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E6%80%BB%E7%BB%93"><span class="nav-number">3.1.5.</span> <span class="nav-text">编码器总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-number">3.2.</span> <span class="nav-text">解码器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Masked-Self-Attention"><span class="nav-number">3.2.1.</span> <span class="nav-text">Masked Self-Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Masked-Encoder-Decoder-Attention"><span class="nav-number">3.2.2.</span> <span class="nav-text">Masked Encoder-Decoder Attention</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Vision-Transformer"><span class="nav-number">4.</span> <span class="nav-text">Vision Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E5%9D%97%E5%B5%8C%E5%85%A5"><span class="nav-number">4.1.</span> <span class="nav-text">图像分块嵌入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Class-Token"><span class="nav-number">4.2.</span> <span class="nav-text">Class Token</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Positional-Encoding"><span class="nav-number">4.3.</span> <span class="nav-text">Positional Encoding</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xin-Bo Qi(亓欣波)"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Xin-Bo Qi(亓欣波)</p>
  <div class="site-description" itemprop="description">Digitize everything to realize Digitalization!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">163</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/qixinbo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qixinbo" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qixinbo@gmail.com" title="E-Mail → mailto:qixinbo@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/qixinbo" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;qixinbo" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fas fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.tsinghua.edu.cn/" title="https:&#x2F;&#x2F;www.tsinghua.edu.cn" rel="noopener" target="_blank">THU</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.imr.cas.cn/" title="http:&#x2F;&#x2F;www.imr.cas.cn" rel="noopener" target="_blank">IMR</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.sdu.edu.cn/" title="http:&#x2F;&#x2F;www.sdu.edu.cn" rel="noopener" target="_blank">SDU</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://liam0205.me/" title="http:&#x2F;&#x2F;liam0205.me&#x2F;" rel="noopener" target="_blank">黄晨成</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xin-Bo Qi(亓欣波)</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://qixinbo.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "http://qixinbo.github.io/2021/11/09/transformer/";
    this.page.identifier = "2021/11/09/transformer/";
    this.page.title = "NLP霸主Transformer及CV新秀Vision Transformer解析";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://qixinbo.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
